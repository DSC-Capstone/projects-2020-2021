,phrase,score
0,neural networks,0.8548830631023001
1,pos tagging,0.78638505060885
2,bi lstm,0.7324850599642999
3,sequence labeling,0.73210725055035
4,word embedding,0.7147973888621
5,transfer learning,0.7037840901735
6,language model,0.6981825698537499
7,word embeddings,0.6678259412851499
8,lstm crf,0.63885003928705
9,conditional random,0.6309577078687499
10,f1 score,0.6255152188374999
11,named entity recognition,0.60818630695
12,task specific,0.6013841448225
13,highway layers,0.5940805581795
14,language models,0.5937706491139999
15,hidden state,0.58223563120415
16,f1,0.5788074311073287
17,highway,0.5586074149750483
18,pre trained,0.55778090682825
19,character level,0.55629055108485
20,word level,0.55135464803915
21,cnn,0.5491844213364353
22,ma,0.5481795033244042
23,conll,0.5343303165546823
24,ner,0.5195999691071842
25,multi task learning,0.5125851580495
26,sequence labeling tasks,0.5058495076305
27,pos,0.4966413359042833
28,lm,0.49519902037623514
29,crf,0.4908594256573547
30,xi,0.4842393422699494
31,wsj,0.4826706570794087
32,art,0.4799319950563937
33,bi,0.4724385061263853
34,learning,0.4719470252860234
35,forward,0.4705051747282329
36,table,0.469837327718152
37,index,0.4676810725279365
38,knowledge,0.4654574289723378
39,backward,0.4639471429366303
40,hidden,0.4628087895824931
41,lstm,0.4615928068154454
42,yang,0.45955546945557785
43,language,0.4567075638429528
44,pr,0.4505865046836495
45,network,0.45021482862845014
46,entity,0.44892001384942953
47,exp,0.4462432732561571
48,chunking,0.4439111899880585
49,embedding,0.44131269319241406
50,neural,0.43881363445983057
51,liu,0.4375564357387417
52,word,0.43640351696912605
53,tagging,0.4361794893590146
54,pf,0.43535711442179315
55,networks,0.4346439489882748
56,yi,0.43367724033276867
57,ci,0.4309621242361821
58,model,0.4307026688259657
59,max,0.42909739410978254
60,transfer,0.4284484426181919
61,labeling,0.4274305705796836
62,training,0.4272087979822724
63,peters,0.4271709903657536
64,score,0.4248333011175449
65,pierre,0.42449402429400296
66,sequence,0.4242267353736883
67,character,0.42312796238965417
68,ner task,0.4215778093407
69,dataset,0.42080936544240416
70,peters et al,0.41974594955865
71,embeddings,0.413799811571685
72,label,0.41021828870213356
73,data,0.40864947298021587
74,task,0.4067129496102425
75,rni,0.40373796748312557
76,unit,0.4033354355261436
77,efficient,0.40165630328570545
78,pre,0.40016013473314066
79,extra,0.3987365099323771
80,luo,0.39713796313771044
81,x1,0.39378203247611804
82,units,0.39319547498121565
83,layers,0.3915737434064249
84,maximum,0.3881534829736931
85,effective,0.38784118397357625
86,models,0.38725998312478205
87,corpus,0.3864026547475483
88,joint,0.3857431730981992
89,state,0.3854924084869496
90,test,0.38307721174326187
91,log,0.3820980525435863
92,comparison,0.3818232388502665
93,comparing,0.379714576911687
94,experiments,0.3790663123791417
95,previous,0.3785569218510345
96,c0,0.37631214706200106
97,manning,0.3760862741935048
98,adopting,0.372783107069507
99,set,0.37092732377811377
100,zi,0.3699909014794219
101,information,0.3695042090063525
102,layer,0.36943901152088604
103,neural language,0.36866854709514996
104,size,0.36408007453644337
105,space,0.36294332739047186
106,words,0.362872863663615
107,lample,0.3615352201761479
108,input,0.36071607787819826
109,dimension,0.35764479218922185
110,order,0.3567321788432458
111,output,0.35454033633200904
112,results,0.3532702604318231
113,probability,0.3529014498244523
114,large,0.34887483804201425
115,development,0.3466417207005393
116,arxiv,0.34467465190978663
117,structure,0.344042781000116
118,features,0.33964333225515353
119,tasks,0.3394233183937991
120,additional,0.33923231187322095
121,level,0.33822185250008674
122,strategy,0.33820543899667344
123,efficiency,0.3366242867786679
124,similar,0.3320626905714782
125,related,0.3313916713682588
126,extract,0.3287431727036592
127,portion,0.32644852937506885
128,annotations,0.3238830185177229
129,trained,0.3216722636043996
130,specific,0.3213082866293572
131,methods,0.3203061817557727
132,capture,0.3201434278307643
133,framework,0.3175953084857193
134,performance,0.31669000819887294
135,reported,0.3162580099996792
136,parameters,0.31452554147922307
137,train,0.30229979902645304
138,datasets,0.29920288874683443
139,named,0.29521776354268053
140,co training,0.29489663114405
141,effectiveness,0.2942010876466798
142,applied,0.293441630096923
143,directly,0.29299274932102526
144,resources,0.29241070083624443
145,baselines,0.2918165461244782
146,proposed,0.2834431086787846
147,setting,0.2805195609695288
148,ner dataset,0.2778529659071
149,adopted,0.27191254354884636
150,e.g,0.26842683823809615
151,outperforms,0.2656140033894967
152,leverage,0.26259021953071415
153,incorporate,0.2622678064376234
154,conduct,0.2609229431135087
155,z,0.2501678814990667
156,co,0.24459821373055934
157,instead,0.23523057262101926
158,part,0.23393699347399305
159,et al,0.22901456472799997
160,state of,0.22434728737905
161,these,0.22214191582610168
162,this,0.21889376416390274
163,w,0.2163658417461138
164,l,0.21518899786971382
165,m,0.21278609772384074
166,t,0.21229085583682028
167,d,0.20970236089270244
168,f,0.20842324143536356
169,s,0.20402340181827155
170,k,0.20101219684934601
171,in,0.19976703775491966
172,first,0.19921631225579342
173,g,0.19818534635353388
174,h,0.19769083616246444
175,e,0.1963602646304639
176,p,0.19488808095245247
177,at,0.19304983887441712
178,j,0.1922037158929668
179,r,0.19187200739626545
180,y,0.19182689696880914
181,x,0.19163216894576818
182,for,0.19105403952510486
183,n,0.19068848211749886
184,on,0.1892233908748164
185,c,0.18847520072326468
186,2000,0.18733720308108276
187,i,0.18687777638868466
188,state size,0.18671264981205002
189,while,0.1858831594595518
190,most,0.18433181806832566
191,thus,0.1843286603157166
192,8192,0.18200125170324324
193,although,0.18072017639066787
194,one,0.17904861482464074
195,many,0.17858658562241025
196,besides,0.17760950362318098
197,as,0.17723770331518546
198,further,0.17689203634255438
199,|,0.1765452723635471
200,because,0.1746736269951141
201,three,0.17401111019910587
202,the,0.17389620950668347
203,1024,0.17389097131558814
204,1,0.17141926202307503
205,a,0.17106911377906445
206,however,0.1699930698951236
207,4096,0.16994320378194477
208,2048,0.1671170996031322
209,to,0.16635031874104597
210,we,0.164364848149931
211,such,0.1641862810296665
212,10,0.16413325906509912
213,an,0.16387953285978993
214,our,0.1635659662845625
215,better,0.163332077702619
216,this paper,0.1628868850119
217,more,0.1626628390108809
218,also,0.16236439533717756
219,it,0.16235060772801443
220,2017,0.16231241254998988
221,from,0.16217240761783328
222,the art,0.16144819986300002
223,two,0.16009513947323603
224,2016,0.15958559745764767
225,trained on,0.15935849661374998
226,time,0.15920578671267924
227,4,0.15896482608088391
228,when,0.15731342533579712
229,all,0.15588033613571714
230,with,0.15583452552032692
231,even,0.15490034122061258
232,much,0.1548738897952977
233,512,0.1538976220549772
234,some,0.15358623772550858
235,but,0.1496145829165607
236,by,0.1488955852908838
237,other,0.14877366667279124
238,+,0.14770874008422846
239,each,0.1474802860782209
240,11,0.14595694235011944
241,100,0.14565373635721746
242,9,0.1449930061346368
243,3,0.14469789387144102
244,8,0.14262399802026382
245,2003,0.14222134372917578
246,2011,0.14197179812491822
247,2014,0.1407038576506507
248,different,0.14020622364869145
249,300,0.13996663446987687
250,200,0.13957796430413216
251,2,0.13957029556657427
252,7,0.13941283707783472
253,6,0.1386203658429207
254,of,0.13820802489492084
255,both,0.1381574232785974
256,only,0.13756514480390467
257,over,0.13660364892432506
258,not,0.13637102150440764
259,5,0.13635378288214867
260,any,0.1342258169227034
261,without,0.13190849333429505
262,2015,0.13178795188887626
263,or,0.12665899948731243
264,its,0.1265908211135048
265,=,0.12509394440027
266,to extract,0.12421387078099999
267,&,0.12360414300551244
268,than,0.12300729406999734
269,into,0.12165043962457875
270,the same,0.1209964665906
271,based on,0.12052745831304999
272,and,0.11604048741341105
273,would,0.11318339611046133
274,could,0.11312057503778669
275,where,0.11238811922579418
276,these models,0.11033932868200001
277,have,0.10976565069913127
278,that,0.1096206783760696
279,has,0.10937977983689631
280,been,0.10881084114452359
281,can,0.10829211878246608
282,which,0.10758917368224129
283,is,0.10736181487089053
284,are,0.107265540471714
285,be,0.10688839301028323
286,to capture,0.10257573857995
287,for example,0.09831803045755001
288,instead of,0.0946344098869
289,the output of,0.08674048843814999
290,knowledge from,0.0858976887531
291,output of,0.0798988402269
292,the output,0.07878351912374999
293,model with,0.0656321708166
294,the conll,0.05597214332595
295,the language model,0.05526371825549999
296,the sequence,0.05409443597575
