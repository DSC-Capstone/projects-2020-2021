{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "graphsage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragPamuru/dsc-180b-capstone-b03/blob/main/graphsage_combine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cSjRs_tj8LO"
      },
      "source": [
        "from data_loader_sen import data_loader\n",
        "from GraphSage import GraphSage"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zZv-MaUSTSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68524016-2979-4f72-bbbb-41952c963cee"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8jh46TUj8LY"
      },
      "source": [
        "loader = data_loader(\"voting_features.csv\", \"tweets.csv\", \"edges.csv\")\n",
        "features, labels, A = loader.get_data()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHyI24qHj8LZ",
        "outputId": "6a2e1670-6766-4268-d613-5287f3602c74"
      },
      "source": [
        "model = GraphSage(A, features, labels, agg_func='MEAN', len_walk=13, num_neigh=10, F=1079)\n",
        "acc = model.train_epoch(epochs=200, lr=1e-4)\n",
        "# acc['acc']"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train length :70, Validation length :30\n",
            "Epoch: 0\n",
            "training loss 194.0902\n",
            "Validtion: Average loss: 157.3358, Accuracy: 0.0000%\n",
            "Epoch: 1\n",
            "training loss 146.5895\n",
            "Validtion: Average loss: 86.8538, Accuracy: 6.6667%\n",
            "Epoch: 2\n",
            "training loss 106.7988\n",
            "Validtion: Average loss: 53.9038, Accuracy: 20.0000%\n",
            "Epoch: 3\n",
            "training loss 76.8291\n",
            "Validtion: Average loss: 35.5670, Accuracy: 30.0000%\n",
            "Epoch: 4\n",
            "training loss 43.8500\n",
            "Validtion: Average loss: 29.1897, Accuracy: 40.0000%\n",
            "Epoch: 5\n",
            "training loss 30.2012\n",
            "Validtion: Average loss: 15.4366, Accuracy: 63.3333%\n",
            "Epoch: 6\n",
            "training loss 18.6025\n",
            "Validtion: Average loss: 17.6969, Accuracy: 50.0000%\n",
            "Epoch: 7\n",
            "training loss 18.7169\n",
            "Validtion: Average loss: 18.6164, Accuracy: 53.3333%\n",
            "Epoch: 8\n",
            "training loss 15.2994\n",
            "Validtion: Average loss: 20.6304, Accuracy: 50.0000%\n",
            "Epoch: 9\n",
            "training loss 20.7981\n",
            "Validtion: Average loss: 22.1648, Accuracy: 56.6667%\n",
            "Epoch: 10\n",
            "training loss 15.6459\n",
            "Validtion: Average loss: 10.8146, Accuracy: 73.3333%\n",
            "Epoch: 11\n",
            "training loss 13.3071\n",
            "Validtion: Average loss: 15.3678, Accuracy: 60.0000%\n",
            "Epoch: 12\n",
            "training loss 11.3012\n",
            "Validtion: Average loss: 16.5906, Accuracy: 63.3333%\n",
            "Epoch: 13\n",
            "training loss 9.5714\n",
            "Validtion: Average loss: 16.0953, Accuracy: 66.6667%\n",
            "Epoch: 14\n",
            "training loss 9.5683\n",
            "Validtion: Average loss: 18.8943, Accuracy: 66.6667%\n",
            "Epoch: 15\n",
            "training loss 6.5222\n",
            "Validtion: Average loss: 21.8173, Accuracy: 60.0000%\n",
            "Epoch: 16\n",
            "training loss 6.4434\n",
            "Validtion: Average loss: 18.5943, Accuracy: 60.0000%\n",
            "Epoch: 17\n",
            "training loss 8.3411\n",
            "Validtion: Average loss: 10.6650, Accuracy: 73.3333%\n",
            "Epoch: 18\n",
            "training loss 8.4906\n",
            "Validtion: Average loss: 17.9345, Accuracy: 70.0000%\n",
            "Epoch: 19\n",
            "training loss 7.0207\n",
            "Validtion: Average loss: 16.7198, Accuracy: 63.3333%\n",
            "Epoch: 20\n",
            "training loss 6.0691\n",
            "Validtion: Average loss: 18.7064, Accuracy: 66.6667%\n",
            "Epoch: 21\n",
            "training loss 6.0180\n",
            "Validtion: Average loss: 14.4167, Accuracy: 76.6667%\n",
            "Epoch: 22\n",
            "training loss 5.4741\n",
            "Validtion: Average loss: 13.0438, Accuracy: 70.0000%\n",
            "Epoch: 23\n",
            "training loss 6.9451\n",
            "Validtion: Average loss: 13.7490, Accuracy: 73.3333%\n",
            "Epoch: 24\n",
            "training loss 7.7808\n",
            "Validtion: Average loss: 11.3290, Accuracy: 83.3333%\n",
            "Epoch: 25\n",
            "training loss 4.4034\n",
            "Validtion: Average loss: 12.2160, Accuracy: 70.0000%\n",
            "Epoch: 26\n",
            "training loss 2.0738\n",
            "Validtion: Average loss: 16.0765, Accuracy: 80.0000%\n",
            "Epoch: 27\n",
            "training loss 3.9930\n",
            "Validtion: Average loss: 14.2067, Accuracy: 66.6667%\n",
            "Epoch: 28\n",
            "training loss 4.7968\n",
            "Validtion: Average loss: 10.0887, Accuracy: 70.0000%\n",
            "Epoch: 29\n",
            "training loss 3.8562\n",
            "Validtion: Average loss: 13.0740, Accuracy: 73.3333%\n",
            "Epoch: 30\n",
            "training loss 4.4082\n",
            "Validtion: Average loss: 14.5521, Accuracy: 66.6667%\n",
            "Epoch: 31\n",
            "training loss 3.4588\n",
            "Validtion: Average loss: 18.6223, Accuracy: 70.0000%\n",
            "Epoch: 32\n",
            "training loss 3.2378\n",
            "Validtion: Average loss: 14.9494, Accuracy: 73.3333%\n",
            "Epoch: 33\n",
            "training loss 5.0251\n",
            "Validtion: Average loss: 13.8123, Accuracy: 63.3333%\n",
            "Epoch: 34\n",
            "training loss 3.3101\n",
            "Validtion: Average loss: 8.3479, Accuracy: 76.6667%\n",
            "Epoch: 35\n",
            "training loss 2.3286\n",
            "Validtion: Average loss: 8.9477, Accuracy: 86.6667%\n",
            "Epoch: 36\n",
            "training loss 1.7041\n",
            "Validtion: Average loss: 7.2724, Accuracy: 83.3333%\n",
            "Epoch: 37\n",
            "training loss 1.8366\n",
            "Validtion: Average loss: 14.4075, Accuracy: 80.0000%\n",
            "Epoch: 38\n",
            "training loss 2.4970\n",
            "Validtion: Average loss: 15.2752, Accuracy: 70.0000%\n",
            "Epoch: 39\n",
            "training loss 4.0684\n",
            "Validtion: Average loss: 14.9610, Accuracy: 73.3333%\n",
            "Epoch: 40\n",
            "training loss 2.1591\n",
            "Validtion: Average loss: 12.8457, Accuracy: 80.0000%\n",
            "Epoch: 41\n",
            "training loss 4.2170\n",
            "Validtion: Average loss: 10.2853, Accuracy: 76.6667%\n",
            "Epoch: 42\n",
            "training loss 0.9935\n",
            "Validtion: Average loss: 9.9680, Accuracy: 73.3333%\n",
            "Epoch: 43\n",
            "training loss 2.3712\n",
            "Validtion: Average loss: 10.5377, Accuracy: 86.6667%\n",
            "Epoch: 44\n",
            "training loss 1.3030\n",
            "Validtion: Average loss: 14.5881, Accuracy: 70.0000%\n",
            "Epoch: 45\n",
            "training loss 2.9708\n",
            "Validtion: Average loss: 18.7777, Accuracy: 73.3333%\n",
            "Epoch: 46\n",
            "training loss 1.6982\n",
            "Validtion: Average loss: 13.7419, Accuracy: 76.6667%\n",
            "Epoch: 47\n",
            "training loss 1.6316\n",
            "Validtion: Average loss: 12.9631, Accuracy: 80.0000%\n",
            "Epoch: 48\n",
            "training loss 0.8807\n",
            "Validtion: Average loss: 10.4533, Accuracy: 73.3333%\n",
            "Epoch: 49\n",
            "training loss 0.9443\n",
            "Validtion: Average loss: 11.5212, Accuracy: 83.3333%\n",
            "Epoch: 50\n",
            "training loss 0.9852\n",
            "Validtion: Average loss: 9.2241, Accuracy: 83.3333%\n",
            "Epoch: 51\n",
            "training loss 1.9513\n",
            "Validtion: Average loss: 11.4134, Accuracy: 80.0000%\n",
            "Epoch: 52\n",
            "training loss 2.0170\n",
            "Validtion: Average loss: 11.0868, Accuracy: 83.3333%\n",
            "Epoch: 53\n",
            "training loss 1.0123\n",
            "Validtion: Average loss: 9.5363, Accuracy: 86.6667%\n",
            "Epoch: 54\n",
            "training loss 1.9963\n",
            "Validtion: Average loss: 11.3881, Accuracy: 80.0000%\n",
            "Epoch: 55\n",
            "training loss 2.3350\n",
            "Validtion: Average loss: 10.6071, Accuracy: 80.0000%\n",
            "Epoch: 56\n",
            "training loss 1.0395\n",
            "Validtion: Average loss: 11.1977, Accuracy: 76.6667%\n",
            "Epoch: 57\n",
            "training loss 1.0876\n",
            "Validtion: Average loss: 9.2291, Accuracy: 76.6667%\n",
            "Epoch: 58\n",
            "training loss 1.4240\n",
            "Validtion: Average loss: 9.3768, Accuracy: 86.6667%\n",
            "Epoch: 59\n",
            "training loss 0.7572\n",
            "Validtion: Average loss: 8.7170, Accuracy: 80.0000%\n",
            "Epoch: 60\n",
            "training loss 1.9120\n",
            "Validtion: Average loss: 13.5426, Accuracy: 70.0000%\n",
            "Epoch: 61\n",
            "training loss 1.9079\n",
            "Validtion: Average loss: 11.4245, Accuracy: 83.3333%\n",
            "Epoch: 62\n",
            "training loss 1.4357\n",
            "Validtion: Average loss: 7.9803, Accuracy: 90.0000%\n",
            "Epoch: 63\n",
            "training loss 0.7807\n",
            "Validtion: Average loss: 9.5905, Accuracy: 96.6667%\n",
            "Epoch: 64\n",
            "training loss 1.0084\n",
            "Validtion: Average loss: 8.8534, Accuracy: 86.6667%\n",
            "Epoch: 65\n",
            "training loss 0.7506\n",
            "Validtion: Average loss: 9.4621, Accuracy: 83.3333%\n",
            "Epoch: 66\n",
            "training loss 0.7865\n",
            "Validtion: Average loss: 9.1882, Accuracy: 90.0000%\n",
            "Epoch: 67\n",
            "training loss 1.0779\n",
            "Validtion: Average loss: 8.9726, Accuracy: 90.0000%\n",
            "Epoch: 68\n",
            "training loss 1.0774\n",
            "Validtion: Average loss: 9.7897, Accuracy: 93.3333%\n",
            "Epoch: 69\n",
            "training loss 0.8564\n",
            "Validtion: Average loss: 13.2763, Accuracy: 86.6667%\n",
            "Epoch: 70\n",
            "training loss 0.4018\n",
            "Validtion: Average loss: 9.0874, Accuracy: 86.6667%\n",
            "Epoch: 71\n",
            "training loss 1.1392\n",
            "Validtion: Average loss: 9.5266, Accuracy: 86.6667%\n",
            "Epoch: 72\n",
            "training loss 1.0482\n",
            "Validtion: Average loss: 9.0254, Accuracy: 86.6667%\n",
            "Epoch: 73\n",
            "training loss 0.2922\n",
            "Validtion: Average loss: 9.2869, Accuracy: 90.0000%\n",
            "Epoch: 74\n",
            "training loss 1.4140\n",
            "Validtion: Average loss: 9.6410, Accuracy: 86.6667%\n",
            "Epoch: 75\n",
            "training loss 0.3436\n",
            "Validtion: Average loss: 9.4416, Accuracy: 90.0000%\n",
            "Epoch: 76\n",
            "training loss 1.2140\n",
            "Validtion: Average loss: 11.3951, Accuracy: 86.6667%\n",
            "Epoch: 77\n",
            "training loss 2.1826\n",
            "Validtion: Average loss: 10.6565, Accuracy: 86.6667%\n",
            "Epoch: 78\n",
            "training loss 0.3639\n",
            "Validtion: Average loss: 9.0706, Accuracy: 83.3333%\n",
            "Epoch: 79\n",
            "training loss 0.6518\n",
            "Validtion: Average loss: 9.4872, Accuracy: 93.3333%\n",
            "Epoch: 80\n",
            "training loss 1.3709\n",
            "Validtion: Average loss: 7.6920, Accuracy: 83.3333%\n",
            "Epoch: 81\n",
            "training loss 0.6673\n",
            "Validtion: Average loss: 8.5520, Accuracy: 90.0000%\n",
            "Epoch: 82\n",
            "training loss 0.8734\n",
            "Validtion: Average loss: 8.7203, Accuracy: 90.0000%\n",
            "Epoch: 83\n",
            "training loss 1.0009\n",
            "Validtion: Average loss: 7.7054, Accuracy: 83.3333%\n",
            "Epoch: 84\n",
            "training loss 1.9984\n",
            "Validtion: Average loss: 6.3159, Accuracy: 90.0000%\n",
            "Epoch: 85\n",
            "training loss 0.8815\n",
            "Validtion: Average loss: 9.9779, Accuracy: 83.3333%\n",
            "Epoch: 86\n",
            "training loss 0.9230\n",
            "Validtion: Average loss: 9.7218, Accuracy: 80.0000%\n",
            "Epoch: 87\n",
            "training loss 0.5190\n",
            "Validtion: Average loss: 8.6535, Accuracy: 86.6667%\n",
            "Epoch: 88\n",
            "training loss 1.5342\n",
            "Validtion: Average loss: 10.9772, Accuracy: 76.6667%\n",
            "Epoch: 89\n",
            "training loss 0.4587\n",
            "Validtion: Average loss: 8.3331, Accuracy: 90.0000%\n",
            "Epoch: 90\n",
            "training loss 0.0884\n",
            "Validtion: Average loss: 9.2510, Accuracy: 90.0000%\n",
            "Epoch: 91\n",
            "training loss 0.2759\n",
            "Validtion: Average loss: 12.2846, Accuracy: 86.6667%\n",
            "Epoch: 92\n",
            "training loss 0.1470\n",
            "Validtion: Average loss: 10.0591, Accuracy: 93.3333%\n",
            "Epoch: 93\n",
            "training loss 0.6119\n",
            "Validtion: Average loss: 10.0845, Accuracy: 93.3333%\n",
            "Epoch: 94\n",
            "training loss 0.4360\n",
            "Validtion: Average loss: 7.4066, Accuracy: 93.3333%\n",
            "Epoch: 95\n",
            "training loss 0.3895\n",
            "Validtion: Average loss: 9.5530, Accuracy: 80.0000%\n",
            "Epoch: 96\n",
            "training loss 0.4787\n",
            "Validtion: Average loss: 8.5639, Accuracy: 90.0000%\n",
            "Epoch: 97\n",
            "training loss 0.4344\n",
            "Validtion: Average loss: 9.4909, Accuracy: 90.0000%\n",
            "Epoch: 98\n",
            "training loss 0.4213\n",
            "Validtion: Average loss: 8.9540, Accuracy: 90.0000%\n",
            "Epoch: 99\n",
            "training loss 1.0032\n",
            "Validtion: Average loss: 8.5112, Accuracy: 90.0000%\n",
            "Epoch: 100\n",
            "training loss 0.7248\n",
            "Validtion: Average loss: 9.8405, Accuracy: 93.3333%\n",
            "Epoch: 101\n",
            "training loss 0.0531\n",
            "Validtion: Average loss: 8.6593, Accuracy: 80.0000%\n",
            "Epoch: 102\n",
            "training loss 0.7193\n",
            "Validtion: Average loss: 10.7522, Accuracy: 86.6667%\n",
            "Epoch: 103\n",
            "training loss 0.3090\n",
            "Validtion: Average loss: 8.9178, Accuracy: 96.6667%\n",
            "Epoch: 104\n",
            "training loss 0.9481\n",
            "Validtion: Average loss: 9.8858, Accuracy: 90.0000%\n",
            "Epoch: 105\n",
            "training loss 0.4353\n",
            "Validtion: Average loss: 9.5244, Accuracy: 83.3333%\n",
            "Epoch: 106\n",
            "training loss 0.6596\n",
            "Validtion: Average loss: 8.2580, Accuracy: 86.6667%\n",
            "Epoch: 107\n",
            "training loss 0.7185\n",
            "Validtion: Average loss: 7.0637, Accuracy: 96.6667%\n",
            "Epoch: 108\n",
            "training loss 0.1929\n",
            "Validtion: Average loss: 12.7615, Accuracy: 83.3333%\n",
            "Epoch: 109\n",
            "training loss 0.2193\n",
            "Validtion: Average loss: 13.9147, Accuracy: 86.6667%\n",
            "Epoch: 110\n",
            "training loss 0.5986\n",
            "Validtion: Average loss: 9.9106, Accuracy: 83.3333%\n",
            "Epoch: 111\n",
            "training loss 0.5060\n",
            "Validtion: Average loss: 10.7513, Accuracy: 83.3333%\n",
            "Epoch: 112\n",
            "training loss 0.0034\n",
            "Validtion: Average loss: 9.2918, Accuracy: 83.3333%\n",
            "Epoch: 113\n",
            "training loss 0.1826\n",
            "Validtion: Average loss: 8.2168, Accuracy: 76.6667%\n",
            "Epoch: 114\n",
            "training loss 0.4208\n",
            "Validtion: Average loss: 9.3765, Accuracy: 86.6667%\n",
            "Epoch: 115\n",
            "training loss 0.2516\n",
            "Validtion: Average loss: 9.0507, Accuracy: 93.3333%\n",
            "Epoch: 116\n",
            "training loss 0.4598\n",
            "Validtion: Average loss: 10.8489, Accuracy: 83.3333%\n",
            "Epoch: 117\n",
            "training loss 0.4818\n",
            "Validtion: Average loss: 8.0131, Accuracy: 90.0000%\n",
            "Epoch: 118\n",
            "training loss 0.1770\n",
            "Validtion: Average loss: 7.4502, Accuracy: 93.3333%\n",
            "Epoch: 119\n",
            "training loss 0.3436\n",
            "Validtion: Average loss: 9.7035, Accuracy: 90.0000%\n",
            "Epoch: 120\n",
            "training loss 0.2487\n",
            "Validtion: Average loss: 8.9020, Accuracy: 86.6667%\n",
            "Epoch: 121\n",
            "training loss 0.8074\n",
            "Validtion: Average loss: 9.4723, Accuracy: 80.0000%\n",
            "Epoch: 122\n",
            "training loss 0.3421\n",
            "Validtion: Average loss: 10.2939, Accuracy: 93.3333%\n",
            "Epoch: 123\n",
            "training loss 0.4799\n",
            "Validtion: Average loss: 7.3504, Accuracy: 90.0000%\n",
            "Epoch: 124\n",
            "training loss 0.2025\n",
            "Validtion: Average loss: 7.9937, Accuracy: 86.6667%\n",
            "Epoch: 125\n",
            "training loss 0.2266\n",
            "Validtion: Average loss: 8.5964, Accuracy: 93.3333%\n",
            "Epoch: 126\n",
            "training loss 0.0062\n",
            "Validtion: Average loss: 8.2516, Accuracy: 90.0000%\n",
            "Epoch: 127\n",
            "training loss 0.2077\n",
            "Validtion: Average loss: 6.3475, Accuracy: 93.3333%\n",
            "Epoch: 128\n",
            "training loss 0.5290\n",
            "Validtion: Average loss: 7.9116, Accuracy: 90.0000%\n",
            "Epoch: 129\n",
            "training loss 0.1649\n",
            "Validtion: Average loss: 7.8655, Accuracy: 93.3333%\n",
            "Epoch: 130\n",
            "training loss 0.6704\n",
            "Validtion: Average loss: 7.0560, Accuracy: 93.3333%\n",
            "Epoch: 131\n",
            "training loss 0.2888\n",
            "Validtion: Average loss: 11.3937, Accuracy: 80.0000%\n",
            "Epoch: 132\n",
            "training loss 0.4180\n",
            "Validtion: Average loss: 8.7768, Accuracy: 86.6667%\n",
            "Epoch: 133\n",
            "training loss 0.1738\n",
            "Validtion: Average loss: 6.7905, Accuracy: 93.3333%\n",
            "Epoch: 134\n",
            "training loss 0.1889\n",
            "Validtion: Average loss: 10.1789, Accuracy: 90.0000%\n",
            "Epoch: 135\n",
            "training loss 0.5613\n",
            "Validtion: Average loss: 7.3225, Accuracy: 90.0000%\n",
            "Epoch: 136\n",
            "training loss 0.1332\n",
            "Validtion: Average loss: 6.9935, Accuracy: 93.3333%\n",
            "Epoch: 137\n",
            "training loss 0.0747\n",
            "Validtion: Average loss: 8.4003, Accuracy: 90.0000%\n",
            "Epoch: 138\n",
            "training loss 0.2135\n",
            "Validtion: Average loss: 7.8618, Accuracy: 90.0000%\n",
            "Epoch: 139\n",
            "training loss 0.7221\n",
            "Validtion: Average loss: 7.6417, Accuracy: 86.6667%\n",
            "Epoch: 140\n",
            "training loss 0.1024\n",
            "Validtion: Average loss: 9.1487, Accuracy: 80.0000%\n",
            "Epoch: 141\n",
            "training loss 0.2816\n",
            "Validtion: Average loss: 9.7711, Accuracy: 93.3333%\n",
            "Epoch: 142\n",
            "training loss 0.0488\n",
            "Validtion: Average loss: 8.4461, Accuracy: 90.0000%\n",
            "Epoch: 143\n",
            "training loss 0.4288\n",
            "Validtion: Average loss: 9.0220, Accuracy: 86.6667%\n",
            "Epoch: 144\n",
            "training loss 0.2601\n",
            "Validtion: Average loss: 11.8249, Accuracy: 83.3333%\n",
            "Epoch: 145\n",
            "training loss 0.0017\n",
            "Validtion: Average loss: 10.8548, Accuracy: 86.6667%\n",
            "Epoch: 146\n",
            "training loss 0.0647\n",
            "Validtion: Average loss: 11.4252, Accuracy: 90.0000%\n",
            "Epoch: 147\n",
            "training loss 0.3277\n",
            "Validtion: Average loss: 9.0627, Accuracy: 90.0000%\n",
            "Epoch: 148\n",
            "training loss 0.8291\n",
            "Validtion: Average loss: 7.5500, Accuracy: 96.6667%\n",
            "Epoch: 149\n",
            "training loss 0.1539\n",
            "Validtion: Average loss: 7.7446, Accuracy: 90.0000%\n",
            "Epoch: 150\n",
            "training loss 0.7876\n",
            "Validtion: Average loss: 9.1809, Accuracy: 86.6667%\n",
            "Epoch: 151\n",
            "training loss 0.4261\n",
            "Validtion: Average loss: 8.8188, Accuracy: 90.0000%\n",
            "Epoch: 152\n",
            "training loss 0.1550\n",
            "Validtion: Average loss: 10.5215, Accuracy: 83.3333%\n",
            "Epoch: 153\n",
            "training loss 0.0012\n",
            "Validtion: Average loss: 9.3242, Accuracy: 86.6667%\n",
            "Epoch: 154\n",
            "training loss 0.3374\n",
            "Validtion: Average loss: 8.7392, Accuracy: 93.3333%\n",
            "Epoch: 155\n",
            "training loss 0.2025\n",
            "Validtion: Average loss: 9.4188, Accuracy: 83.3333%\n",
            "Epoch: 156\n",
            "training loss 0.2438\n",
            "Validtion: Average loss: 9.6514, Accuracy: 90.0000%\n",
            "Epoch: 157\n",
            "training loss 0.1374\n",
            "Validtion: Average loss: 8.4197, Accuracy: 90.0000%\n",
            "Epoch: 158\n",
            "training loss 0.2856\n",
            "Validtion: Average loss: 9.4240, Accuracy: 90.0000%\n",
            "Epoch: 159\n",
            "training loss 0.0012\n",
            "Validtion: Average loss: 7.6942, Accuracy: 93.3333%\n",
            "Epoch: 160\n",
            "training loss 0.0433\n",
            "Validtion: Average loss: 10.0677, Accuracy: 83.3333%\n",
            "Epoch: 161\n",
            "training loss 0.1769\n",
            "Validtion: Average loss: 8.9538, Accuracy: 90.0000%\n",
            "Epoch: 162\n",
            "training loss 0.0095\n",
            "Validtion: Average loss: 10.7938, Accuracy: 90.0000%\n",
            "Epoch: 163\n",
            "training loss 0.0848\n",
            "Validtion: Average loss: 11.9013, Accuracy: 90.0000%\n",
            "Epoch: 164\n",
            "training loss 0.2336\n",
            "Validtion: Average loss: 10.1724, Accuracy: 90.0000%\n",
            "Epoch: 165\n",
            "training loss 0.3519\n",
            "Validtion: Average loss: 9.3248, Accuracy: 86.6667%\n",
            "Epoch: 166\n",
            "training loss 0.0176\n",
            "Validtion: Average loss: 6.9294, Accuracy: 90.0000%\n",
            "Epoch: 167\n",
            "training loss 0.0343\n",
            "Validtion: Average loss: 8.1910, Accuracy: 90.0000%\n",
            "Epoch: 168\n",
            "training loss 0.1895\n",
            "Validtion: Average loss: 7.6140, Accuracy: 86.6667%\n",
            "Epoch: 169\n",
            "training loss 0.0140\n",
            "Validtion: Average loss: 7.5890, Accuracy: 86.6667%\n",
            "Epoch: 170\n",
            "training loss 0.3424\n",
            "Validtion: Average loss: 9.7162, Accuracy: 90.0000%\n",
            "Epoch: 171\n",
            "training loss 0.2124\n",
            "Validtion: Average loss: 8.7485, Accuracy: 86.6667%\n",
            "Epoch: 172\n",
            "training loss 0.1465\n",
            "Validtion: Average loss: 7.8870, Accuracy: 93.3333%\n",
            "Epoch: 173\n",
            "training loss 0.1984\n",
            "Validtion: Average loss: 7.3143, Accuracy: 93.3333%\n",
            "Epoch: 174\n",
            "training loss 0.0273\n",
            "Validtion: Average loss: 9.4966, Accuracy: 90.0000%\n",
            "Epoch: 175\n",
            "training loss 0.2649\n",
            "Validtion: Average loss: 7.2115, Accuracy: 93.3333%\n",
            "Epoch: 176\n",
            "training loss 0.3400\n",
            "Validtion: Average loss: 7.5228, Accuracy: 83.3333%\n",
            "Epoch: 177\n",
            "training loss 0.1234\n",
            "Validtion: Average loss: 8.8416, Accuracy: 93.3333%\n",
            "Epoch: 178\n",
            "training loss 0.2535\n",
            "Validtion: Average loss: 9.1261, Accuracy: 93.3333%\n",
            "Epoch: 179\n",
            "training loss 0.5041\n",
            "Validtion: Average loss: 6.8220, Accuracy: 96.6667%\n",
            "Epoch: 180\n",
            "training loss 0.0773\n",
            "Validtion: Average loss: 8.1848, Accuracy: 93.3333%\n",
            "Epoch: 181\n",
            "training loss 0.1720\n",
            "Validtion: Average loss: 9.6892, Accuracy: 93.3333%\n",
            "Epoch: 182\n",
            "training loss 0.2186\n",
            "Validtion: Average loss: 6.0552, Accuracy: 90.0000%\n",
            "Epoch: 183\n",
            "training loss 0.2129\n",
            "Validtion: Average loss: 6.8134, Accuracy: 86.6667%\n",
            "Epoch: 184\n",
            "training loss 0.0008\n",
            "Validtion: Average loss: 9.9076, Accuracy: 90.0000%\n",
            "Epoch: 185\n",
            "training loss 0.1741\n",
            "Validtion: Average loss: 8.4141, Accuracy: 86.6667%\n",
            "Epoch: 186\n",
            "training loss 0.0001\n",
            "Validtion: Average loss: 7.9454, Accuracy: 86.6667%\n",
            "Epoch: 187\n",
            "training loss 0.0041\n",
            "Validtion: Average loss: 6.6855, Accuracy: 90.0000%\n",
            "Epoch: 188\n",
            "training loss 0.0984\n",
            "Validtion: Average loss: 8.9558, Accuracy: 86.6667%\n",
            "Epoch: 189\n",
            "training loss 0.4358\n",
            "Validtion: Average loss: 6.6986, Accuracy: 96.6667%\n",
            "Epoch: 190\n",
            "training loss 0.2531\n",
            "Validtion: Average loss: 7.9622, Accuracy: 90.0000%\n",
            "Epoch: 191\n",
            "training loss 0.0444\n",
            "Validtion: Average loss: 6.8372, Accuracy: 90.0000%\n",
            "Epoch: 192\n",
            "training loss 0.1234\n",
            "Validtion: Average loss: 7.6878, Accuracy: 86.6667%\n",
            "Epoch: 193\n",
            "training loss 0.1023\n",
            "Validtion: Average loss: 9.9385, Accuracy: 90.0000%\n",
            "Epoch: 194\n",
            "training loss 0.2795\n",
            "Validtion: Average loss: 6.2462, Accuracy: 96.6667%\n",
            "Epoch: 195\n",
            "training loss 0.3931\n",
            "Validtion: Average loss: 9.3814, Accuracy: 93.3333%\n",
            "Epoch: 196\n",
            "training loss 0.2345\n",
            "Validtion: Average loss: 9.2720, Accuracy: 86.6667%\n",
            "Epoch: 197\n",
            "training loss 0.0669\n",
            "Validtion: Average loss: 8.6838, Accuracy: 86.6667%\n",
            "Epoch: 198\n",
            "training loss 0.0813\n",
            "Validtion: Average loss: 10.0427, Accuracy: 86.6667%\n",
            "Epoch: 199\n",
            "training loss 0.0002\n",
            "Validtion: Average loss: 8.1822, Accuracy: 96.6667%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAdnQ2b1cfdC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CADG04OOzD_P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Set of modules for aggregating embeddings of neighbors.\n",
        "\"\"\"\n",
        "\n",
        "class MeanAggregator(nn.Module):\n",
        "    \"\"\"\n",
        "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, features, cuda=False, gcn=False): \n",
        "        \"\"\"\n",
        "        Initializes the aggregator for a specific graph.\n",
        "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
        "        cuda -- whether to use GPU\n",
        "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
        "        \"\"\"\n",
        "\n",
        "        super(MeanAggregator, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.cuda = cuda\n",
        "        self.gcn = gcn\n",
        "        \n",
        "    def forward(self, nodes, to_neighs, num_sample=10):\n",
        "        \"\"\"\n",
        "        nodes --- list of nodes in a batch\n",
        "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
        "        num_sample --- number of neighbors to sample. No sampling if None.\n",
        "        \"\"\"\n",
        "        # Local pointers to functions (speed hack)\n",
        "        _set = set\n",
        "        if not num_sample is None:\n",
        "            _sample = random.sample\n",
        "            samp_neighs = [_set(_sample(to_neigh, \n",
        "                            num_sample,\n",
        "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
        "        else:\n",
        "            samp_neighs = to_neighs\n",
        "\n",
        "        if self.gcn:\n",
        "            samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
        "        \n",
        "        unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
        "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        mask[row_indices, column_indices] = 1\n",
        "        \n",
        "        if self.cuda:\n",
        "            mask = mask.cuda()\n",
        "        \n",
        "        num_neigh = mask.sum(1, keepdim=True)\n",
        "        mask = mask.div(num_neigh)\n",
        "        if self.cuda:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
        "        else:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
        "        to_feats = mask.mm(embed_matrix)\n",
        "        return to_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5hoUV24zpV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for GraphSAGE implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, features, feature_dim, \n",
        "            embed_dim, adj_lists, aggregator,\n",
        "            num_sample=10,\n",
        "            base_model=None, gcn=False, cuda=False, \n",
        "            feature_transform=False): \n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.feat_dim = feature_dim\n",
        "        self.adj_lists = adj_lists\n",
        "        self.aggregator = aggregator\n",
        "        self.num_sample = num_sample\n",
        "        if base_model != None:\n",
        "            self.base_model = base_model\n",
        "\n",
        "        self.gcn = gcn\n",
        "        self.embed_dim = embed_dim\n",
        "        self.cuda = cuda\n",
        "        self.aggregator.cuda = cuda\n",
        "        self.weight = nn.Parameter(\n",
        "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        \"\"\"\n",
        "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], \n",
        "                self.num_sample)\n",
        "        \n",
        "        if not self.gcn:\n",
        "            if self.cuda:\n",
        "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
        "            else:\n",
        "                self_feats = self.features(torch.LongTensor(nodes))\n",
        "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
        "        else:\n",
        "            combined = neigh_feats\n",
        "        combined = F.relu(self.weight.mm(combined.t()))\n",
        "        return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1aqCFXC43h-"
      },
      "source": [
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"Vanilla GCN Layer.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # multiply input by weight\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # multiply adjacency matrix by weighted product \n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class LPAConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    GCN LPA Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, adj, bias=True):\n",
        "        super(LPAConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        self.adjacency_mask = Parameter(adj.clone()).to_dense()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, adj, y):\n",
        "        adj = adj.to_dense()\n",
        "        \n",
        "        # W * x\n",
        "        support = torch.mm(x, self.weight)\n",
        "        \n",
        "        # Hadamard Product: A' = Hadamard(A, M)\n",
        "        adj = adj * self.adjacency_mask\n",
        "        \n",
        "        # Row-Normalize: D^-1 * (A')\n",
        "        adj = F.normalize(adj, p=1, dim=1)\n",
        "\n",
        "        # output = D^-1 * A' * X * W\n",
        "        output = torch.mm(adj, support)\n",
        "        \n",
        "        # y' = D^-1 * A' * y\n",
        "        y_hat = torch.mm(adj, y)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias, y_hat\n",
        "        else:\n",
        "            return output, y_hat\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO3BFiF75PlK"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.linalg import fractional_matrix_power as matrix_frac_power\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import networkx as nx\n",
        "\n",
        "def one_hot_embedding(labels, num_classes):\n",
        "    \"\"\"Embedding labels to one-hot form.\n",
        "    Args:\n",
        "      labels: (LongTensor) class labels, sized [N,].\n",
        "      num_classes: (int) number of classes.\n",
        "    Returns:\n",
        "      (tensor) encoded labels, sized [N, #classes].\n",
        "    \"\"\"\n",
        "    y = torch.eye(num_classes)\n",
        "    return y[labels]\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"Returns the accuracy given prediction outputs and target labels.\n",
        "\n",
        "    Args:\n",
        "        output: model predictions\n",
        "        labels: one hot encoded labels\n",
        "\n",
        "    Returns:\n",
        "        Accuracy metric of the model\n",
        "    \"\"\"\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def plot_stats(outdir, model_name, training_losses, val_losses):\n",
        "    e = len(training_losses)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, training_losses, label=\"Training Loss\")\n",
        "    plt.plot(x_axis, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(model_name + \" Loss Plot\")\n",
        "    if not os.path.exists('data/out'):\n",
        "        os.makedirs('data/out')\n",
        "    plt.savefig(\"{}_loss_plot.png\".format(outdir + model_name))\n",
        "    \n",
        "    \n",
        "def draw(outdir, outputname, adj, predictions, labels):\n",
        "    G = nx.from_numpy_matrix(adj.to_dense().detach().numpy(), nx.DiGraph())\n",
        "    plt.figure(figsize=(10,10))\n",
        "    nx.draw(G, node_size=10, edge_size=1,node_color=predictions.detach().numpy(), cmap = 'tab10')\n",
        "    plt.savefig(f\"{outdir}{outputname}.png\")\n",
        "\n",
        "        \n",
        "def plotGCN(model, adj, features, labels, outdir, outputname):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "    \n",
        "    \n",
        "def plotGCNLPA(model, adj, features, labels, outdir, outputname):\n",
        "    output, _ = model(features, adj, labels)\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "    \n",
        "    \n",
        "def plotGraphSAGE(model, adj, adj_lists, features, labels, outdir, outputname):\n",
        "    model.eval()\n",
        "    output = model(torch.LongTensor(range(labels.shape[0])))\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayDmJB0wzECe"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from models import *\n",
        "# from encoders import *\n",
        "# from aggregators import *\n",
        "# from utils import accuracy, one_hot_embedding, plot_stats\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from layers import *\n",
        "\n",
        "\n",
        "def train_GraphSAGE(model, optimizer, adj, features, labels, idx_train, idx_val, epoch, fastmode = False):\n",
        "\n",
        "    batch_nodes = idx_train\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss_train, train_output = model.loss(batch_nodes, \n",
        "            Variable(torch.LongTensor(labels[batch_nodes])))\n",
        "    \n",
        "    acc_train = accuracy_score(labels[batch_nodes], train_output.data.numpy().argmax(axis=1))    \n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    loss_val, val_output = model.loss(idx_val, Variable(torch.LongTensor(labels[idx_val])))\n",
        "    acc_val = accuracy_score(labels[idx_val], val_output.data.numpy().argmax(axis=1))\n",
        "    print('Epoch: {:04d}'.format(epoch + 1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - start_time))\n",
        "    \n",
        "    return loss_train.item(), loss_val.item() \n",
        "    \n",
        "def test_GraphSAGE(model, adj, features, labels, idx_test):\n",
        "    model.eval()\n",
        "    loss_test, test_output = model.loss(idx_test, Variable(torch.LongTensor(labels[idx_test])))\n",
        "    acc_test = accuracy_score(labels[idx_test], test_output.data.numpy().argmax(axis=1))\n",
        "    print(\"Test:\",\n",
        "          \"loss: {:.4f}\".format(loss_test.item()),\n",
        "          \"acc: {:.4f}\".format(acc_test.item()))\n",
        "    \n",
        "    \n",
        "def model_build(name, adj, features, labels, idx_train, idx_val, idx_test, outdir,\n",
        "                no_cuda = True, fastmode = False, seed = 42, epochs = 50, learning_rate = 0.1,\n",
        "                weight_decay = 5e-4, hidden = 16, dropout = 0.5, lpa_weight_decay = 0, sample1 = 25, sample2 = 10, gcn_mode=True):\n",
        "    \n",
        "    \"\"\"Builds and trains the model given data and model parameters.\"\"\"\n",
        "    \n",
        "    cuda = (not no_cuda) and torch.cuda.is_available()\n",
        "    if cuda and name == 'GraphSAGE': \n",
        "        print(\"CUDA not available for GraphSAGE...\")\n",
        "        cuda = False\n",
        "    \n",
        "    if cuda: print(\"Using CUDA...\")\n",
        "    \n",
        "    # set seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda: torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    # Set device\n",
        "    device = 'cuda' if cuda else 'cpu'\n",
        "    torch.device(device)\n",
        "    \n",
        "    # Load data\n",
        "    if name in ['GraphSAGE']:\n",
        "        n_feats = features.shape[1]\n",
        "        n_classes = len(np.unique(labels))\n",
        "        new_features = nn.Embedding(*features.shape)\n",
        "        new_features.weight = nn.Parameter(torch.FloatTensor(features), requires_grad=False)\n",
        "        features = new_features\n",
        "        \n",
        "    features = features.to(device)\n",
        "    if name in ['GCN', 'GCNLPA']:\n",
        "        adj = adj.to(device)\n",
        "    labels = labels.to(device)\n",
        "    idx_train = idx_train.to(device)\n",
        "    idx_val = idx_val.to(device)\n",
        "    idx_test = idx_test.to(device)\n",
        "    \n",
        "    lpa_labels = None\n",
        "    \n",
        "    # Create model\n",
        "        \n",
        "    \n",
        "    if name == \"GraphSAGE\": \n",
        "        \n",
        "        if not gcn_mode:\n",
        "            agg1 = MeanAggregator(features, cuda=False)\n",
        "            enc1 = Encoder(features, n_feats, hidden, adj, agg1, gcn=False, cuda=False)\n",
        "            agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
        "            enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj, agg2, base_model=enc1, gcn=False, cuda=False)\n",
        "        else:\n",
        "            agg1 = MeanAggregator(features, cuda=True)\n",
        "            enc1 = Encoder(features, n_feats, hidden, adj, agg1, gcn=True, cuda=False)\n",
        "            agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
        "            enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, hidden, adj, agg2,\n",
        "                    base_model=enc1, gcn=True, cuda=False)\n",
        "        \n",
        "        enc1.num_sample = sample1\n",
        "        enc2.num_sample = sample2\n",
        "        \n",
        "        model = SupervisedGraphSage(n_classes, enc2)\n",
        "        model.to(device)\n",
        "        \n",
        "        optimizer = optim.SGD(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate, weight_decay = weight_decay)\n",
        "        \n",
        "        total_time = time.time()\n",
        "        print(\"Training {} model for {} epochs...\".format(name, epochs))\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            train_loss, val_loss = train_GraphSAGE(model, optimizer, adj, features, labels, idx_train, idx_val, epoch = e, fastmode=fastmode)\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Finished. Total time elapsed: {:.4f}s\".format(time.time() - total_time))\n",
        "        \n",
        "        # plot_stats(outdir, \"GraphSAGE\", train_losses, val_losses)\n",
        "        \n",
        "        test_GraphSAGE(model, adj, features, labels, idx_test)\n",
        "        \n",
        "        return model\n",
        "class SupervisedGraphSage(nn.Module):\n",
        "    def __init__(self, nclass, enc):\n",
        "        super(SupervisedGraphSage, self).__init__()\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "        self.enc = enc\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(nclass, enc.embed_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.enc(nodes)\n",
        "        scores = self.weight.mm(embeds)\n",
        "        return scores.t()\n",
        "    \n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        if list(labels.size()) == [1]: # incase the batch only contains 1 row...\n",
        "            return self.xent(scores, labels), scores\n",
        "        return self.xent(scores, labels.squeeze()), scores\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z2kHCcEzEuw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from layers import *\n",
        "   \n",
        "    \n",
        "class SupervisedGraphSage(nn.Module):\n",
        "    def __init__(self, nclass, enc):\n",
        "        super(SupervisedGraphSage, self).__init__()\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "        self.enc = enc\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(nclass, enc.embed_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.enc(nodes)\n",
        "        scores = self.weight.mm(embeds)\n",
        "        return scores.t()\n",
        "    \n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        if list(labels.size()) == [1]: # incase the batch only contains 1 row...\n",
        "            return self.xent(scores, labels), scores\n",
        "        return self.xent(scores, labels.squeeze()), scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md5zxMx-7BWj"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import one_hot_embedding, normalize, sparse_mx_to_torch_sparse_tensor, encode_onehot\n",
        "from scipy.linalg import fractional_matrix_power as matrix_frac_power\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_data(outdir, path=\"\", dataset=\"cora\", nodefile=\"cora.content\", edgefile=\"cora.cites\", symm_norm=False, ratio_train = 0.05, ratio_val = 0.20, ratio_test = 0.40, shuffle = False, seed = 42):\n",
        "    \n",
        "    print('Loading {} dataset from {}...'.format(dataset, path))\n",
        "\n",
        "    print(\"Parsing node feature and label data...\")\n",
        "    # Load feature and label data into a numpy array\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}\".format(path, nodefile),\n",
        "                                        dtype=np.dtype(str))\n",
        "    \n",
        "    # Put features into a sparse matrix, exclude first and last columns\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    \n",
        "    feat_data = idx_features_labels[:, 1:-1].astype(float)\n",
        "    \n",
        "    # Onehot encode labels\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "        \n",
        "    # Get node id\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    \n",
        "    # Map index to node id\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    \n",
        "    print(\"Parsing edge data...\")\n",
        "    # Load edge data into a numpy array\n",
        "    edges_unordered = np.genfromtxt(\"{}{}\".format(path, edgefile),\n",
        "                                    dtype=np.int32)\n",
        "    \n",
        "    # Create list of edges encoded into indices using index map\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "\n",
        "    adj_lists = defaultdict(set)\n",
        "    for i in edges:\n",
        "        adj_lists[i[0]].add(i[1])\n",
        "        adj_lists[i[1]].add(i[0])\n",
        "    \n",
        "    # Create adjacency matrix\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # Build symmetric adjacency matrix    \n",
        "    if not symm_norm:\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    else:\n",
        "        # use the D^-1/2AD^-1/2 formula\n",
        "        adj = adj + sp.eye(adj.shape[0]) # add self loop\n",
        "        D = np.diag(np.array(adj.sum(axis=1)).flatten()) # build degree matrix\n",
        "        D_prime = matrix_frac_power(D,-0.5)\n",
        "        D_prime = sp.coo_matrix(D_prime, shape=(adj.shape[0],adj.shape[0]),dtype=np.float32) # convert to sparse format\n",
        "        adj = D_prime @ adj @ D_prime # compute the normalized symmetric version\n",
        "\n",
        "    # Normalize features\n",
        "    features = normalize(features)\n",
        "    \n",
        "    # save features \n",
        "    # if not os.path.exists(outdir):\n",
        "    #     os.makedirs(outdir)\n",
        "    # with open(outdir + 'features.npz', 'wb+') as fh:\n",
        "    #     sp.save_npz(fh, features, compressed=False)\n",
        "    # # save adj\n",
        "    # with open(outdir + 'adjmatrix.npz', 'wb+') as fh:\n",
        "    #     sp.save_npz(fh, adj, compressed=False)\n",
        "    # # save labels\n",
        "    # with open(outdir + 'labels.txt', 'w+') as fh:\n",
        "    #     np.savetxt(fh, labels)\n",
        "     \n",
        "    # Convert features, labels, and adjacency matrix to Tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    \n",
        "    idx_train, idx_val, idx_test = data_split(labels.size()[0], ratio_train = ratio_train, ratio_val = ratio_val, ratio_test = ratio_test, shuffle = shuffle, seed = seed)\n",
        "\n",
        "    return adj, adj_lists, features, feat_data, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def data_split(n, ratio_train = 0.05, ratio_val = 0.20, ratio_test = None, shuffle = False, seed = 42):\n",
        "    \"\"\"Splits n data into train, validation and test splits by indices.\"\"\"\n",
        "    \n",
        "    if ratio_test is None or ratio_test > (1 - ratio_train - ratio_val):\n",
        "        ratio_test = 1 - ratio_train - ratio_val\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.seed(seed)\n",
        "        indices = np.random.permutation(n)\n",
        "    else:\n",
        "        indices = range(n)\n",
        "    \n",
        "    # Train-Validation-Test Split\n",
        "    n_train = int(n * ratio_train)\n",
        "    n_val = int(n * ratio_val)\n",
        "    n_test = int(n * ratio_test)\n",
        "\n",
        "    idx_train = indices[:n_train]\n",
        "    idx_val = indices[n_train: n_train + n_val]\n",
        "    idx_test = indices[n_train + n_val: n_train + n_val + n_test]\n",
        "    \n",
        "    # Convert split indices to Tensors\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFGSuJVu7Fm3",
        "outputId": "4e9cf07f-ae51-4808-cb51-0369001b2ef7"
      },
      "source": [
        "adj, adj_lists, features, feat_data, labels, idx_train, idx_val, idx_test = load_data(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset from ...\n",
            "Parsing node feature and label data...\n",
            "Parsing edge data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ROoI_z8WiZ",
        "outputId": "5fb3f708-c511-4138-e394-496499d88e18"
      },
      "source": [
        "len(idx_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnsT6nSyzL0L",
        "outputId": "379f0cd5-7972-44dd-8a7c-d1a64e5526ed"
      },
      "source": [
        "GraphSAGE_model = model_build(\"GraphSAGE\", adj_lists, feat_data, labels, idx_train, idx_val, idx_test, outdir=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training GraphSAGE model for 50 epochs...\n",
            "Epoch: 0001 loss_train: 1.9419 acc_train: 0.1185 loss_val: 1.9359 acc_val: 0.1608 time: 0.2584s\n",
            "Epoch: 0002 loss_train: 1.9415 acc_train: 0.1037 loss_val: 1.9333 acc_val: 0.1682 time: 0.1429s\n",
            "Epoch: 0003 loss_train: 1.9388 acc_train: 0.1185 loss_val: 1.9310 acc_val: 0.1904 time: 0.2216s\n",
            "Epoch: 0004 loss_train: 1.9346 acc_train: 0.1704 loss_val: 1.9298 acc_val: 0.2384 time: 0.1416s\n",
            "Epoch: 0005 loss_train: 1.9323 acc_train: 0.2000 loss_val: 1.9281 acc_val: 0.2144 time: 0.1549s\n",
            "Epoch: 0006 loss_train: 1.9300 acc_train: 0.2074 loss_val: 1.9256 acc_val: 0.2754 time: 0.1502s\n",
            "Epoch: 0007 loss_train: 1.9282 acc_train: 0.2370 loss_val: 1.9249 acc_val: 0.2717 time: 0.1465s\n",
            "Epoch: 0008 loss_train: 1.9257 acc_train: 0.2889 loss_val: 1.9224 acc_val: 0.2994 time: 0.1464s\n",
            "Epoch: 0009 loss_train: 1.9221 acc_train: 0.3407 loss_val: 1.9187 acc_val: 0.3124 time: 0.1487s\n",
            "Epoch: 0010 loss_train: 1.9198 acc_train: 0.3778 loss_val: 1.9190 acc_val: 0.3142 time: 0.1612s\n",
            "Epoch: 0011 loss_train: 1.9166 acc_train: 0.3852 loss_val: 1.9149 acc_val: 0.3420 time: 0.1620s\n",
            "Epoch: 0012 loss_train: 1.9135 acc_train: 0.3926 loss_val: 1.9128 acc_val: 0.3530 time: 0.1551s\n",
            "Epoch: 0013 loss_train: 1.9105 acc_train: 0.3852 loss_val: 1.9117 acc_val: 0.3660 time: 0.1518s\n",
            "Epoch: 0014 loss_train: 1.9086 acc_train: 0.4074 loss_val: 1.9079 acc_val: 0.3734 time: 0.1608s\n",
            "Epoch: 0015 loss_train: 1.9038 acc_train: 0.3852 loss_val: 1.9058 acc_val: 0.3752 time: 0.1423s\n",
            "Epoch: 0016 loss_train: 1.9005 acc_train: 0.4074 loss_val: 1.9031 acc_val: 0.3771 time: 0.1417s\n",
            "Epoch: 0017 loss_train: 1.8968 acc_train: 0.3926 loss_val: 1.8985 acc_val: 0.3789 time: 0.1411s\n",
            "Epoch: 0018 loss_train: 1.8949 acc_train: 0.3926 loss_val: 1.8950 acc_val: 0.3808 time: 0.1505s\n",
            "Epoch: 0019 loss_train: 1.8892 acc_train: 0.3926 loss_val: 1.8911 acc_val: 0.3641 time: 0.1436s\n",
            "Epoch: 0020 loss_train: 1.8862 acc_train: 0.3778 loss_val: 1.8891 acc_val: 0.3678 time: 0.1415s\n",
            "Epoch: 0021 loss_train: 1.8845 acc_train: 0.3852 loss_val: 1.8835 acc_val: 0.3771 time: 0.1391s\n",
            "Epoch: 0022 loss_train: 1.8781 acc_train: 0.3926 loss_val: 1.8834 acc_val: 0.3623 time: 0.1395s\n",
            "Epoch: 0023 loss_train: 1.8735 acc_train: 0.3778 loss_val: 1.8768 acc_val: 0.3660 time: 0.1431s\n",
            "Epoch: 0024 loss_train: 1.8681 acc_train: 0.3704 loss_val: 1.8720 acc_val: 0.3604 time: 0.1412s\n",
            "Epoch: 0025 loss_train: 1.8646 acc_train: 0.3556 loss_val: 1.8701 acc_val: 0.3641 time: 0.1464s\n",
            "Epoch: 0026 loss_train: 1.8600 acc_train: 0.3556 loss_val: 1.8654 acc_val: 0.3586 time: 0.1412s\n",
            "Epoch: 0027 loss_train: 1.8515 acc_train: 0.3481 loss_val: 1.8582 acc_val: 0.3549 time: 0.1441s\n",
            "Epoch: 0028 loss_train: 1.8476 acc_train: 0.3481 loss_val: 1.8498 acc_val: 0.3623 time: 0.1514s\n",
            "Epoch: 0029 loss_train: 1.8419 acc_train: 0.3407 loss_val: 1.8477 acc_val: 0.3494 time: 0.1405s\n",
            "Epoch: 0030 loss_train: 1.8369 acc_train: 0.3407 loss_val: 1.8426 acc_val: 0.3530 time: 0.1423s\n",
            "Epoch: 0031 loss_train: 1.8290 acc_train: 0.3407 loss_val: 1.8355 acc_val: 0.3549 time: 0.1408s\n",
            "Epoch: 0032 loss_train: 1.8225 acc_train: 0.3333 loss_val: 1.8322 acc_val: 0.3512 time: 0.1434s\n",
            "Epoch: 0033 loss_train: 1.8164 acc_train: 0.3333 loss_val: 1.8254 acc_val: 0.3494 time: 0.1402s\n",
            "Epoch: 0034 loss_train: 1.8106 acc_train: 0.3333 loss_val: 1.8167 acc_val: 0.3475 time: 0.1385s\n",
            "Epoch: 0035 loss_train: 1.8024 acc_train: 0.3259 loss_val: 1.8114 acc_val: 0.3438 time: 0.1468s\n",
            "Epoch: 0036 loss_train: 1.7972 acc_train: 0.3259 loss_val: 1.8010 acc_val: 0.3457 time: 0.1391s\n",
            "Epoch: 0037 loss_train: 1.7861 acc_train: 0.3185 loss_val: 1.8018 acc_val: 0.3457 time: 0.1458s\n",
            "Epoch: 0038 loss_train: 1.7790 acc_train: 0.3111 loss_val: 1.7911 acc_val: 0.3457 time: 0.1387s\n",
            "Epoch: 0039 loss_train: 1.7722 acc_train: 0.3111 loss_val: 1.7851 acc_val: 0.3457 time: 0.1461s\n",
            "Epoch: 0040 loss_train: 1.7601 acc_train: 0.3111 loss_val: 1.7731 acc_val: 0.3457 time: 0.1417s\n",
            "Epoch: 0041 loss_train: 1.7523 acc_train: 0.3111 loss_val: 1.7691 acc_val: 0.3457 time: 0.1461s\n",
            "Epoch: 0042 loss_train: 1.7450 acc_train: 0.3111 loss_val: 1.7591 acc_val: 0.3457 time: 0.1395s\n",
            "Epoch: 0043 loss_train: 1.7360 acc_train: 0.3111 loss_val: 1.7493 acc_val: 0.3475 time: 0.1412s\n",
            "Epoch: 0044 loss_train: 1.7276 acc_train: 0.3111 loss_val: 1.7449 acc_val: 0.3475 time: 0.1451s\n",
            "Epoch: 0045 loss_train: 1.7170 acc_train: 0.3111 loss_val: 1.7345 acc_val: 0.3475 time: 0.1447s\n",
            "Epoch: 0046 loss_train: 1.7072 acc_train: 0.3111 loss_val: 1.7309 acc_val: 0.3475 time: 0.1420s\n",
            "Epoch: 0047 loss_train: 1.6986 acc_train: 0.3111 loss_val: 1.7204 acc_val: 0.3475 time: 0.1456s\n",
            "Epoch: 0048 loss_train: 1.6898 acc_train: 0.3111 loss_val: 1.7086 acc_val: 0.3475 time: 0.1433s\n",
            "Epoch: 0049 loss_train: 1.6807 acc_train: 0.3111 loss_val: 1.7004 acc_val: 0.3475 time: 0.1419s\n",
            "Epoch: 0050 loss_train: 1.6664 acc_train: 0.3111 loss_val: 1.6989 acc_val: 0.3475 time: 0.1436s\n",
            "Finished. Total time elapsed: 7.5047s\n",
            "Test: loss: 1.7293 acc: 0.2825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT782HS28zvf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}