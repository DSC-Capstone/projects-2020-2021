{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "graphsage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnuragPamuru/dsc-180b-capstone-b03/blob/main/graphsage_voting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cSjRs_tj8LO"
      },
      "source": [
        "from data_loader import data_loader\n",
        "from GraphSage import GraphSage"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8jh46TUj8LY"
      },
      "source": [
        "loader = data_loader(\"voting_features.csv\", \"edges.csv\")\n",
        "features, labels, A = loader.get_data()"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHyI24qHj8LZ",
        "outputId": "7a47987e-abdb-416c-8616-41112df99e68"
      },
      "source": [
        "model = GraphSage(A, features, labels, agg_func='MEAN', len_walk=15, num_neigh=15, F=79)\n",
        "acc = model.train_epoch(epochs=100, lr=1e-4)\n",
        "# acc['acc']"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train length :70, Validation length :30\n",
            "Epoch: 0\n",
            "training loss 1.6163\n",
            "Validtion: Average loss: 1.3406, Accuracy: 46.6667%\n",
            "Epoch: 1\n",
            "training loss 1.4221\n",
            "Validtion: Average loss: 1.1473, Accuracy: 53.3333%\n",
            "Epoch: 2\n",
            "training loss 1.2214\n",
            "Validtion: Average loss: 1.0059, Accuracy: 56.6667%\n",
            "Epoch: 3\n",
            "training loss 1.0629\n",
            "Validtion: Average loss: 0.9260, Accuracy: 73.3333%\n",
            "Epoch: 4\n",
            "training loss 0.9851\n",
            "Validtion: Average loss: 0.7793, Accuracy: 90.0000%\n",
            "Epoch: 5\n",
            "training loss 0.8535\n",
            "Validtion: Average loss: 0.7162, Accuracy: 90.0000%\n",
            "Epoch: 6\n",
            "training loss 0.7593\n",
            "Validtion: Average loss: 0.6509, Accuracy: 93.3333%\n",
            "Epoch: 7\n",
            "training loss 0.6751\n",
            "Validtion: Average loss: 0.5645, Accuracy: 96.6667%\n",
            "Epoch: 8\n",
            "training loss 0.6012\n",
            "Validtion: Average loss: 0.4819, Accuracy: 96.6667%\n",
            "Epoch: 9\n",
            "training loss 0.5273\n",
            "Validtion: Average loss: 0.4282, Accuracy: 96.6667%\n",
            "Epoch: 10\n",
            "training loss 0.4659\n",
            "Validtion: Average loss: 0.3896, Accuracy: 100.0000%\n",
            "Epoch: 11\n",
            "training loss 0.4204\n",
            "Validtion: Average loss: 0.3305, Accuracy: 100.0000%\n",
            "Epoch: 12\n",
            "training loss 0.3809\n",
            "Validtion: Average loss: 0.3114, Accuracy: 96.6667%\n",
            "Epoch: 13\n",
            "training loss 0.3492\n",
            "Validtion: Average loss: 0.2566, Accuracy: 100.0000%\n",
            "Epoch: 14\n",
            "training loss 0.3203\n",
            "Validtion: Average loss: 0.2404, Accuracy: 96.6667%\n",
            "Epoch: 15\n",
            "training loss 0.3036\n",
            "Validtion: Average loss: 0.2130, Accuracy: 100.0000%\n",
            "Epoch: 16\n",
            "training loss 0.2794\n",
            "Validtion: Average loss: 0.2140, Accuracy: 96.6667%\n",
            "Epoch: 17\n",
            "training loss 0.2433\n",
            "Validtion: Average loss: 0.1843, Accuracy: 100.0000%\n",
            "Epoch: 18\n",
            "training loss 0.2226\n",
            "Validtion: Average loss: 0.1640, Accuracy: 100.0000%\n",
            "Epoch: 19\n",
            "training loss 0.2077\n",
            "Validtion: Average loss: 0.1534, Accuracy: 100.0000%\n",
            "Epoch: 20\n",
            "training loss 0.2100\n",
            "Validtion: Average loss: 0.1312, Accuracy: 100.0000%\n",
            "Epoch: 21\n",
            "training loss 0.1897\n",
            "Validtion: Average loss: 0.1142, Accuracy: 100.0000%\n",
            "Epoch: 22\n",
            "training loss 0.1860\n",
            "Validtion: Average loss: 0.1072, Accuracy: 100.0000%\n",
            "Epoch: 23\n",
            "training loss 0.1787\n",
            "Validtion: Average loss: 0.1131, Accuracy: 96.6667%\n",
            "Epoch: 24\n",
            "training loss 0.1674\n",
            "Validtion: Average loss: 0.1006, Accuracy: 100.0000%\n",
            "Epoch: 25\n",
            "training loss 0.1637\n",
            "Validtion: Average loss: 0.0928, Accuracy: 100.0000%\n",
            "Epoch: 26\n",
            "training loss 0.1624\n",
            "Validtion: Average loss: 0.0956, Accuracy: 96.6667%\n",
            "Epoch: 27\n",
            "training loss 0.1484\n",
            "Validtion: Average loss: 0.0874, Accuracy: 100.0000%\n",
            "Epoch: 28\n",
            "training loss 0.1492\n",
            "Validtion: Average loss: 0.0892, Accuracy: 96.6667%\n",
            "Epoch: 29\n",
            "training loss 0.1447\n",
            "Validtion: Average loss: 0.0733, Accuracy: 100.0000%\n",
            "Epoch: 30\n",
            "training loss 0.1410\n",
            "Validtion: Average loss: 0.0778, Accuracy: 96.6667%\n",
            "Epoch: 31\n",
            "training loss 0.1317\n",
            "Validtion: Average loss: 0.0668, Accuracy: 100.0000%\n",
            "Epoch: 32\n",
            "training loss 0.1338\n",
            "Validtion: Average loss: 0.0729, Accuracy: 100.0000%\n",
            "Epoch: 33\n",
            "training loss 0.1335\n",
            "Validtion: Average loss: 0.0679, Accuracy: 100.0000%\n",
            "Epoch: 34\n",
            "training loss 0.1307\n",
            "Validtion: Average loss: 0.0649, Accuracy: 100.0000%\n",
            "Epoch: 35\n",
            "training loss 0.1314\n",
            "Validtion: Average loss: 0.0748, Accuracy: 96.6667%\n",
            "Epoch: 36\n",
            "training loss 0.1323\n",
            "Validtion: Average loss: 0.0566, Accuracy: 100.0000%\n",
            "Epoch: 37\n",
            "training loss 0.1296\n",
            "Validtion: Average loss: 0.0613, Accuracy: 100.0000%\n",
            "Epoch: 38\n",
            "training loss 0.1266\n",
            "Validtion: Average loss: 0.0480, Accuracy: 100.0000%\n",
            "Epoch: 39\n",
            "training loss 0.1292\n",
            "Validtion: Average loss: 0.0528, Accuracy: 100.0000%\n",
            "Epoch: 40\n",
            "training loss 0.1119\n",
            "Validtion: Average loss: 0.0584, Accuracy: 100.0000%\n",
            "Epoch: 41\n",
            "training loss 0.1139\n",
            "Validtion: Average loss: 0.0548, Accuracy: 100.0000%\n",
            "Epoch: 42\n",
            "training loss 0.1175\n",
            "Validtion: Average loss: 0.0521, Accuracy: 100.0000%\n",
            "Epoch: 43\n",
            "training loss 0.1146\n",
            "Validtion: Average loss: 0.0504, Accuracy: 100.0000%\n",
            "Epoch: 44\n",
            "training loss 0.1158\n",
            "Validtion: Average loss: 0.0540, Accuracy: 100.0000%\n",
            "Epoch: 45\n",
            "training loss 0.1138\n",
            "Validtion: Average loss: 0.0494, Accuracy: 100.0000%\n",
            "Epoch: 46\n",
            "training loss 0.1175\n",
            "Validtion: Average loss: 0.0448, Accuracy: 100.0000%\n",
            "Epoch: 47\n",
            "training loss 0.1076\n",
            "Validtion: Average loss: 0.0632, Accuracy: 96.6667%\n",
            "Epoch: 48\n",
            "training loss 0.1141\n",
            "Validtion: Average loss: 0.0492, Accuracy: 100.0000%\n",
            "Epoch: 49\n",
            "training loss 0.1185\n",
            "Validtion: Average loss: 0.0504, Accuracy: 100.0000%\n",
            "Epoch: 50\n",
            "training loss 0.1022\n",
            "Validtion: Average loss: 0.0499, Accuracy: 100.0000%\n",
            "Epoch: 51\n",
            "training loss 0.1055\n",
            "Validtion: Average loss: 0.0426, Accuracy: 100.0000%\n",
            "Epoch: 52\n",
            "training loss 0.1038\n",
            "Validtion: Average loss: 0.0499, Accuracy: 100.0000%\n",
            "Epoch: 53\n",
            "training loss 0.1093\n",
            "Validtion: Average loss: 0.0569, Accuracy: 96.6667%\n",
            "Epoch: 54\n",
            "training loss 0.1053\n",
            "Validtion: Average loss: 0.0441, Accuracy: 100.0000%\n",
            "Epoch: 55\n",
            "training loss 0.0981\n",
            "Validtion: Average loss: 0.0502, Accuracy: 100.0000%\n",
            "Epoch: 56\n",
            "training loss 0.1085\n",
            "Validtion: Average loss: 0.0443, Accuracy: 100.0000%\n",
            "Epoch: 57\n",
            "training loss 0.1054\n",
            "Validtion: Average loss: 0.0417, Accuracy: 100.0000%\n",
            "Epoch: 58\n",
            "training loss 0.0995\n",
            "Validtion: Average loss: 0.0501, Accuracy: 100.0000%\n",
            "Epoch: 59\n",
            "training loss 0.1019\n",
            "Validtion: Average loss: 0.0462, Accuracy: 100.0000%\n",
            "Epoch: 60\n",
            "training loss 0.0955\n",
            "Validtion: Average loss: 0.0468, Accuracy: 100.0000%\n",
            "Epoch: 61\n",
            "training loss 0.0982\n",
            "Validtion: Average loss: 0.0394, Accuracy: 100.0000%\n",
            "Epoch: 62\n",
            "training loss 0.0912\n",
            "Validtion: Average loss: 0.0455, Accuracy: 100.0000%\n",
            "Epoch: 63\n",
            "training loss 0.0932\n",
            "Validtion: Average loss: 0.0398, Accuracy: 100.0000%\n",
            "Epoch: 64\n",
            "training loss 0.0964\n",
            "Validtion: Average loss: 0.0430, Accuracy: 100.0000%\n",
            "Epoch: 65\n",
            "training loss 0.0968\n",
            "Validtion: Average loss: 0.0420, Accuracy: 100.0000%\n",
            "Epoch: 66\n",
            "training loss 0.0915\n",
            "Validtion: Average loss: 0.0398, Accuracy: 100.0000%\n",
            "Epoch: 67\n",
            "training loss 0.0950\n",
            "Validtion: Average loss: 0.0417, Accuracy: 100.0000%\n",
            "Epoch: 68\n",
            "training loss 0.0910\n",
            "Validtion: Average loss: 0.0439, Accuracy: 100.0000%\n",
            "Epoch: 69\n",
            "training loss 0.0877\n",
            "Validtion: Average loss: 0.0481, Accuracy: 100.0000%\n",
            "Epoch: 70\n",
            "training loss 0.0921\n",
            "Validtion: Average loss: 0.0431, Accuracy: 100.0000%\n",
            "Epoch: 71\n",
            "training loss 0.0925\n",
            "Validtion: Average loss: 0.0519, Accuracy: 100.0000%\n",
            "Epoch: 72\n",
            "training loss 0.0842\n",
            "Validtion: Average loss: 0.0478, Accuracy: 100.0000%\n",
            "Epoch: 73\n",
            "training loss 0.0928\n",
            "Validtion: Average loss: 0.0464, Accuracy: 100.0000%\n",
            "Epoch: 74\n",
            "training loss 0.0894\n",
            "Validtion: Average loss: 0.0439, Accuracy: 100.0000%\n",
            "Epoch: 75\n",
            "training loss 0.0845\n",
            "Validtion: Average loss: 0.0421, Accuracy: 100.0000%\n",
            "Epoch: 76\n",
            "training loss 0.0849\n",
            "Validtion: Average loss: 0.0395, Accuracy: 100.0000%\n",
            "Epoch: 77\n",
            "training loss 0.0899\n",
            "Validtion: Average loss: 0.0447, Accuracy: 100.0000%\n",
            "Epoch: 78\n",
            "training loss 0.0774\n",
            "Validtion: Average loss: 0.0438, Accuracy: 100.0000%\n",
            "Epoch: 79\n",
            "training loss 0.0848\n",
            "Validtion: Average loss: 0.0551, Accuracy: 96.6667%\n",
            "Epoch: 80\n",
            "training loss 0.0881\n",
            "Validtion: Average loss: 0.0493, Accuracy: 100.0000%\n",
            "Epoch: 81\n",
            "training loss 0.0882\n",
            "Validtion: Average loss: 0.0410, Accuracy: 100.0000%\n",
            "Epoch: 82\n",
            "training loss 0.0820\n",
            "Validtion: Average loss: 0.0407, Accuracy: 100.0000%\n",
            "Epoch: 83\n",
            "training loss 0.0872\n",
            "Validtion: Average loss: 0.0426, Accuracy: 100.0000%\n",
            "Epoch: 84\n",
            "training loss 0.0872\n",
            "Validtion: Average loss: 0.0437, Accuracy: 100.0000%\n",
            "Epoch: 85\n",
            "training loss 0.0865\n",
            "Validtion: Average loss: 0.0469, Accuracy: 100.0000%\n",
            "Epoch: 86\n",
            "training loss 0.0836\n",
            "Validtion: Average loss: 0.0387, Accuracy: 100.0000%\n",
            "Epoch: 87\n",
            "training loss 0.0834\n",
            "Validtion: Average loss: 0.0404, Accuracy: 100.0000%\n",
            "Epoch: 88\n",
            "training loss 0.0832\n",
            "Validtion: Average loss: 0.0390, Accuracy: 100.0000%\n",
            "Epoch: 89\n",
            "training loss 0.0710\n",
            "Validtion: Average loss: 0.0439, Accuracy: 100.0000%\n",
            "Epoch: 90\n",
            "training loss 0.0770\n",
            "Validtion: Average loss: 0.0410, Accuracy: 100.0000%\n",
            "Epoch: 91\n",
            "training loss 0.0763\n",
            "Validtion: Average loss: 0.0433, Accuracy: 100.0000%\n",
            "Epoch: 92\n",
            "training loss 0.0862\n",
            "Validtion: Average loss: 0.0452, Accuracy: 100.0000%\n",
            "Epoch: 93\n",
            "training loss 0.0830\n",
            "Validtion: Average loss: 0.0383, Accuracy: 100.0000%\n",
            "Epoch: 94\n",
            "training loss 0.0778\n",
            "Validtion: Average loss: 0.0420, Accuracy: 100.0000%\n",
            "Epoch: 95\n",
            "training loss 0.0682\n",
            "Validtion: Average loss: 0.0422, Accuracy: 100.0000%\n",
            "Epoch: 96\n",
            "training loss 0.0768\n",
            "Validtion: Average loss: 0.0381, Accuracy: 100.0000%\n",
            "Epoch: 97\n",
            "training loss 0.0720\n",
            "Validtion: Average loss: 0.0388, Accuracy: 100.0000%\n",
            "Epoch: 98\n",
            "training loss 0.0727\n",
            "Validtion: Average loss: 0.0337, Accuracy: 100.0000%\n",
            "Epoch: 99\n",
            "training loss 0.0733\n",
            "Validtion: Average loss: 0.0324, Accuracy: 100.0000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAdnQ2b1cfdC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CADG04OOzD_P"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import random\n",
        "\n",
        "\"\"\"\n",
        "Set of modules for aggregating embeddings of neighbors.\n",
        "\"\"\"\n",
        "\n",
        "class MeanAggregator(nn.Module):\n",
        "    \"\"\"\n",
        "    Aggregates a node's embeddings using mean of neighbors' embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, features, cuda=False, gcn=False): \n",
        "        \"\"\"\n",
        "        Initializes the aggregator for a specific graph.\n",
        "        features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
        "        cuda -- whether to use GPU\n",
        "        gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
        "        \"\"\"\n",
        "\n",
        "        super(MeanAggregator, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.cuda = cuda\n",
        "        self.gcn = gcn\n",
        "        \n",
        "    def forward(self, nodes, to_neighs, num_sample=10):\n",
        "        \"\"\"\n",
        "        nodes --- list of nodes in a batch\n",
        "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
        "        num_sample --- number of neighbors to sample. No sampling if None.\n",
        "        \"\"\"\n",
        "        # Local pointers to functions (speed hack)\n",
        "        _set = set\n",
        "        if not num_sample is None:\n",
        "            _sample = random.sample\n",
        "            samp_neighs = [_set(_sample(to_neigh, \n",
        "                            num_sample,\n",
        "                            )) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
        "        else:\n",
        "            samp_neighs = to_neighs\n",
        "\n",
        "        if self.gcn:\n",
        "            samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
        "        \n",
        "        unique_nodes_list = list(set.union(*samp_neighs))\n",
        "        unique_nodes = {n:i for i,n in enumerate(unique_nodes_list)}\n",
        "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
        "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
        "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
        "        mask[row_indices, column_indices] = 1\n",
        "        \n",
        "        if self.cuda:\n",
        "            mask = mask.cuda()\n",
        "        \n",
        "        num_neigh = mask.sum(1, keepdim=True)\n",
        "        mask = mask.div(num_neigh)\n",
        "        if self.cuda:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
        "        else:\n",
        "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
        "        to_feats = mask.mm(embed_matrix)\n",
        "        return to_feats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU5hoUV24zpV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder for GraphSAGE implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, features, feature_dim, \n",
        "            embed_dim, adj_lists, aggregator,\n",
        "            num_sample=10,\n",
        "            base_model=None, gcn=False, cuda=False, \n",
        "            feature_transform=False): \n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.features = features\n",
        "        self.feat_dim = feature_dim\n",
        "        self.adj_lists = adj_lists\n",
        "        self.aggregator = aggregator\n",
        "        self.num_sample = num_sample\n",
        "        if base_model != None:\n",
        "            self.base_model = base_model\n",
        "\n",
        "        self.gcn = gcn\n",
        "        self.embed_dim = embed_dim\n",
        "        self.cuda = cuda\n",
        "        self.aggregator.cuda = cuda\n",
        "        self.weight = nn.Parameter(\n",
        "                torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        \"\"\"\n",
        "        Generates embeddings for a batch of nodes.\n",
        "        \"\"\"\n",
        "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], \n",
        "                self.num_sample)\n",
        "        \n",
        "        if not self.gcn:\n",
        "            if self.cuda:\n",
        "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
        "            else:\n",
        "                self_feats = self.features(torch.LongTensor(nodes))\n",
        "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
        "        else:\n",
        "            combined = neigh_feats\n",
        "        combined = F.relu(self.weight.mm(combined.t()))\n",
        "        return combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1aqCFXC43h-"
      },
      "source": [
        "from math import sqrt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class GraphConvolution(nn.Module):\n",
        "    \"\"\"Vanilla GCN Layer.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # multiply input by weight\n",
        "        support = torch.mm(input, self.weight)\n",
        "        # multiply adjacency matrix by weighted product \n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "\n",
        "class LPAConvolution(nn.Module):\n",
        "    \"\"\"\n",
        "    GCN LPA Layer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, adj, bias=True):\n",
        "        super(LPAConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "        \n",
        "        self.adjacency_mask = Parameter(adj.clone()).to_dense()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x, adj, y):\n",
        "        adj = adj.to_dense()\n",
        "        \n",
        "        # W * x\n",
        "        support = torch.mm(x, self.weight)\n",
        "        \n",
        "        # Hadamard Product: A' = Hadamard(A, M)\n",
        "        adj = adj * self.adjacency_mask\n",
        "        \n",
        "        # Row-Normalize: D^-1 * (A')\n",
        "        adj = F.normalize(adj, p=1, dim=1)\n",
        "\n",
        "        # output = D^-1 * A' * X * W\n",
        "        output = torch.mm(adj, support)\n",
        "        \n",
        "        # y' = D^-1 * A' * y\n",
        "        y_hat = torch.mm(adj, y)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias, y_hat\n",
        "        else:\n",
        "            return output, y_hat\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO3BFiF75PlK"
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "from scipy.linalg import fractional_matrix_power as matrix_frac_power\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import networkx as nx\n",
        "\n",
        "def one_hot_embedding(labels, num_classes):\n",
        "    \"\"\"Embedding labels to one-hot form.\n",
        "    Args:\n",
        "      labels: (LongTensor) class labels, sized [N,].\n",
        "      num_classes: (int) number of classes.\n",
        "    Returns:\n",
        "      (tensor) encoded labels, sized [N, #classes].\n",
        "    \"\"\"\n",
        "    y = torch.eye(num_classes)\n",
        "    return y[labels]\n",
        "\n",
        "\n",
        "def encode_onehot(labels):\n",
        "    classes = set(labels)\n",
        "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
        "                    enumerate(classes)}\n",
        "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
        "                             dtype=np.int32)\n",
        "    return labels_onehot\n",
        "\n",
        "\n",
        "def normalize(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    \"\"\"Returns the accuracy given prediction outputs and target labels.\n",
        "\n",
        "    Args:\n",
        "        output: model predictions\n",
        "        labels: one hot encoded labels\n",
        "\n",
        "    Returns:\n",
        "        Accuracy metric of the model\n",
        "    \"\"\"\n",
        "    preds = output.max(1)[1].type_as(labels)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def plot_stats(outdir, model_name, training_losses, val_losses):\n",
        "    e = len(training_losses)\n",
        "    x_axis = np.arange(1, e + 1, 1)\n",
        "    plt.figure()\n",
        "    plt.plot(x_axis, training_losses, label=\"Training Loss\")\n",
        "    plt.plot(x_axis, val_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.title(model_name + \" Loss Plot\")\n",
        "    if not os.path.exists('data/out'):\n",
        "        os.makedirs('data/out')\n",
        "    plt.savefig(\"{}_loss_plot.png\".format(outdir + model_name))\n",
        "    \n",
        "    \n",
        "def draw(outdir, outputname, adj, predictions, labels):\n",
        "    G = nx.from_numpy_matrix(adj.to_dense().detach().numpy(), nx.DiGraph())\n",
        "    plt.figure(figsize=(10,10))\n",
        "    nx.draw(G, node_size=10, edge_size=1,node_color=predictions.detach().numpy(), cmap = 'tab10')\n",
        "    plt.savefig(f\"{outdir}{outputname}.png\")\n",
        "\n",
        "        \n",
        "def plotGCN(model, adj, features, labels, outdir, outputname):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "    \n",
        "    \n",
        "def plotGCNLPA(model, adj, features, labels, outdir, outputname):\n",
        "    output, _ = model(features, adj, labels)\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "    \n",
        "    \n",
        "def plotGraphSAGE(model, adj, adj_lists, features, labels, outdir, outputname):\n",
        "    model.eval()\n",
        "    output = model(torch.LongTensor(range(labels.shape[0])))\n",
        "    predictions = output.argmax(dim = 1)\n",
        "    draw(outdir, outputname, adj, predictions, labels)\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayDmJB0wzECe"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# from models import *\n",
        "# from encoders import *\n",
        "# from aggregators import *\n",
        "# from utils import accuracy, one_hot_embedding, plot_stats\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from layers import *\n",
        "\n",
        "\n",
        "def train_GraphSAGE(model, optimizer, adj, features, labels, idx_train, idx_val, epoch, fastmode = False):\n",
        "\n",
        "    batch_nodes = idx_train\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss_train, train_output = model.loss(batch_nodes, \n",
        "            Variable(torch.LongTensor(labels[batch_nodes])))\n",
        "    \n",
        "    acc_train = accuracy_score(labels[batch_nodes], train_output.data.numpy().argmax(axis=1))    \n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    loss_val, val_output = model.loss(idx_val, Variable(torch.LongTensor(labels[idx_val])))\n",
        "    acc_val = accuracy_score(labels[idx_val], val_output.data.numpy().argmax(axis=1))\n",
        "    print('Epoch: {:04d}'.format(epoch + 1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - start_time))\n",
        "    \n",
        "    return loss_train.item(), loss_val.item() \n",
        "    \n",
        "def test_GraphSAGE(model, adj, features, labels, idx_test):\n",
        "    model.eval()\n",
        "    loss_test, test_output = model.loss(idx_test, Variable(torch.LongTensor(labels[idx_test])))\n",
        "    acc_test = accuracy_score(labels[idx_test], test_output.data.numpy().argmax(axis=1))\n",
        "    print(\"Test:\",\n",
        "          \"loss: {:.4f}\".format(loss_test.item()),\n",
        "          \"acc: {:.4f}\".format(acc_test.item()))\n",
        "    \n",
        "    \n",
        "def model_build(name, adj, features, labels, idx_train, idx_val, idx_test, outdir,\n",
        "                no_cuda = True, fastmode = False, seed = 42, epochs = 50, learning_rate = 0.1,\n",
        "                weight_decay = 5e-4, hidden = 16, dropout = 0.5, lpa_weight_decay = 0, sample1 = 25, sample2 = 10, gcn_mode=True):\n",
        "    \n",
        "    \"\"\"Builds and trains the model given data and model parameters.\"\"\"\n",
        "    \n",
        "    cuda = (not no_cuda) and torch.cuda.is_available()\n",
        "    if cuda and name == 'GraphSAGE': \n",
        "        print(\"CUDA not available for GraphSAGE...\")\n",
        "        cuda = False\n",
        "    \n",
        "    if cuda: print(\"Using CUDA...\")\n",
        "    \n",
        "    # set seed\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda: torch.cuda.manual_seed(seed)\n",
        "    \n",
        "    # Set device\n",
        "    device = 'cuda' if cuda else 'cpu'\n",
        "    torch.device(device)\n",
        "    \n",
        "    # Load data\n",
        "    if name in ['GraphSAGE']:\n",
        "        n_feats = features.shape[1]\n",
        "        n_classes = len(np.unique(labels))\n",
        "        new_features = nn.Embedding(*features.shape)\n",
        "        new_features.weight = nn.Parameter(torch.FloatTensor(features), requires_grad=False)\n",
        "        features = new_features\n",
        "        \n",
        "    features = features.to(device)\n",
        "    if name in ['GCN', 'GCNLPA']:\n",
        "        adj = adj.to(device)\n",
        "    labels = labels.to(device)\n",
        "    idx_train = idx_train.to(device)\n",
        "    idx_val = idx_val.to(device)\n",
        "    idx_test = idx_test.to(device)\n",
        "    \n",
        "    lpa_labels = None\n",
        "    \n",
        "    # Create model\n",
        "        \n",
        "    \n",
        "    if name == \"GraphSAGE\": \n",
        "        \n",
        "        if not gcn_mode:\n",
        "            agg1 = MeanAggregator(features, cuda=False)\n",
        "            enc1 = Encoder(features, n_feats, hidden, adj, agg1, gcn=False, cuda=False)\n",
        "            agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
        "            enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj, agg2, base_model=enc1, gcn=False, cuda=False)\n",
        "        else:\n",
        "            agg1 = MeanAggregator(features, cuda=True)\n",
        "            enc1 = Encoder(features, n_feats, hidden, adj, agg1, gcn=True, cuda=False)\n",
        "            agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
        "            enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, hidden, adj, agg2,\n",
        "                    base_model=enc1, gcn=True, cuda=False)\n",
        "        \n",
        "        enc1.num_sample = sample1\n",
        "        enc2.num_sample = sample2\n",
        "        \n",
        "        model = SupervisedGraphSage(n_classes, enc2)\n",
        "        model.to(device)\n",
        "        \n",
        "        optimizer = optim.SGD(filter(lambda p : p.requires_grad, model.parameters()), lr = learning_rate, weight_decay = weight_decay)\n",
        "        \n",
        "        total_time = time.time()\n",
        "        print(\"Training {} model for {} epochs...\".format(name, epochs))\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        \n",
        "        for e in range(epochs):\n",
        "            train_loss, val_loss = train_GraphSAGE(model, optimizer, adj, features, labels, idx_train, idx_val, epoch = e, fastmode=fastmode)\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "        print(\"Finished. Total time elapsed: {:.4f}s\".format(time.time() - total_time))\n",
        "        \n",
        "        # plot_stats(outdir, \"GraphSAGE\", train_losses, val_losses)\n",
        "        \n",
        "        test_GraphSAGE(model, adj, features, labels, idx_test)\n",
        "        \n",
        "        return model\n",
        "class SupervisedGraphSage(nn.Module):\n",
        "    def __init__(self, nclass, enc):\n",
        "        super(SupervisedGraphSage, self).__init__()\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "        self.enc = enc\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(nclass, enc.embed_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.enc(nodes)\n",
        "        scores = self.weight.mm(embeds)\n",
        "        return scores.t()\n",
        "    \n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        if list(labels.size()) == [1]: # incase the batch only contains 1 row...\n",
        "            return self.xent(scores, labels), scores\n",
        "        return self.xent(scores, labels.squeeze()), scores\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z2kHCcEzEuw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# from layers import *\n",
        "   \n",
        "    \n",
        "class SupervisedGraphSage(nn.Module):\n",
        "    def __init__(self, nclass, enc):\n",
        "        super(SupervisedGraphSage, self).__init__()\n",
        "        self.xent = nn.CrossEntropyLoss()\n",
        "        self.enc = enc\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(nclass, enc.embed_dim))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, nodes):\n",
        "        embeds = self.enc(nodes)\n",
        "        scores = self.weight.mm(embeds)\n",
        "        return scores.t()\n",
        "    \n",
        "    def loss(self, nodes, labels):\n",
        "        scores = self.forward(nodes)\n",
        "        if list(labels.size()) == [1]: # incase the batch only contains 1 row...\n",
        "            return self.xent(scores, labels), scores\n",
        "        return self.xent(scores, labels.squeeze()), scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md5zxMx-7BWj"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from utils import one_hot_embedding, normalize, sparse_mx_to_torch_sparse_tensor, encode_onehot\n",
        "from scipy.linalg import fractional_matrix_power as matrix_frac_power\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_data(outdir, path=\"\", dataset=\"cora\", nodefile=\"cora.content\", edgefile=\"cora.cites\", symm_norm=False, ratio_train = 0.05, ratio_val = 0.20, ratio_test = 0.40, shuffle = False, seed = 42):\n",
        "    \n",
        "    print('Loading {} dataset from {}...'.format(dataset, path))\n",
        "\n",
        "    print(\"Parsing node feature and label data...\")\n",
        "    # Load feature and label data into a numpy array\n",
        "    idx_features_labels = np.genfromtxt(\"{}{}\".format(path, nodefile),\n",
        "                                        dtype=np.dtype(str))\n",
        "    \n",
        "    # Put features into a sparse matrix, exclude first and last columns\n",
        "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
        "    \n",
        "    feat_data = idx_features_labels[:, 1:-1].astype(float)\n",
        "    \n",
        "    # Onehot encode labels\n",
        "    labels = encode_onehot(idx_features_labels[:, -1])\n",
        "        \n",
        "    # Get node id\n",
        "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
        "    \n",
        "    # Map index to node id\n",
        "    idx_map = {j: i for i, j in enumerate(idx)}\n",
        "    \n",
        "    print(\"Parsing edge data...\")\n",
        "    # Load edge data into a numpy array\n",
        "    edges_unordered = np.genfromtxt(\"{}{}\".format(path, edgefile),\n",
        "                                    dtype=np.int32)\n",
        "    \n",
        "    # Create list of edges encoded into indices using index map\n",
        "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
        "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
        "\n",
        "    adj_lists = defaultdict(set)\n",
        "    for i in edges:\n",
        "        adj_lists[i[0]].add(i[1])\n",
        "        adj_lists[i[1]].add(i[0])\n",
        "    \n",
        "    # Create adjacency matrix\n",
        "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
        "                        shape=(labels.shape[0], labels.shape[0]),\n",
        "                        dtype=np.float32)\n",
        "\n",
        "    # Build symmetric adjacency matrix    \n",
        "    if not symm_norm:\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        adj = normalize(adj + sp.eye(adj.shape[0]))\n",
        "    else:\n",
        "        # use the D^-1/2AD^-1/2 formula\n",
        "        adj = adj + sp.eye(adj.shape[0]) # add self loop\n",
        "        D = np.diag(np.array(adj.sum(axis=1)).flatten()) # build degree matrix\n",
        "        D_prime = matrix_frac_power(D,-0.5)\n",
        "        D_prime = sp.coo_matrix(D_prime, shape=(adj.shape[0],adj.shape[0]),dtype=np.float32) # convert to sparse format\n",
        "        adj = D_prime @ adj @ D_prime # compute the normalized symmetric version\n",
        "\n",
        "    # Normalize features\n",
        "    features = normalize(features)\n",
        "    \n",
        "    # save features \n",
        "    # if not os.path.exists(outdir):\n",
        "    #     os.makedirs(outdir)\n",
        "    # with open(outdir + 'features.npz', 'wb+') as fh:\n",
        "    #     sp.save_npz(fh, features, compressed=False)\n",
        "    # # save adj\n",
        "    # with open(outdir + 'adjmatrix.npz', 'wb+') as fh:\n",
        "    #     sp.save_npz(fh, adj, compressed=False)\n",
        "    # # save labels\n",
        "    # with open(outdir + 'labels.txt', 'w+') as fh:\n",
        "    #     np.savetxt(fh, labels)\n",
        "     \n",
        "    # Convert features, labels, and adjacency matrix to Tensors\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(np.where(labels)[1])\n",
        "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "    \n",
        "    idx_train, idx_val, idx_test = data_split(labels.size()[0], ratio_train = ratio_train, ratio_val = ratio_val, ratio_test = ratio_test, shuffle = shuffle, seed = seed)\n",
        "\n",
        "    return adj, adj_lists, features, feat_data, labels, idx_train, idx_val, idx_test\n",
        "\n",
        "def data_split(n, ratio_train = 0.05, ratio_val = 0.20, ratio_test = None, shuffle = False, seed = 42):\n",
        "    \"\"\"Splits n data into train, validation and test splits by indices.\"\"\"\n",
        "    \n",
        "    if ratio_test is None or ratio_test > (1 - ratio_train - ratio_val):\n",
        "        ratio_test = 1 - ratio_train - ratio_val\n",
        "    \n",
        "    if shuffle:\n",
        "        np.random.seed(seed)\n",
        "        indices = np.random.permutation(n)\n",
        "    else:\n",
        "        indices = range(n)\n",
        "    \n",
        "    # Train-Validation-Test Split\n",
        "    n_train = int(n * ratio_train)\n",
        "    n_val = int(n * ratio_val)\n",
        "    n_test = int(n * ratio_test)\n",
        "\n",
        "    idx_train = indices[:n_train]\n",
        "    idx_val = indices[n_train: n_train + n_val]\n",
        "    idx_test = indices[n_train + n_val: n_train + n_val + n_test]\n",
        "    \n",
        "    # Convert split indices to Tensors\n",
        "    idx_train = torch.LongTensor(idx_train)\n",
        "    idx_val = torch.LongTensor(idx_val)\n",
        "    idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFGSuJVu7Fm3",
        "outputId": "4e9cf07f-ae51-4808-cb51-0369001b2ef7"
      },
      "source": [
        "adj, adj_lists, features, feat_data, labels, idx_train, idx_val, idx_test = load_data(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cora dataset from ...\n",
            "Parsing node feature and label data...\n",
            "Parsing edge data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9ROoI_z8WiZ",
        "outputId": "5fb3f708-c511-4138-e394-496499d88e18"
      },
      "source": [
        "len(idx_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnsT6nSyzL0L",
        "outputId": "379f0cd5-7972-44dd-8a7c-d1a64e5526ed"
      },
      "source": [
        "GraphSAGE_model = model_build(\"GraphSAGE\", adj_lists, feat_data, labels, idx_train, idx_val, idx_test, outdir=\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training GraphSAGE model for 50 epochs...\n",
            "Epoch: 0001 loss_train: 1.9419 acc_train: 0.1185 loss_val: 1.9359 acc_val: 0.1608 time: 0.2584s\n",
            "Epoch: 0002 loss_train: 1.9415 acc_train: 0.1037 loss_val: 1.9333 acc_val: 0.1682 time: 0.1429s\n",
            "Epoch: 0003 loss_train: 1.9388 acc_train: 0.1185 loss_val: 1.9310 acc_val: 0.1904 time: 0.2216s\n",
            "Epoch: 0004 loss_train: 1.9346 acc_train: 0.1704 loss_val: 1.9298 acc_val: 0.2384 time: 0.1416s\n",
            "Epoch: 0005 loss_train: 1.9323 acc_train: 0.2000 loss_val: 1.9281 acc_val: 0.2144 time: 0.1549s\n",
            "Epoch: 0006 loss_train: 1.9300 acc_train: 0.2074 loss_val: 1.9256 acc_val: 0.2754 time: 0.1502s\n",
            "Epoch: 0007 loss_train: 1.9282 acc_train: 0.2370 loss_val: 1.9249 acc_val: 0.2717 time: 0.1465s\n",
            "Epoch: 0008 loss_train: 1.9257 acc_train: 0.2889 loss_val: 1.9224 acc_val: 0.2994 time: 0.1464s\n",
            "Epoch: 0009 loss_train: 1.9221 acc_train: 0.3407 loss_val: 1.9187 acc_val: 0.3124 time: 0.1487s\n",
            "Epoch: 0010 loss_train: 1.9198 acc_train: 0.3778 loss_val: 1.9190 acc_val: 0.3142 time: 0.1612s\n",
            "Epoch: 0011 loss_train: 1.9166 acc_train: 0.3852 loss_val: 1.9149 acc_val: 0.3420 time: 0.1620s\n",
            "Epoch: 0012 loss_train: 1.9135 acc_train: 0.3926 loss_val: 1.9128 acc_val: 0.3530 time: 0.1551s\n",
            "Epoch: 0013 loss_train: 1.9105 acc_train: 0.3852 loss_val: 1.9117 acc_val: 0.3660 time: 0.1518s\n",
            "Epoch: 0014 loss_train: 1.9086 acc_train: 0.4074 loss_val: 1.9079 acc_val: 0.3734 time: 0.1608s\n",
            "Epoch: 0015 loss_train: 1.9038 acc_train: 0.3852 loss_val: 1.9058 acc_val: 0.3752 time: 0.1423s\n",
            "Epoch: 0016 loss_train: 1.9005 acc_train: 0.4074 loss_val: 1.9031 acc_val: 0.3771 time: 0.1417s\n",
            "Epoch: 0017 loss_train: 1.8968 acc_train: 0.3926 loss_val: 1.8985 acc_val: 0.3789 time: 0.1411s\n",
            "Epoch: 0018 loss_train: 1.8949 acc_train: 0.3926 loss_val: 1.8950 acc_val: 0.3808 time: 0.1505s\n",
            "Epoch: 0019 loss_train: 1.8892 acc_train: 0.3926 loss_val: 1.8911 acc_val: 0.3641 time: 0.1436s\n",
            "Epoch: 0020 loss_train: 1.8862 acc_train: 0.3778 loss_val: 1.8891 acc_val: 0.3678 time: 0.1415s\n",
            "Epoch: 0021 loss_train: 1.8845 acc_train: 0.3852 loss_val: 1.8835 acc_val: 0.3771 time: 0.1391s\n",
            "Epoch: 0022 loss_train: 1.8781 acc_train: 0.3926 loss_val: 1.8834 acc_val: 0.3623 time: 0.1395s\n",
            "Epoch: 0023 loss_train: 1.8735 acc_train: 0.3778 loss_val: 1.8768 acc_val: 0.3660 time: 0.1431s\n",
            "Epoch: 0024 loss_train: 1.8681 acc_train: 0.3704 loss_val: 1.8720 acc_val: 0.3604 time: 0.1412s\n",
            "Epoch: 0025 loss_train: 1.8646 acc_train: 0.3556 loss_val: 1.8701 acc_val: 0.3641 time: 0.1464s\n",
            "Epoch: 0026 loss_train: 1.8600 acc_train: 0.3556 loss_val: 1.8654 acc_val: 0.3586 time: 0.1412s\n",
            "Epoch: 0027 loss_train: 1.8515 acc_train: 0.3481 loss_val: 1.8582 acc_val: 0.3549 time: 0.1441s\n",
            "Epoch: 0028 loss_train: 1.8476 acc_train: 0.3481 loss_val: 1.8498 acc_val: 0.3623 time: 0.1514s\n",
            "Epoch: 0029 loss_train: 1.8419 acc_train: 0.3407 loss_val: 1.8477 acc_val: 0.3494 time: 0.1405s\n",
            "Epoch: 0030 loss_train: 1.8369 acc_train: 0.3407 loss_val: 1.8426 acc_val: 0.3530 time: 0.1423s\n",
            "Epoch: 0031 loss_train: 1.8290 acc_train: 0.3407 loss_val: 1.8355 acc_val: 0.3549 time: 0.1408s\n",
            "Epoch: 0032 loss_train: 1.8225 acc_train: 0.3333 loss_val: 1.8322 acc_val: 0.3512 time: 0.1434s\n",
            "Epoch: 0033 loss_train: 1.8164 acc_train: 0.3333 loss_val: 1.8254 acc_val: 0.3494 time: 0.1402s\n",
            "Epoch: 0034 loss_train: 1.8106 acc_train: 0.3333 loss_val: 1.8167 acc_val: 0.3475 time: 0.1385s\n",
            "Epoch: 0035 loss_train: 1.8024 acc_train: 0.3259 loss_val: 1.8114 acc_val: 0.3438 time: 0.1468s\n",
            "Epoch: 0036 loss_train: 1.7972 acc_train: 0.3259 loss_val: 1.8010 acc_val: 0.3457 time: 0.1391s\n",
            "Epoch: 0037 loss_train: 1.7861 acc_train: 0.3185 loss_val: 1.8018 acc_val: 0.3457 time: 0.1458s\n",
            "Epoch: 0038 loss_train: 1.7790 acc_train: 0.3111 loss_val: 1.7911 acc_val: 0.3457 time: 0.1387s\n",
            "Epoch: 0039 loss_train: 1.7722 acc_train: 0.3111 loss_val: 1.7851 acc_val: 0.3457 time: 0.1461s\n",
            "Epoch: 0040 loss_train: 1.7601 acc_train: 0.3111 loss_val: 1.7731 acc_val: 0.3457 time: 0.1417s\n",
            "Epoch: 0041 loss_train: 1.7523 acc_train: 0.3111 loss_val: 1.7691 acc_val: 0.3457 time: 0.1461s\n",
            "Epoch: 0042 loss_train: 1.7450 acc_train: 0.3111 loss_val: 1.7591 acc_val: 0.3457 time: 0.1395s\n",
            "Epoch: 0043 loss_train: 1.7360 acc_train: 0.3111 loss_val: 1.7493 acc_val: 0.3475 time: 0.1412s\n",
            "Epoch: 0044 loss_train: 1.7276 acc_train: 0.3111 loss_val: 1.7449 acc_val: 0.3475 time: 0.1451s\n",
            "Epoch: 0045 loss_train: 1.7170 acc_train: 0.3111 loss_val: 1.7345 acc_val: 0.3475 time: 0.1447s\n",
            "Epoch: 0046 loss_train: 1.7072 acc_train: 0.3111 loss_val: 1.7309 acc_val: 0.3475 time: 0.1420s\n",
            "Epoch: 0047 loss_train: 1.6986 acc_train: 0.3111 loss_val: 1.7204 acc_val: 0.3475 time: 0.1456s\n",
            "Epoch: 0048 loss_train: 1.6898 acc_train: 0.3111 loss_val: 1.7086 acc_val: 0.3475 time: 0.1433s\n",
            "Epoch: 0049 loss_train: 1.6807 acc_train: 0.3111 loss_val: 1.7004 acc_val: 0.3475 time: 0.1419s\n",
            "Epoch: 0050 loss_train: 1.6664 acc_train: 0.3111 loss_val: 1.6989 acc_val: 0.3475 time: 0.1436s\n",
            "Finished. Total time elapsed: 7.5047s\n",
            "Test: loss: 1.7293 acc: 0.2825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT782HS28zvf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}