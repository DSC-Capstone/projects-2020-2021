{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../src')\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results from other Text Analyzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the performance of our application <i> AutoLibrary </i>, we experiment it with three other text analyzers.\n",
    "\n",
    "- AutoLibrary: Used AutoPhrase to extract phrases and keywords and assigns quality scores to each of them. Requires human labor to label quality scores.\n",
    "\n",
    "- Jstor: https://www.jstor.org/analyze/. This search engine has its own text analyzer. It requires human labor to label quality scores.\n",
    "\n",
    "- Webtools:  https://www.webtools.services/text-analyzer\n",
    "    This website tool helps us find the frequency of phrases in a document. We then standardize it to form scores in range of 0 to 1.\n",
    "\n",
    "- MonkeyLearn: https://monkeylearn.com/text-analyzer/\n",
    "    This text analyzer extracts keywords from the document. It requires human labor to label quality scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe for accuracy\n",
    "top40_acc_df = pd.DataFrame(columns = [\"Analyzer\", \"Accuracy\", \"Domain\"])\n",
    "top10_acc_df = pd.DataFrame(columns = [\"Analyzer\", \"Accuracy\", \"Domain\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run AutoLibrary to get weighted scores for all domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = os.listdir('../references/experiment data')\n",
    "if '.DS_Store' in dirs:\n",
    "    dirs.remove('.DS_Store')\n",
    "\n",
    "weighted = {}\n",
    "weighted_all = {}\n",
    "weighted_stats = pd.DataFrame()\n",
    "for directory in dirs:\n",
    "    fp = '../references/experiment data/' + directory + '/weighted_AutoPhrase.csv'\n",
    "    df = pd.read_csv(fp, index_col='Unnamed: 0')\n",
    "    weighted_all[directory] = df\n",
    "    weighted[directory] = df\n",
    "    weighted_stats[directory] = df['score'].describe()\n",
    "weighted_df = pd.concat(weighted, axis=1)\n",
    "weighted_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_cs_phrases = [\"Psychotherapy\",\n",
    "\"Machine learning\",\n",
    "\"Text analytics\",\n",
    "\"Health care quality\",\n",
    "\"Academic aptitude\",\n",
    "\"Art therapy\",\n",
    "\"Big data\",\n",
    "\"Biomedical data\",\n",
    "\"Computer centers\",\n",
    "\"Audio engineering\",\n",
    "\"Code pages\",\n",
    "\"Compensatory education\",\n",
    "\"Computer programming\",\n",
    "\"Data analysis\",\n",
    "\"Data collection\",\n",
    "\"Educational technology\",\n",
    "\"Data transcription\",\n",
    "\"Data models\",\n",
    "\"Government information\",\n",
    "\"Health care utilization\",\n",
    "\"In state students\",\n",
    "\"Information attributes\",\n",
    "\"Learning\",\n",
    "\"Learning by doing\",\n",
    "\"Logistic regression\",\n",
    "\"Learning strategies\",\n",
    "\"Out of state students\",\n",
    "\"Psychological research\",\n",
    "\"Public health\",\n",
    "\"Speech acts\",\n",
    "\"School counseling\",\n",
    "\"Second language learning\",\n",
    "\"Student interests\",\n",
    "\"Students\",\n",
    "\"Time series\",\n",
    "\"Test theory\",\n",
    "\"Small area data\",\n",
    "\"Surveillance\",\n",
    "\"Time series forecasting\",\n",
    "\"ZIP codes\"]\n",
    "len(jstor_cs_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_cs_labels = [1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_cs_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Computer Science\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_cs_df = weighted_df['Computer Science'].dropna().head(40)\n",
    "autophrase_cs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_cs_labels = [1, 1, 1, 1, 1, 0, 1, 1, 1, 0, \n",
    "                       0, 0, 1, 0, 1, 0, 1, 0, 1, 1,\n",
    "                       0, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
    "                       0, 0, 1, 1, 0, 0, 0, 1, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_cs_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Computer Science\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_cs_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Computer Science\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_cs_phrases = {\"of the\": 74,\n",
    "\"et al\": 45,\n",
    "\"to the\": 36,\n",
    "\"in the\": 28,\n",
    "\"for the\": 27,\n",
    "\"in order\": 15,\n",
    "\"the therapist\": 15,\n",
    "\"and the\": 15,\n",
    "\"in order to\": 14,\n",
    "\"the system\": 14,\n",
    "\"by the\": 13,\n",
    "\"it is\": 12,\n",
    "\"of a\": 12,\n",
    "\"the session\": 12,\n",
    "\"at the\": 12,\n",
    "\"from the\": 12,\n",
    "\"of psychotherapy\": 11,\n",
    "\"able to\": 11,\n",
    "\"with the\": 10,\n",
    "\"we have\": 10,\n",
    "\"we are\": 10,\n",
    "\"is not\": 10,\n",
    "\"the two\": 10,\n",
    "\"between the\": 10,\n",
    "\"university of\": 9,\n",
    "\"in a\": 9,\n",
    "\"has been\": 9,\n",
    "\"behavioral coding\": 9,\n",
    "\"of the system\": 9,\n",
    "\"speech and\": 8,\n",
    "\"more than\": 8,\n",
    "\"is a\": 8,\n",
    "\"as a\": 8,\n",
    "\"a variety of\": 8,\n",
    "\"in this\": 8,\n",
    "\"based on\": 8,\n",
    "\"the client\": 8,\n",
    "\"to be\": 8,\n",
    "\"using the\": 8,\n",
    "\"automated evaluation\": 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_cs_phrases = utils.min_max_scale(webtools_cs_phrases)\n",
    "webtools_cs_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_cs_labels = [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]\n",
    "len(webtools_cs_labels) == len(webtools_cs_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_cs_phrases = [\"sessions\",\n",
    "\"codes\",\n",
    "\"utterances\",\n",
    "\"behavioral coding\",\n",
    "\"therapist\",\n",
    "\"international speech communication\",\n",
    "\"Automatic Speech Recognition\",\n",
    "\"speech communication association\",\n",
    "\"Proc\",\n",
    "\"psychotherapy\"]\n",
    "len(monkeylearn_cs_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_cs_labels = [1, 0, 0, 1, 1, 1, 1, 1, 0, 1]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_cs_labels[:10]), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Computer Science\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_econ_df = weighted_df['Economics'].dropna().head(40)\n",
    "autophrase_econ_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_econ_labels = [1, 1, 1, 0, 1, 0, 1, 1, 0, 0, \n",
    "                         0, 0, 0, 1, 1, 1, 0, 0, 0, 0, \n",
    "                         0, 0, 0, 0, 0, 1, 0, 1, 0, 1,\n",
    "                         0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_econ_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Economics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_econ_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Economics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_econ_phrases = {\"Treaty lands\",\n",
    "\"Axiom of choice\",\n",
    "\"Test theory\",\n",
    "\"Affirmative action\",\n",
    "\"Indian history\",\n",
    "\"Antennas\",\n",
    "\"Caste system\",\n",
    "\"Casting\",\n",
    "\"Fire protection\",\n",
    "\"Cognitive dissonance\",\n",
    "\"Equal protection\",\n",
    "\"Environmental protection\",\n",
    "\"Genetic correlation\",\n",
    "\"Grammemes\",\n",
    "\"Legislative branch\",\n",
    "\"Incantations\",\n",
    "\"Caste determination\",\n",
    "\"Logistic regression\",\n",
    "\"Lost wax casting\",\n",
    "\"Neuroscience\",\n",
    "\"Multilevel models\",\n",
    "\"Medicaid\",\n",
    "\"Personality inventories\",\n",
    "\"Pro choice movements\",\n",
    "\"Rational choice theory\",\n",
    "\"Rule of 72\",\n",
    "\"Simplex method\",\n",
    "\"Transfer pricing\",\n",
    "\"Sundials\",\n",
    "\"Rule of 78\",\n",
    "\"School choice\",\n",
    "\"Ray tracing\",\n",
    "\"Product choice\",\n",
    "\"Plasticizers\",\n",
    "\"Preferential voting\",\n",
    "\"Recursion\",\n",
    "\"Rule of 70\",\n",
    "\"Self control\",\n",
    "\"Syntax\",\n",
    "\"Tribal constitutions\"}\n",
    "len(jstor_econ_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_econ_labels = [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_econ_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Economics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_econ_phrases = {\"choice rule\": 101,\n",
    "\"of the\": 58,\n",
    "\"hr protections\": 36,\n",
    "\"set of\": 34,\n",
    "\"i i\": 33,\n",
    "\"of individuals\": 28,\n",
    "\"in india\": 26,\n",
    "\"sciakg choice rule\": 26,\n",
    "\"in the\": 25,\n",
    "\"set of individuals\": 24,\n",
    "\"category v\": 23,\n",
    "\"affirmative action\": 22,\n",
    "\"is the\":\t21,\n",
    "\"can be\":\t21,\n",
    "\"justified envy\": 21,\n",
    "\"the sciakg choice rule\": 21,\n",
    "\"the set\": 18,\n",
    "\"for every\": 18,\n",
    "\"in section\": 17,\n",
    "\"an individual\": 17,\n",
    "\"m i\": 17,\n",
    "\"individuals in\": 16,\n",
    "\"v v\": 16,\n",
    "\"merit score\": 16,\n",
    "\"the set of\": 16,\n",
    "\"meritorious horizontal\": 16,\n",
    "\"2smh choice rule\": 16,\n",
    "\"minimum guarantee\": 15,\n",
    "\"number of\": 15,\n",
    "\"sonmez and\": 15,\n",
    "\"and yenmez\": 15,\n",
    "\"meritorious horizontal choice rule\": 15,\n",
    "\"c2s m\": 15,\n",
    "\"et al\": 15,\n",
    "\"a set\": 14,\n",
    "\"the akghas\": 14,\n",
    "\"hrprotected position\": 14,\n",
    "\"the highest\": 14,\n",
    "\"for the\": 14,\n",
    "\"it is\": 14,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_econ_phrases = utils.min_max_scale(webtools_econ_phrases)\n",
    "webtools_econ_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_econ_labels = [1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "len(webtools_econ_labels) == len(webtools_econ_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkenLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_econ_phrases = [\"affirmative action\",\n",
    "\"horizontal reservations\",\n",
    "\"highest merit score\",\n",
    "\"akg choice rules\",\n",
    "\"2smh choice rules\",\n",
    "\"justified envy\",\n",
    "\"set of individual\",\n",
    "\"vr protection\",\n",
    "\"hr protection\",\n",
    "\"traits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_econ_labels = [1, 1, 1, 0, 0, 1, 0, 1, 0, 0]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_econ_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Economics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrical Engineering and Systems Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_eess_df = weighted_df['Electrical Engineering and Systems Science'].dropna().head(40)\n",
    "autophrase_eess_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_eess_labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                         1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
    "                         1, 1, 0, 1, 1, 1, 0, 0, 1, 0,\n",
    "                         0, 1, 1, 0, 1, 1, 1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_eess_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"EE & System Design\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_eess_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"EE & System Design\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_eess_phrases = [\"Imaging\",\n",
    "\"Computer networking\",\n",
    "\"Machine learning\",\n",
    "\"Image reconstruction\",\n",
    "\"Nuclear magnetic resonance\",\n",
    "\"Toilet training\",\n",
    "\"Systems librarians\",\n",
    "\"ARPA computer network\",\n",
    "\"Anthropic principle\",\n",
    "\"Astronomical cosmology\",\n",
    "                      \n",
    "\"Artificial neural networks\",\n",
    "\"Astronomical spectroscopy\",\n",
    "\"Bayesian analysis\",\n",
    "\"Community structure\",\n",
    "\"Citation indexes\",\n",
    "\"Bayesian networks\",\n",
    "\"Diagnostic imaging\",\n",
    "\"Data models\",\n",
    "\"Electrical networks\",\n",
    "\"Fractals\",\n",
    "                      \n",
    "\"Functional neuroimaging\",\n",
    "\"Image files\",\n",
    "\"Human biology\",\n",
    "\"Hilbert spaces\",\n",
    "\"Images\", \n",
    "\"Inverse problems\",\n",
    "\"Logic circuits\",\n",
    "\"Kalman filters\",\n",
    "\"Magnetic resonance angiography\", \n",
    "\"Magnetic resonance imaging\",\n",
    "                      \n",
    "\"Multilevel marketing\", \n",
    "\"Microvessels\",\n",
    "\"Mathematical objects\",\n",
    "\"Network topology\",\n",
    "\"Neuroscience\",\n",
    "\"Online social networking\",\n",
    "\"RLC circuits\",\n",
    "\"Radiology\",\n",
    "\"Particle image velocimetry\",\n",
    "\"Spacetime\"]\n",
    "len(jstor_eess_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_eess_labels = [0, 1, 1, 1, 1, 0, 0, 1, 1, 1, \n",
    "                    1, 1, 1, 0, 0, 1, 0, 1, 0, 0,\n",
    "                    0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
    "                    0, 0, 0, 1, 1, 0, 1, 1, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_eess_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"EE & System Design\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_eess_phrases = {\"of the\": 127,\n",
    "\"in the\": 56,\n",
    "\"for the\": 30,\n",
    "\"can be\": 29,\n",
    "\"the network\": 28,\n",
    "\"the cnnblock\": 28,\n",
    "\"to the\": 27,\n",
    "\"and the\": 27,\n",
    "\"as well\": 26,\n",
    "\"the proposed\": 25,\n",
    "                         \n",
    "\"and ncg\": 25,\n",
    "\"number of\": 24,\n",
    "\"our proposed\": 23,\n",
    "\"the entire\": 21,\n",
    "\"network architecture\": 20,\n",
    "\"as well as\": 19,\n",
    "\"in terms of\": 18,\n",
    "\"of the cnnblock\": 18,\n",
    "\"to be\": 17,\n",
    "\"note that\": 17,\n",
    "                         \n",
    "\"the entire network\": 16,\n",
    "\"m 1\": 16,\n",
    "\"reconstruction network\": 15,\n",
    "\"ah i\": 15,\n",
    "\"as the\": 14,\n",
    "\"the training\": 14,\n",
    "\"kspace data\": 13,\n",
    "\"on the\": 13,\n",
    "\"by the\": 13,\n",
    "\"radial spokes\": 13,\n",
    "                         \n",
    "\"1 and\": 13,\n",
    "\"that the\": 12,\n",
    "\"the forward\": 12,\n",
    "\"image reconstruction\": 12,\n",
    "\"from the\": 12,\n",
    "\"m 1 and\": 12,\n",
    "\"the measured\": 11,\n",
    "\"of our\": 11,\n",
    "\"proposed method\": 11,\n",
    "\"proposed cnnblock\": 11\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_eess_phrases = utils.min_max_scale(webtools_eess_phrases)\n",
    "webtools_eess_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_eess_labels = [0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
    "                       1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "                       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, \n",
    "                       0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
    "len(webtools_eess_labels) == len(webtools_eess_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_eess_phrases = [\"cine mr image\",\n",
    "\"mr image reconstruction\",\n",
    "\"cnn\",\n",
    "\"proposed methods\",\n",
    "\"test time\",\n",
    "\"block\",\n",
    "\"proposed cnn\",\n",
    "\"number of radial\",\n",
    "\"entire network\",\n",
    "\"radial spoke\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_eess_labels = [1, 1, 1, 0, 0, 0, 0, 0, 1, 1]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_eess_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"EE & System Design\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_math_df = weighted_df['Mathematics'].dropna().head(40)\n",
    "autophrase_math_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_math_labels = [1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
    "                         1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
    "                         1, 1, 0, 1, 1, 0, 1, 1, 0, 0, \n",
    "                         1, 0, 1, 0, 0, 0, 0, 0, 1, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_math_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Mathematics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_math_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Mathematics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_math_phrases = [\"Policy making\",\n",
    "\"Power functions\",\n",
    "\"Quantitative genetics\",\n",
    "\"Newtons method\",\n",
    "\"Mathematical induction\",\n",
    "\"Work functions\",\n",
    "\"Too big to fail policy\",\n",
    "\"Resuscitation orders\", \n",
    "\"Rational functions\",\n",
    "\"Stochastic processes\",\n",
    "                      \n",
    "\"Public policy\", \n",
    "\"Probability distributions\",\n",
    "\"Piecewise continuous functions\",\n",
    "\"Polynomials\",\n",
    "\"Pareto efficiency\",\n",
    "\"Oxygen consumption\",\n",
    "\"One to one functions\",\n",
    "\"Nonstandard analysis\",\n",
    "\"Mental health policy\",\n",
    "\"Mathematical problems\",\n",
    "                      \n",
    "\"Mathematical continuity\",\n",
    "\"Markov chains\",\n",
    "\"Mathematical completeness\",\n",
    "\"Machine learning\",\n",
    "\"Log buildings\",\n",
    "\"Lattice theory\",\n",
    "\"International environmental policy\",\n",
    "\"International trade policy\",\n",
    "\"Information policy\",\n",
    "\"Homeowners insurance\",\n",
    "                      \n",
    "\"Hilbert spaces\",\n",
    "\"Environmental policy\",\n",
    "\"Formal languages\",\n",
    "\"Function words\",\n",
    "\"Entropy\",\n",
    "\"Crime control\",\n",
    "\"Claims made policies\",\n",
    "\"Area function\",\n",
    "\"Artificial neural networks\",\n",
    "\"Central limit theorem\"]\n",
    "len(jstor_math_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_math_labels = [0, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
    "                    0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
    "                    1, 1, 0, 1, 0, 1, 0, 0, 0, 0,\n",
    "                    1, 0, 0, 0, 1, 0, 0, 1, 1, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_math_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Mathematics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_math_phrases = {\"s 1\": 70,\n",
    "\"s0 t\": 56,\n",
    "\"1 2\": 52,\n",
    "\"for any\": 51,\n",
    "\"v s\": 48,\n",
    "\"s s0\": 46,\n",
    "\"of the\": 40,\n",
    "\"s s0 t\": 35,\n",
    "\"proof of\": 33,\n",
    "\"1 1\": 31,\n",
    "                         \n",
    "\"for all\": 31,\n",
    "\"s 2\": 30,\n",
    "\"s h\": 29,\n",
    "\"with the\": 26,\n",
    "\"2 s\": 25,\n",
    "\"and any\": 24,\n",
    "\"any t\": 24,\n",
    "\"one has\": 23,\n",
    "\"sk s\": 23,\n",
    "\"any s\": 22,\n",
    "                         \n",
    "\"sk s s0\": 21,\n",
    "\"1 s\": 21,\n",
    "\"p sk\": 21,\n",
    "\"a1 1\": 20,\n",
    "\"and the\": 19,\n",
    "\"the proof\": 19,\n",
    "\"the last\": 19,\n",
    "\"and any t\": 19,\n",
    "\"s1 s\": 19,\n",
    "\"sk s s0 t\": 19,\n",
    "                         \n",
    "\"the following\": 18,\n",
    "\"ta1 s\": 18,\n",
    "\"s 3\": 18,\n",
    "\"we have\": 18,\n",
    "\"for any s\": 18,\n",
    "\"all t\": 18,\n",
    "\"that v\": 18,\n",
    "\"p s0\": 18,\n",
    "\"p sk s\": 18,\n",
    "\"t p\": 18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_math_phrases = utils.min_max_scale(webtools_math_phrases)\n",
    "webtools_math_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_math_labels = [0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
    "                       1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
    "                       0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
    "                       1, 0, 0, 0, 1, 0, 1, 0, 0, 0]\n",
    "len(webtools_math_labels) == len(webtools_math_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_math_phrases = [\"softmax pg methods\",\n",
    "\"proof of lemma\",\n",
    "\"lemma\",\n",
    "\"a0\",\n",
    "\"natural policy gradient\",\n",
    "\"a1\",\n",
    "\"a1 a2 a1\",\n",
    "\"a2 a1 a2\",\n",
    "\"a1 a2 a0\",\n",
    "\"last inequality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_math_labels = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_math_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Mathematics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_physics_df = weighted_df['Physics'].dropna().head(40)\n",
    "autophrase_physics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_physics_labels = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                            0, 1, 1, 1, 0, 1, 1, 1, 1, 1, \n",
    "                            1, 1, 1, 1, 1, 0, 0, 1, 0, 1, \n",
    "                            1, 1, 1, 0, 1, 1, 0, 1, 0, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_physics_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Physics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_physics_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Physics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_physics_phrases = [\"Relativity\",\n",
    "\"Quantum field theory\",\n",
    "\"Quantum mechanics\",\n",
    "\"Gravitational waves\",\n",
    "\"Lectures\",\n",
    "\"Astronomical cosmology\",\n",
    "\"Black holes\",\n",
    "\"Conservation laws\",\n",
    "\"General relativity\",\n",
    "\"Field research\",\n",
    "                         \n",
    "\"Far fields\",\n",
    "\"Gravitation theory\",\n",
    "\"Gravitational fields\",\n",
    "\"Gravitational potential\",\n",
    "\"Hawking radiation\",\n",
    "\"Gravity\",\n",
    "\"Lunar gravitation\",\n",
    "\"Methodism\",\n",
    "\"Near fields\",\n",
    "\"Old fields\",\n",
    "                         \n",
    "\"Physical sciences\",\n",
    "\"Philosophy of religion\",\n",
    "\"Quantum computers\",\n",
    "\"Quantum cosmology\",\n",
    "\"Quantum states\",\n",
    "\"Radio astronomy\",\n",
    "\"Quantum wells\",\n",
    "\"Reissner Nordstrom black holes\",\n",
    "\"Ring theory\",\n",
    "\"Special relativity\",\n",
    "                         \n",
    "\"Space research\",\n",
    "\"Schwarzschild radius\",\n",
    "\"String theory\",\n",
    "\"Tensors\",\n",
    "\"Waves\",\n",
    "\"Waving\",\n",
    "\"Yang Mills theory\",\n",
    "\"Bondi\",\n",
    "\"1966-7\",\n",
    "\"GM\"]\n",
    "len(jstor_physics_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_physics_labels = [1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
    "                       0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
    "                       0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "                       0, 1, 1, 0, 0, 0, 1, 0, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_physics_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Physics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_physics_phrases = {\"of the\": 108,\n",
    "\"in the\": 68,\n",
    "\"to the\": 37,\n",
    "\"gravitational waves\": 31,\n",
    "\"on the\": 30,\n",
    "\"the gravitational\": 28,\n",
    "\"to be\": 24,\n",
    "\"gravitational field\": 24,\n",
    "\"quantum mechanics\": 23,\n",
    "\"chapel hill\": 21,\n",
    "                            \n",
    "\"general relativity\": 20,\n",
    "\"with the\": 20,\n",
    "\"of a\": 20,\n",
    "\"that the\": 20,\n",
    "\"for the\": 19,\n",
    "\"at the\": 17,\n",
    "\"it is\": 17,\n",
    "\"the gravitational field\": 17,\n",
    "\"1 p\": 16,\n",
    "\"and the\": 15,\n",
    "                            \n",
    "\"of gravitational\": 15,\n",
    "\"from the\": 14,\n",
    "\"the chapel hill\": 12,\n",
    "\"as a\": 12,\n",
    "\"the first\": 12,\n",
    "\"in a\": 12,\n",
    "\"chapel hill conference\": 12,\n",
    "\"of gravitational waves\": 12,\n",
    "\"in particular\": 11,\n",
    "\"the chapel hill conference\": 11,\n",
    "                            \n",
    "\"by the\": 10,\n",
    "\"of the gravitational\": 10,\n",
    "\"with a\": 10,\n",
    "\"a gravitational\": 10,\n",
    "\"quantum gravity\": 9,\n",
    "\"of quantum\": 9,\n",
    "\"due to\": 9,\n",
    "\"to a\": 9,\n",
    "\"such a\": 9,\n",
    "\"the conference\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_physics_phrases = utils.min_max_scale(webtools_physics_phrases)\n",
    "webtools_physics_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_physics_labels = [0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
    "                          1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
    "                          0, 0, 0, 0, 0, 0, 1, 0, 0, 0, \n",
    "                          0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
    "len(webtools_physics_labels) == len(webtools_physics_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_physics_phrases = [\"feynman\",\n",
    "\"gravity\",\n",
    "\"chapel hill conference\",\n",
    "\"quantum mechanics\",\n",
    "\"quantum gravity\",\n",
    "\"caltech lecture\",\n",
    "\"general relativity\",\n",
    "\"quantum theory\",\n",
    "\"gravitational wave\",\n",
    "\"gravitational field\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_physics_labels = [0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_physics_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Physics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Biology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_qbio_df = weighted_df['Quantitative Biology'].dropna().head(40)\n",
    "autophrase_qbio_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_qbio_labels = [1, 1, 1, 1, 0, 1, 0, 0, 1, 0,\n",
    "                         1, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
    "                         1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
    "                         0, 0, 1, 0, 1, 0, 0, 1, 1, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_qbio_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Quant Biology\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_qbio_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Quant Biology\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_qbio_phrases = [\"Materials tests\",\n",
    "\"Elasticity\",\n",
    "\"Brain\",\n",
    "\"Fluids\",\n",
    "\"Soil strength\",\n",
    "\"Astronomical cosmology\",\n",
    "\"Computer networking\",\n",
    "\"Deformation\",\n",
    "\"Creep rupture strength\",\n",
    "\"Fluid solid interactions\",\n",
    "                      \n",
    "\"Forced expiratory flow rates\",\n",
    "\"Glaciers\",\n",
    "\"Fracture strength\",\n",
    "\"Fracture mechanics\",\n",
    "\"Gray literature\",\n",
    "\"Hydrogels\",\n",
    "\"Information resources\",\n",
    "\"Labor force participation rates\",\n",
    "\"Interstitial fluids\",\n",
    "\"Material world\",\n",
    "                      \n",
    "\"Materials\",\n",
    "\"Materials flow analysis\",\n",
    "\"Mechanical engineering\",\n",
    "\"Materials science\",\n",
    "\"Moduli of elasticity\",\n",
    "\"Neuroglia\",\n",
    "\"Ocean tides\",\n",
    "\"Optics\",\n",
    "\"Spacetime\",\n",
    "\"Price rigidities\",\n",
    "                      \n",
    "\"Physiology\",\n",
    "\"Steels\",\n",
    "\"Stress distribution\",\n",
    "\"Stress strain diagrams\",\n",
    "\"Tectonic plate interactions\",\n",
    "\"Stress strain relationships\",\n",
    "\"Temperature\",\n",
    "\"Tensile stress\",\n",
    "\"Tools\",\n",
    "\"Video recording\"]\n",
    "len(jstor_qbio_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_qbio_labels = [0, 0, 0, 0, 1, 1, 1, 0, 1, 1, \n",
    "                    1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
    "                    0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
    "                    0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_qbio_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Quant Biology\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_qbio_phrases = {\"of the\": 109,\n",
    "\"et al\": 77,\n",
    "\"in the\": 74,\n",
    "\"to the\": 46,\n",
    "\"brain tissue\": 33,\n",
    "\"on the\": 30,\n",
    "\"is the\": 27,\n",
    "\"the crack\": 25,\n",
    "\"crack propagation\": 24,\n",
    "\"and the\": 21,\n",
    "                         \n",
    "\"the brain\": 21,\n",
    "\"forte et al\": 21,\n",
    "\"can be\": 21,\n",
    "\"for the\": 20,\n",
    "\"by the\": 19,\n",
    "\"we have\": 19,\n",
    "\"the fracture\": 18,\n",
    "\"the brain tissue\": 18,\n",
    "\"that the\": 18,\n",
    "\"the cracktip\": 18,\n",
    "                         \n",
    "\"wire cutting\": 17,\n",
    "\"the material\": 17,\n",
    "\"crack tip\": 17,\n",
    "\"the wire\": 16,\n",
    "\"of fluid\": 15,\n",
    "\"due to\": 15,\n",
    "\"in fig\": 15,\n",
    "\"of a\": 14,\n",
    "\"as a\": 13,\n",
    "\"the solid\": 13,\n",
    "                         \n",
    "\"wire diameter\": 13,\n",
    "\"with the\": 12,\n",
    "\"dw 0\": 12,\n",
    "\"process zone\": 11,\n",
    "\"from the\": 11,\n",
    "\"with respect to\": 11,\n",
    "\"in the brain tissue\": 11,\n",
    "\"fracture process\": 11,\n",
    "\"is a\": 10,\n",
    "\"which is\": 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_qbio_phrases = utils.min_max_scale(webtools_qbio_phrases)\n",
    "webtools_qbio_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_qbio_labels = [0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
    "                       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, \n",
    "                       0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
    "                       1, 0, 0, 1, 0, 0, 1, 1, 0, 0]\n",
    "len(webtools_qbio_labels) == len(webtools_qbio_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_qbio_phrases = [\"brain tissue\",\n",
    "\"budday et al\",\n",
    "\"forte et al\",\n",
    "\"energy dissipation\",\n",
    "\"fracture process\",\n",
    "\"crack propagation\",\n",
    "\"fracture toughness\",\n",
    "\"tips region\",\n",
    "\"wire diameter dw\",\n",
    "\"physics of solid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_qbio_labels = [1, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_qbio_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Quant Biology\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_qfin_df = weighted_df['Quantitative Finance'].dropna().head(40)\n",
    "autophrase_qfin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_qfin_labels = [1, 1, 0, 1, 0, 1, 0, 1, 1, 1, \n",
    "                         1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
    "                         0, 0, 1, 0, 0, 0, 0, 1, 0, 1,\n",
    "                         0, 1, 0, 0, 1, 0, 0, 1, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_qfin_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Quant Finance\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_qfin_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Quant Finance\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases\n",
    "jstor_qfin_phrases = [\"Microeconomics\",\n",
    "\"Game theory\",\n",
    "\"Nonstandard analysis\",\n",
    "\"Hilbert spaces\",\n",
    "\"Monte Carlo methods\",\n",
    "\"Anesthetics\",\n",
    "\"Antiallergics\",\n",
    "\"Antifertility agents\",\n",
    "\"Antimitotics\",\n",
    "\"Astronomical cosmology\",\n",
    "                      \n",
    "\"Central limit theorem\",\n",
    "\"Convexity\",\n",
    "\"Defense policy\",\n",
    "\"Economic competition\",\n",
    "\"Economic principles\",\n",
    "\"Economic theory\",\n",
    "\"Economics\",\n",
    "\"Expected utility\",\n",
    "\"Formal languages\",\n",
    "\"Free agents\",\n",
    "                      \n",
    "\"Genetic drift\",\n",
    "\"Hallucinogens\",\n",
    "\"Hardy Weinberg law\",\n",
    "\"Home economics\",\n",
    "\"Iron oxides\",\n",
    "\"Market clearing prices\",\n",
    "\"Mathematical objects\",\n",
    "\"Mathematical problems\",\n",
    "\"Musical scales\",\n",
    "\"Nash equilibrium\",\n",
    "                      \n",
    "\"Neoclassical economics\",\n",
    "\"Optimal control\",\n",
    "\"Pareto efficiency\",\n",
    "\"Positive economics\",\n",
    "\"Posted price markets\",\n",
    "\"Prices\",\n",
    "\"Saltwater economics\",\n",
    "\"Simian virus 40\",\n",
    "\"Steady state economies\",\n",
    "\"Stochastic processes\"]\n",
    "len(jstor_qfin_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_qfin_labels = [1, 1, 0, 1, 0, 0, 0, 0, 1, 0,\n",
    "                    1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                    0, 0, 1, 0, 0, 1, 0, 0, 0, 1, \n",
    "                    1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_qfin_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Quant Finance\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_qfin_phrases = {\"0 t\": 283,\n",
    "\"c 0\": 125,\n",
    "\"t c\": 114,\n",
    "\"1 t\": 114,\n",
    "\"i t\": 112,\n",
    "\"c 0 t\": 88,\n",
    "\"0 t c\": 78,\n",
    "\"t t\": 64,\n",
    "\"c 0 t c\": 59,\n",
    "\"of the\": 58,\n",
    "                         \n",
    "\"c i\": 58,\n",
    "\"e 0 t\": 53,\n",
    "\"t c 0\": 51,\n",
    "\"c 0 t c i\": 46,\n",
    "\"t c 0 t\": 42,\n",
    "\"in the\": 41,\n",
    "\"t y\": 40,\n",
    "\"0 0\": 38,\n",
    "\"to the\": 37,\n",
    "\"n 0\": 36,\n",
    "                         \n",
    "\"c 1 t\": 36,\n",
    "\"for the\": 35,\n",
    "\"c i t\": 35,\n",
    "\"t x\": 35,\n",
    "\"y 1\": 33,\n",
    "\"major agent\": 32,\n",
    "\"y 1 t\": 32,\n",
    "\"n i1\": 31,\n",
    "\"t y 1\": 31,\n",
    "\"t y 1 t\": 30,\n",
    "                         \n",
    "\"t 0\": 29,\n",
    "\"the major\": 28,\n",
    "\"t dt\": 28,\n",
    "\"the major agent\": 26,\n",
    "\"c 0 t c i t\": 26,\n",
    "\"j t\": 26,\n",
    "\"0 t y\": 26,\n",
    "\"dw 0 t\": 25,\n",
    "\"c 0 ci\": 25,\n",
    "\"0 0 0\": 24}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_qfin_phrases = utils.min_max_scale(webtools_qfin_phrases)\n",
    "webtools_qfin_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_qfin_labels = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \n",
    "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                       0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
    "                       0, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "len(webtools_qfin_labels) == len(webtools_qfin_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_qfin_phrases = [\"equilibrium price formation\",\n",
    "\"equilibrium price process\",\n",
    "\"means field games\",\n",
    "\"major agent\",\n",
    "\"stochastic differential equation\",\n",
    "\"large population limit\",\n",
    "\"minor agent\",\n",
    "\"dw\",\n",
    "\"c0\",\n",
    "\"system of fbsdes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_qfin_labels = [1, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_qfin_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Quant Finance\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoLibrary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_stat_df = weighted_df['Statistics'].dropna().head(40)\n",
    "autophrase_stat_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autophrase_stat_labels = [1, 1, 0, 1, 1, 1, 1, 1, 0, 0, \n",
    "                         1, 1, 0, 0, 1, 1, 0, 1, 0, 1,\n",
    "                         0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
    "                         0, 1, 0, 0, 1, 1, 1, 1, 0, 0]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(autophrase_stat_labels), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Statistics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(autophrase_stat_labels[:10]), \"Analyzer\": \"AutoLibrary\", \"Domain\": \"Statistics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jstor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "jstor_stat_phrases = [\"Text analytics\",\n",
    "\"Parallel computing\",\n",
    "\"Credit ratings\",\n",
    "\"Machine learning\",\n",
    "\"Information classification\",\n",
    "\"Z score\",\n",
    "\"Time series forecasting\",\n",
    "\"Work credits\",\n",
    "\"Test theory\",\n",
    "\"Test scores\",\n",
    "                      \n",
    "\"Test data\",\n",
    "\"Teaching methods\",\n",
    "\"T score\",\n",
    "\"Test score decline\",\n",
    "\"Ratio test\",\n",
    "\"Preeclampsia\",\n",
    "\"Production estimates\",\n",
    "\"Opportunity equality\",\n",
    "\"Mathematical programming\",\n",
    "\"Long run profit maximization\",\n",
    "                      \n",
    "\"Judicial powers\",\n",
    "\"Letters of credit\",\n",
    "\"Logistic regression\",\n",
    "\"Information science\",\n",
    "\"Information attributes\",\n",
    "\"Full scores\",\n",
    "\"Financial institutions\",\n",
    "\"Diversity indices\",\n",
    "\"Education credits\",\n",
    "\"Fairness\",\n",
    "                      \n",
    "\"Disparate impact\",\n",
    "\"Dimensionality reduction\",\n",
    "\"Debt collection\",\n",
    "\"Credit default swaps\",\n",
    "\"Big data\",\n",
    "\"Charge accounts\",\n",
    "\"Classified information\",\n",
    "\"Data models\",\n",
    "\"Credit\",\n",
    "\"Yield curves\"]\n",
    "len(jstor_stat_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jstor_stat_labels = [0, 0, 1, 1, 1, 0, 1, 0, 0, 0,\n",
    "                    0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
    "                    0, 0, 1, 1, 0, 0, 0, 1, 1, 1, \n",
    "                    1, 1, 0, 0, 0, 0, 0, 0, 0, 1]\n",
    "top40_acc_df = top40_acc_df.append({\"Accuracy\": np.mean(jstor_stat_labels), \"Analyzer\": \"Jstor\", \"Domain\": \"Statistics\"}, ignore_index = True)\n",
    "top40_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Webtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_stat_phrases = {\"et al\": 62,\n",
    "\"of the\": 53,\n",
    "\"credit scoring\": 38,\n",
    "\"on the\": 28,\n",
    "\"to the\": 27,\n",
    "\"and the\": 25,\n",
    "\"in the\": 24,\n",
    "\"of a\": 23,\n",
    "\"is the\": 21,\n",
    "\"fairness processors\": 20,\n",
    "                         \n",
    "\"the same\": 19,\n",
    "\"based on\": 19,\n",
    "\"p 1\": 18,\n",
    "\"that the\": 17,\n",
    "\"sensitive attribute\": 17,\n",
    "\"of fairness\": 17,\n",
    "\"can be\": 15,\n",
    "\"fairness criteria\": 15,\n",
    "\"the sensitive\": 14,\n",
    "\"1 1\": 14,\n",
    "                         \n",
    "\"0 1\": 13,\n",
    "\"is a\": 13,\n",
    "\"for the\": 12,\n",
    "\"to a\": 11,\n",
    "\"has a\": 11,\n",
    "\"based on the\": 11,\n",
    "\"the other\": 11,\n",
    "\"data sets\": 11,\n",
    "\"1 0\": 10,\n",
    "\"the separation\": 10,\n",
    "                         \n",
    "\"the emp\": 10,\n",
    "\"the sensitive attribute\": 9,\n",
    "\"barocas et al\": 9,\n",
    "\"hardt et al\": 9,\n",
    "\"a loan\": 9,\n",
    "\"the separation criterion\": 9,\n",
    "\"with a\": 9,\n",
    "\"in credit scoring\": 9,\n",
    "\"scoring model\": 9,\n",
    "\"platt scaling\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize it\n",
    "webtools_stat_phrases = utils.min_max_scale(webtools_stat_phrases)\n",
    "webtools_stat_phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webtools_stat_labels = [0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
    "                       0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
    "                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                       0, 1, 0, 0, 0, 0, 0, 0, 1, 1]\n",
    "len(webtools_stat_labels) == len(webtools_stat_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_stat_phrases = [\"credit scoring\",\n",
    "\"fairness criteria\",\n",
    "\"fairness processor\",\n",
    "\"processor\",\n",
    "\"separation\",\n",
    "\"barocas et al\",\n",
    "\"sufficiency\",\n",
    "\"reject options classification\",\n",
    "\"disparate impact remover\",\n",
    "\"equalized odds processor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monkeylearn_stat_labels = [1, 1, 0, 0, 0, 0, 0, 1, 1, 1]\n",
    "top10_acc_df = top10_acc_df.append({\"Accuracy\": np.mean(monkeylearn_stat_labels), \"Analyzer\": \"MonkeyLearn\", \"Domain\": \"Statistics\"}, ignore_index = True)\n",
    "top10_acc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# autolibrary vs. webtools\n",
    "utils.graph_precision_recall(autophrase_cs_labels, autophrase_cs_df,\n",
    "    webtools_cs_labels, webtools_cs_phrases, \"Computer Science\")\n",
    "utils.graph_precision_recall(autophrase_econ_labels, autophrase_econ_df,\n",
    "    webtools_econ_labels, webtools_econ_phrases, \"Economics\")\n",
    "utils.graph_precision_recall(autophrase_eess_labels, autophrase_eess_df,\n",
    "    webtools_eess_labels, webtools_eess_phrases, \"EE & System Design\")\n",
    "utils.graph_precision_recall(autophrase_math_labels, autophrase_math_df,\n",
    "    webtools_math_labels, webtools_math_phrases, \"Mathematics\")\n",
    "utils.graph_precision_recall(autophrase_physics_labels, autophrase_physics_df,\n",
    "    webtools_physics_labels, webtools_physics_phrases, \"Physics\")\n",
    "utils.graph_precision_recall(autophrase_qbio_labels, autophrase_qbio_df,\n",
    "    webtools_qbio_labels, webtools_qbio_phrases, \"Quantitative Biology\")\n",
    "utils.graph_precision_recall(autophrase_qfin_labels, autophrase_qfin_df,\n",
    "    webtools_qfin_labels, webtools_qfin_phrases, \"Quantitative Finance\")\n",
    "utils.graph_precision_recall(autophrase_stat_labels, autophrase_stat_df,\n",
    "    webtools_stat_labels, webtools_stat_phrases, \"Statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the area under the precision-recall curve of AutoLibrary is bigger than that of Webtools in all eight domains. This means that AutoLibrary performs better than the Webtools analyzer across different domains. It makes sense intuitively since the latter one is very simple. Its results often contain non-quality phrases, such as “of the”, “can be”, and etc. Moreover, in the domain of Quantitative Finance, where there were lots of mathematical equations, the Webtools analyzer failed to filter out symbols like “ts”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autolibrary vs. jstor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (27, 12))\n",
    "sns.set(font_scale = 1.8)\n",
    "ax = sns.histplot(data = top40_acc_df, x = \"Domain\", weights = \"Accuracy\", hue = \"Analyzer\", \n",
    "                  hue_order = [\"AutoLibrary\", \"Jstor\"], multiple=\"dodge\", shrink=.8)\n",
    "ax.set_xlabel('Domains', fontsize = 30)\n",
    "ax.set_ylabel('Accuracy', fontsize = 30)\n",
    "ax.legend(labels = [\"Jstor\", \"AutoLibrary\"], fontsize = 30)\n",
    "ax.set_title(\"Precision of Top 40 Extracted Phrases: AutoPhrase vs. Jstor\", fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in all domains, AutoLibrary performs better than the Jstor analyzer. The main con of Jstor is that its recommendation is based on a fixed set of predefined topics. Therefore, it cannot make customized recommendations for specific papers. For example, when analyzing a statistics paper, it recommended “debt collection”, which doesn’t even exist in the original text. In contrast, AutoLibrary first extracts phrases from the input paper, which improves the contingency between the quality phrases and the original paper.\n",
    "\n",
    "Another big shortcoming of Jstor is that the topics it recommends are sometimes too general. Although they make sense in English, they don't qualify as quality phrases. For  example, when analyzing the Computer Science paper, one of the phrases extracted is “computer programming.” It doesn’t really help users since it’s too general for them to search for related papers.\n",
    "\n",
    "Last but not least, Jstor doesn’t offer quality scores to recommended phrases, which means that users don’t know which phrases can best represent their input papers. AutoLibrary, on the other hand, ranks the top 5 phrases in order, so that users can have a better overview of the papers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autolibrary vs. MonkeyLearn\n",
    "fig = plt.figure(figsize = (27, 12))\n",
    "sns.set(font_scale = 1.8)\n",
    "ax = sns.histplot(data = top10_acc_df, x = \"Domain\", weights = \"Accuracy\", hue = \"Analyzer\", \n",
    "                  hue_order = [\"AutoLibrary\", \"MonkeyLearn\"], multiple=\"dodge\", shrink=.8)\n",
    "ax.set_xlabel('Domains', fontsize = 30)\n",
    "ax.set_ylabel('Accuracy', fontsize = 30)\n",
    "ax.legend(labels = [\"MonkeyLearn\", \"AutoLibrary\"], fontsize = 30)\n",
    "ax.set_title(\"Precision of Top 10 Extracted Phrases: AutoPhrase vs. MonkeyLearn\", fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the figure, we can see that AutoPhrase outperformed MonkeyLearn in all domains. One big disadvantage of MonkeyLearn we found is that it probably relies heavily on the frequency of phrases. Although it can extract some really meaningful phrases amongst its top 5 phrases, it also recommends some phrases that make no sense. For example, when analyzing the Mathematics paper, it extracted \"a1 a2 a1\", \"a2 a1 a2\" and \"a1 a2 a0\" amongst the top 10 phrases. We believe that AutoLibrary defeats MonkeyLearn by weighting phrases against domain knowledge pools, which eliminates the reckless ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
