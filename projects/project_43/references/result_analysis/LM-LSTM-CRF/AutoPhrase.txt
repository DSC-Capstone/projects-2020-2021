0.9455000000	neural networks
0.8955000000	conll03 ner
0.8715000000	highway layers
0.8135000000	language model
0.8105000000	pos tagging
0.8095000000	bi lstm
0.7855000000	highway units
0.7615000000	sequence labeling
0.7555000000	lstm crf
0.7545000000	conditional random
0.7525000000	transfer learning
0.7485000000	ner task
0.7365000000	state size
0.7355000000	word embedding
0.7345000000	task specific
0.7165720890	cnn
0.7155000000	lm lstm crf
0.7136625970	f1
0.6935000000	word embeddings
0.6862867556	model
0.6816602324	highway
0.6736572487	knowledge
0.6715000000	language models
0.6695000000	lstm cnn crf
0.6645000000	lample et al
0.6585000000	pre trained
0.6575000000	f1 score
0.6561588123	learning
0.6549002094	models
0.6546334214	ma
0.6537537675	ner
0.6510093624	task
0.6505000000	hidden state
0.6491156657	sequence
0.6465000000	2016 † reported
0.6456648174	forward
0.6454701422	language
0.6435000000	peters et al
0.6420962564	conll
0.6415000000	index 10
0.6395000000	named entity recognition
0.6385596283	dataset
0.6365000000	character level knowledge
0.6355000000	character level layers
0.6350802219	word
0.6325266792	pos
0.6325000000	sequence labeling tasks
0.6320561882	network
0.6310575276	neural
0.6295000000	index 4
0.6274024406	training
0.6271232442	data
0.6269444564	reported
0.6225000000	word level knowledge
0.6221312643	index
0.6220928001	layers
0.6215000000	lm lstm crf nh
0.6215000000	lm lstm crf nl
0.6186530583	‡
0.6175278615	art
0.6135000000	lstm at
0.6115000000	− 1
0.6114971541	†
0.6112500796	hidden
0.6092683219	backward
0.6088404090	crf
0.6076867339	table
0.6065947785	capture
0.6065947785	transfer
0.6065000000	word level
0.6058656730	lm
0.6054547367	−
0.6048261557	xi
0.6045000000	luo et al
0.6039621863	lstm
0.6032929185	tasks
0.6025000000	multi task learning
0.6021447632	trained
0.6012621033	extract
0.6003406907	networks
0.5982294403	units
0.5971268635	embedding
0.5958111394	±
0.5955000000	character level
0.5952540720	wsj
0.5945000000	lm lstm
0.5917381647	input
0.5914290231	size
0.5914290231	entity
0.5895000000	conll 2000
0.5875000000	rei 2017
0.5833992235	state
0.5820676948	pre
0.5817815298	specific
0.5810608797	tagging
0.5796314979	output
0.5792747987	character
0.5781049113	labeling
0.5756651460	named
0.5737978640	bi
0.5705981947	directly
0.5705981947	test
0.5684292701	score
0.5671023428	luo
0.5662382095	e.g
0.5662382095	train
0.5657387265	peters
0.5655126034	embeddings
0.5654470598	conll03
0.5648416590	lample
0.5633590177	strategy
0.5633590177	label
0.5625467872	rei
0.5623966887	zi
0.5595096544	level
0.5565000000	4096 8192 1024
0.5564788780	extra
0.5558323892	experiments
0.5558323892	information
0.5553932874	comparison
0.5542444408	structure
0.5495244657	chunking
0.5488654117	outperforms
0.5488654117	results
0.5488654117	dimension
0.5488654117	exp
0.5488654117	probability
0.5488654117	leverage
0.5488654117	setting
0.5488654117	c0
0.5488654117	portion
0.5488654117	incorporate
0.5488654117	applied
0.5488654117	efficient
0.5488654117	previous
0.5488654117	adopted
0.5488654117	yi
0.5488654117	adopting
0.5484170890	framework
0.5484170890	efficiency
0.5478812170	similar
0.5455976657	performance
0.5455029603	pr
0.5455029603	␣
0.5455029603	words
0.5455029603	datasets
0.5455029603	order
0.5455029603	proposed
0.5451940078	rni
0.5445000000	state of
0.5445000000	xi |
0.5444953068	layer
0.5443679984	methods
0.5425000000	the conll03 ner dataset
0.5415000000	2017 † reported
0.5403671846	arxiv
0.5403671846	maximum
0.5403671846	log
0.5403671846	effectiveness
0.5403671846	conduct
0.5403671846	effective
0.5398985565	development
0.5398985565	yj
0.5398985565	∑
0.5398985565	fni
0.5398985565	large
0.5398985565	space
0.5398985565	annotations
0.5398985565	features
0.5393891568	related
0.5393891568	additional
0.5365101906	parameters
0.5329918542	set
0.5329918542	unit
0.5329918542	corpus
0.5325000000	ma and hovy 2016
0.5312816226	baselines
0.5312816226	max
0.5312816226	pf
0.5312816226	x1
0.5312816226	resources
0.5302800923	yang
0.5270188265	comparing
0.5256613389	f1score
0.5256613389	collobert
0.5256613389	jozefowicz
0.5256613389	conll00
0.5255701579	taglm
0.5255000000	output of
0.5231613389	pierre
0.5216434076	ci
0.5195000000	co training
0.5192773762	manning
0.5185000000	91.71 ± 0.10
0.5165000000	4096 8192
0.5163797959	joint
0.5155000000	± std
0.5145000000	and hovy
0.5139397819	søgaard
0.5139397819	chiu
0.5139397819	liu
0.5045000000	2017 † ‡
0.5035000000	neural language
0.5035000000	ner dataset
0.5025000000	2048 512
0.5015000000	et al
0.4905000000	the language model
0.4895000000	† ‡ mean
0.4885000000	± 0.10
0.4795000000	† reported
0.4725000000	trained on
0.4695000000	the sequence labeling
0.4675000000	these models
0.4635000000	in fig
0.4575000000	lstm 2048
0.4555000000	the output of
0.4555000000	8192 1024
0.4535000000	2016 † mean
0.4515000000	this paper
0.4475000000	to extract
0.4435000000	based on
0.4425000000	± 0.08
0.4425000000	± 0.23
0.4425000000	± 0.22
0.4255000000	instead of
0.4225000000	character level lstm
0.4215000000	knowledge from
0.4105000000	the same
0.4105000000	in table
0.4055000000	to capture
0.4045000000	for example
0.3835000000	to sl
0.3745000000	for sequence
0.3745000000	the output
0.3745000000	the conll03
0.3743300286	2000
0.3715000000	of training
0.3652704356	instead
0.3615000000	the art
0.3522171124	91.71
0.3418737499	this
0.3418041243	these
0.3405000000	model with
0.3390325008	co
0.3377969450	for
0.3373002883	|
0.3368142882	1024
0.3368142882	4096
0.3368142882	2048
0.3364636436	512
0.3362587326	8192
0.3357569881	part
0.3329310342	z
0.3318202158	at
0.3315000000	for sequence labeling
0.3309291147	2016
0.3285838466	10
0.3261517156	1
0.3257786688	from
0.3249055617	2017
0.3241531166	4
0.3215481526	on
0.3205000000	the sequence
0.3176623879	the
0.3175000000	of lstm
0.3171623879	to
0.3165000000	the conll
0.3132727379	of
0.3131029412	in
0.3105000000	forward to
0.3075000000	backward to
0.3053293730	with
0.3005689766	first
0.2953585903	t
0.2944069218	m
0.2942109691	w
0.2942109691	l
0.2942109691	d
0.2934566173	f
0.2911394282	an
0.2894403969	only
0.2884582252	11
0.2881154928	100
0.2881154928	three
0.2881154928	one
0.2881154928	thus
0.2881154928	2014
0.2880736523	other
0.2879561579	3
0.2875859692	8
0.2875859692	some
0.2875859692	each
0.2875859692	because
0.2875859692	when
0.2875859692	better
0.2875859692	9
0.2875859692	while
0.2875859692	even
0.2875859692	many
0.2871750572	s
0.2866285623	k
0.2863380703	however
0.2853066693	&
0.2853066693	300
0.2853066693	over
0.2853066693	into
0.2853066693	would
0.2853066693	any
0.2853066693	200
0.2853066693	much
0.2853066693	7
0.2848257426	such
0.2845229555	different
0.2840295152	both
0.2840295152	all
0.2835705804	could
0.2835705804	6
0.2835705804	its
0.2814109746	time
0.2814109746	more
0.2814109746	without
0.2809979686	besides
0.2799419161	as
0.2793494533	2011
0.2793494533	2003
0.2786972875	than
0.2777096917	it
0.2776619272	not
0.2774701274	a
0.2774691913	g
0.2774078774	5
0.2764303485	by
0.2760821658	our
0.2758127208	+
0.2756315641	we
0.2754869193	where
0.2754869193	have
0.2754869193	most
0.2754869193	further
0.2749524892	also
0.2746142720	although
0.2745646574	2
0.2745646574	2015
0.2742626502	that
0.2741619272	but
0.2736521629	n
0.2734370941	i
0.2734266363	p
0.2730905636	two
0.2730905636	been
0.2730905636	has
0.2718685181	e
0.2712694557	h
0.2700244550	which
0.2693135321	x
0.2685382388	are
0.2676554788	r
0.2675541660	or
0.2675541660	can
0.2675382388	=
0.2675382388	be
0.2667204205	is
0.2655470917	c
0.2637643625	and
0.2627945591	j
0.2622100586	y
