{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enormous-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "import os\n",
    "os.chdir('/home/rcgonzal/DSC180Malware/m2v-adversarial-hindroid/')\n",
    "\n",
    "from __future__ import print_function\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from src.imbalanced_dataset_sampler.imbalanced import ImbalancedDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "administrative-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6421, 2535703)\n"
     ]
    }
   ],
   "source": [
    "with open('data/out/all-apps/hindroid-train-set/A_test.npz', 'rb') as file:\n",
    "    print(sparse.load_npz(file).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifty-miami",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2674056, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/out/train-set/method_map.csv').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nearby-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindroidDataset(Dataset):\n",
    "    def __init__(self, features_path, labels_path, label_col='m2vDroid', transform=None):\n",
    "        '''\n",
    "        Creates a  dataset from the A matrix representation of apps and their associated labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -------------------\n",
    "        features_path: Path to A matrix in sparse format.\n",
    "        labels_path: Path to labels in csv format.\n",
    "        label_col: Default 'm2vDroid'. Useful for specifying which kernel to use for HinDroid.\n",
    "        '''\n",
    "        self.features = sparse.load_npz(os.path.join(features_path))\n",
    "        self.feature_width = self.features.shape[1]\n",
    "        features_folder = os.path.split(features_path)[0]\n",
    "        self.features_idx = list(pd.read_csv(\n",
    "            os.path.join(features_folder, 'predictions.csv'),\n",
    "            usecols=['app'], \n",
    "            squeeze=True\n",
    "        ))\n",
    "        self.transform = transform\n",
    "        \n",
    "        try:\n",
    "            self.labels = pd.read_csv(\n",
    "                labels_path, \n",
    "                usecols=['app', label_col],\n",
    "                index_col = 'app',\n",
    "                squeeze=True\n",
    "            )\n",
    "            self.labels = self.labels[self.features_idx].values # align labels with features index\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(e)\n",
    "            print('Seems like you may be trying to use a different model. This class is setup for m2vDroid by default.')\n",
    "            print('For HinDroid you must specify `label_col` as either AAT, ABAT, APAT, ABPBTAT, or APBPTAT.')\n",
    "            \n",
    "        assert (self.features.shape[0] == self.labels.size), 'Length mismatch between features and labels.'\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        features = self.features[idx]\n",
    "        features = features.todense().astype('float').A\n",
    "        labels = self.labels[idx]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             features = self.transform(features)\n",
    "#             labels = self.transform(labels)\n",
    "        \n",
    "#         sample = {'features': features, 'labels': labels}\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def get_labels(self, idx):\n",
    "        return self.labels[idx]\n",
    "    \n",
    "def hindroid_custom_get_label(dataset, idx):\n",
    "    return dataset.get_labels(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "architectural-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HindroidSubstitute(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(HindroidSubstitute, self).__init__()\n",
    "        self.layer_1 = nn.Linear(n_features, 64, bias=False)\n",
    "        # Linear - how to freeze layer ^\n",
    "        # biases = false\n",
    "        self.layer_2 = nn.Linear(64, 64, bias=False)\n",
    "        self.layer_3 = nn.Linear(64, 64, bias=False)\n",
    "        self.layer_4 = nn.Linear(64, 2, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not torch.is_tensor(x):\n",
    "            x = torch.from_numpy(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.relu(self.layer_3(x))\n",
    "        x = self.layer_4(x)\n",
    "        return x # logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "judicial-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model\n",
    "# model = HindroidSubstitute(dataset.feature_width).double()\n",
    "# model(dataset[-100:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dangerous-glasgow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "The negative log likelihood loss.\n",
       "\n",
       "See :class:`~torch.nn.NLLLoss` for details.\n",
       "\n",
       "Args:\n",
       "    input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`\n",
       "        in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \\geq 1`\n",
       "        in the case of K-dimensional loss.\n",
       "    target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`,\n",
       "        or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \\geq 1` for\n",
       "        K-dimensional loss.\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each\n",
       "        class. If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when reduce is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets. Default: -100\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (string, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,\n",
       "        ``'mean'``: the sum of the output will be divided by the number of\n",
       "        elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
       "        specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> # input is of size N x C = 3 x 5\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> # each element in target has to have 0 <= value < C\n",
       "    >>> target = torch.tensor([1, 0, 4])\n",
       "    >>> output = F.nll_loss(F.log_softmax(input), target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.8/site-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F.nll_loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "timely-compromise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, weight=None):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = F.log_softmax(model(data), dim=1)\n",
    "        loss = F.nll_loss(output, target, weight=weight) # do we use different loss?\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # logging\n",
    "        log_interval = 100\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "#         if batch_idx % args.log_interval == 0:\n",
    "#             print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#                 epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                 100. * batch_idx / len(train_loader), loss.item()))\n",
    "#             if args.dry_run:\n",
    "#                 break\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, weight=None):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "#             print(output)\n",
    "            output = F.log_softmax(output, dim=1)\n",
    "#             print(output)\n",
    "            loss = F.nll_loss(output, target, weight=weight, reduction='sum').item()  # sum up batch loss\n",
    "#             print('loss: ', loss)\n",
    "            test_loss += loss\n",
    "#             print(output.argmax(dim=1, keepdim=True))\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "#             print(target.view_as(pred))\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "qualified-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = HindroidSubstitute(dataset.feature_width).double()\n",
    "# weights = torch.Tensor([dataset.labels.mean() / (1-dataset.labels.mean()), 1]).double()\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test(model, 'cpu', test_loader, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-istanbul",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "operational-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "# use_cuda = False\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "# train_kwargs = {'batch_size': args.batch_size}\n",
    "# test_kwargs = {'batch_size': args.test_batch_size}\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "#                    'shuffle': True,\n",
    "                   'pin_memory': True}\n",
    "#     train_kwargs.update(cuda_kwargs)\n",
    "#     test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "# load data (will need to be adapted as well)\n",
    "# 1) load A_test\n",
    "# 2) load labels\n",
    "# 3) Perform train-test-split\n",
    "dataset = HindroidDataset(\n",
    "    'data/out/all-apps/hindroid-train-set/A_test.npz', \n",
    "    'data/out/all-apps/hindroid-train-set/predictions.csv',\n",
    "    'ABAT',\n",
    ")\n",
    "# weights = torch.Tensor([dataset.labels.mean() / (1-dataset.labels.mean()), 1]).to(device).double()\n",
    "weights = None\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=5,\n",
    "    shuffle = False,\n",
    "    sampler = ImbalancedDatasetSampler(dataset, callback_get_label = hindroid_custom_get_label),\n",
    "    **cuda_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=5,\n",
    "    shuffle = False,\n",
    "    sampler = ImbalancedDatasetSampler(dataset, callback_get_label = hindroid_custom_get_label),\n",
    "    **cuda_kwargs)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "# test_loader = torch.utils.data.DataLoader(dataset, batch_size=10)\n",
    "\n",
    "model = HindroidSubstitute(dataset.feature_width).double().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cellular-hours",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/6421 (0%)]\tLoss: 0.693676\n",
      "Train Epoch: 1 [500/6421 (8%)]\tLoss: 0.562266\n",
      "Train Epoch: 1 [1000/6421 (16%)]\tLoss: 0.458389\n",
      "Train Epoch: 1 [1500/6421 (23%)]\tLoss: 0.218673\n",
      "Train Epoch: 1 [2000/6421 (31%)]\tLoss: 0.348942\n",
      "Train Epoch: 1 [2500/6421 (39%)]\tLoss: 0.119268\n",
      "Train Epoch: 1 [3000/6421 (47%)]\tLoss: 0.312027\n",
      "Train Epoch: 1 [3500/6421 (54%)]\tLoss: 0.133438\n",
      "Train Epoch: 1 [4000/6421 (62%)]\tLoss: 0.129907\n",
      "Train Epoch: 1 [4500/6421 (70%)]\tLoss: 0.046253\n",
      "Train Epoch: 1 [5000/6421 (78%)]\tLoss: 0.246845\n",
      "Train Epoch: 1 [5500/6421 (86%)]\tLoss: 0.211596\n",
      "Train Epoch: 1 [6000/6421 (93%)]\tLoss: 0.077840\n",
      "\n",
      "Test set: Average loss: 0.1030, Accuracy: 6247/6421 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/6421 (0%)]\tLoss: 0.065740\n",
      "Train Epoch: 2 [500/6421 (8%)]\tLoss: 0.124861\n",
      "Train Epoch: 2 [1000/6421 (16%)]\tLoss: 0.101204\n",
      "Train Epoch: 2 [1500/6421 (23%)]\tLoss: 0.168214\n",
      "Train Epoch: 2 [2000/6421 (31%)]\tLoss: 0.086768\n",
      "Train Epoch: 2 [2500/6421 (39%)]\tLoss: 0.049627\n",
      "Train Epoch: 2 [3000/6421 (47%)]\tLoss: 0.058959\n",
      "Train Epoch: 2 [3500/6421 (54%)]\tLoss: 0.031021\n",
      "Train Epoch: 2 [4000/6421 (62%)]\tLoss: 0.053021\n",
      "Train Epoch: 2 [4500/6421 (70%)]\tLoss: 0.058635\n",
      "Train Epoch: 2 [5000/6421 (78%)]\tLoss: 0.060430\n",
      "Train Epoch: 2 [5500/6421 (86%)]\tLoss: 0.293104\n",
      "Train Epoch: 2 [6000/6421 (93%)]\tLoss: 0.056529\n",
      "\n",
      "Test set: Average loss: 0.0878, Accuracy: 6276/6421 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/6421 (0%)]\tLoss: 0.048059\n",
      "Train Epoch: 3 [500/6421 (8%)]\tLoss: 0.001703\n",
      "Train Epoch: 3 [1000/6421 (16%)]\tLoss: 0.175307\n",
      "Train Epoch: 3 [1500/6421 (23%)]\tLoss: 0.027076\n",
      "Train Epoch: 3 [2000/6421 (31%)]\tLoss: 0.020998\n",
      "Train Epoch: 3 [2500/6421 (39%)]\tLoss: 0.053884\n",
      "Train Epoch: 3 [3000/6421 (47%)]\tLoss: 0.039356\n",
      "Train Epoch: 3 [3500/6421 (54%)]\tLoss: 0.149187\n",
      "Train Epoch: 3 [4000/6421 (62%)]\tLoss: 0.067854\n",
      "Train Epoch: 3 [4500/6421 (70%)]\tLoss: 0.101583\n",
      "Train Epoch: 3 [5000/6421 (78%)]\tLoss: 0.079707\n",
      "Train Epoch: 3 [5500/6421 (86%)]\tLoss: 0.016027\n",
      "Train Epoch: 3 [6000/6421 (93%)]\tLoss: 0.214449\n",
      "\n",
      "Test set: Average loss: 0.0949, Accuracy: 6246/6421 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/6421 (0%)]\tLoss: 0.012855\n",
      "Train Epoch: 4 [500/6421 (8%)]\tLoss: 0.119941\n",
      "Train Epoch: 4 [1000/6421 (16%)]\tLoss: 0.015713\n",
      "Train Epoch: 4 [1500/6421 (23%)]\tLoss: 0.061861\n",
      "Train Epoch: 4 [2000/6421 (31%)]\tLoss: 0.041287\n",
      "Train Epoch: 4 [2500/6421 (39%)]\tLoss: 0.256004\n",
      "Train Epoch: 4 [3000/6421 (47%)]\tLoss: 0.065811\n",
      "Train Epoch: 4 [3500/6421 (54%)]\tLoss: 0.211883\n",
      "Train Epoch: 4 [4000/6421 (62%)]\tLoss: 0.043112\n",
      "Train Epoch: 4 [4500/6421 (70%)]\tLoss: 0.035907\n",
      "Train Epoch: 4 [5000/6421 (78%)]\tLoss: 0.091850\n",
      "Train Epoch: 4 [5500/6421 (86%)]\tLoss: 0.035243\n",
      "Train Epoch: 4 [6000/6421 (93%)]\tLoss: 0.076098\n",
      "\n",
      "Test set: Average loss: 0.0972, Accuracy: 6242/6421 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/6421 (0%)]\tLoss: 0.101396\n",
      "Train Epoch: 5 [500/6421 (8%)]\tLoss: 0.030647\n",
      "Train Epoch: 5 [1000/6421 (16%)]\tLoss: 0.064100\n",
      "Train Epoch: 5 [1500/6421 (23%)]\tLoss: 0.182577\n",
      "Train Epoch: 5 [2000/6421 (31%)]\tLoss: 0.191182\n",
      "Train Epoch: 5 [2500/6421 (39%)]\tLoss: 0.292523\n",
      "Train Epoch: 5 [3000/6421 (47%)]\tLoss: 0.087683\n",
      "Train Epoch: 5 [3500/6421 (54%)]\tLoss: 0.000750\n",
      "Train Epoch: 5 [4000/6421 (62%)]\tLoss: 0.064348\n",
      "Train Epoch: 5 [4500/6421 (70%)]\tLoss: 0.152698\n",
      "Train Epoch: 5 [5000/6421 (78%)]\tLoss: 0.241080\n",
      "Train Epoch: 5 [5500/6421 (86%)]\tLoss: 0.028797\n",
      "Train Epoch: 5 [6000/6421 (93%)]\tLoss: 0.205214\n",
      "\n",
      "Test set: Average loss: 0.0952, Accuracy: 6233/6421 (97%)\n",
      "\n",
      "Train Epoch: 6 [0/6421 (0%)]\tLoss: 0.019515\n",
      "Train Epoch: 6 [500/6421 (8%)]\tLoss: 0.054404\n",
      "Train Epoch: 6 [1000/6421 (16%)]\tLoss: 0.024312\n",
      "Train Epoch: 6 [1500/6421 (23%)]\tLoss: 0.194392\n",
      "Train Epoch: 6 [2000/6421 (31%)]\tLoss: 0.070195\n",
      "Train Epoch: 6 [2500/6421 (39%)]\tLoss: 0.013169\n",
      "Train Epoch: 6 [3000/6421 (47%)]\tLoss: 0.128733\n",
      "Train Epoch: 6 [3500/6421 (54%)]\tLoss: 0.028932\n",
      "Train Epoch: 6 [4000/6421 (62%)]\tLoss: 0.351769\n",
      "Train Epoch: 6 [4500/6421 (70%)]\tLoss: 0.083707\n",
      "Train Epoch: 6 [5000/6421 (78%)]\tLoss: 0.199642\n",
      "Train Epoch: 6 [5500/6421 (86%)]\tLoss: 0.063545\n",
      "Train Epoch: 6 [6000/6421 (93%)]\tLoss: 0.103169\n",
      "\n",
      "Test set: Average loss: 0.0964, Accuracy: 6258/6421 (97%)\n",
      "\n",
      "Train Epoch: 7 [0/6421 (0%)]\tLoss: 0.030470\n",
      "Train Epoch: 7 [500/6421 (8%)]\tLoss: 0.051900\n",
      "Train Epoch: 7 [1000/6421 (16%)]\tLoss: 0.036736\n",
      "Train Epoch: 7 [1500/6421 (23%)]\tLoss: 0.176721\n",
      "Train Epoch: 7 [2000/6421 (31%)]\tLoss: 0.032521\n",
      "Train Epoch: 7 [2500/6421 (39%)]\tLoss: 0.058601\n",
      "Train Epoch: 7 [3000/6421 (47%)]\tLoss: 0.049856\n",
      "Train Epoch: 7 [3500/6421 (54%)]\tLoss: 0.190768\n",
      "Train Epoch: 7 [4000/6421 (62%)]\tLoss: 0.031534\n",
      "Train Epoch: 7 [4500/6421 (70%)]\tLoss: 0.029874\n",
      "Train Epoch: 7 [5000/6421 (78%)]\tLoss: 0.052386\n",
      "Train Epoch: 7 [5500/6421 (86%)]\tLoss: 0.109500\n",
      "Train Epoch: 7 [6000/6421 (93%)]\tLoss: 0.011604\n",
      "\n",
      "Test set: Average loss: 0.0927, Accuracy: 6241/6421 (97%)\n",
      "\n",
      "Train Epoch: 8 [0/6421 (0%)]\tLoss: 0.105583\n",
      "Train Epoch: 8 [500/6421 (8%)]\tLoss: 0.006342\n",
      "Train Epoch: 8 [1000/6421 (16%)]\tLoss: 0.061674\n",
      "Train Epoch: 8 [1500/6421 (23%)]\tLoss: 0.108845\n",
      "Train Epoch: 8 [2000/6421 (31%)]\tLoss: 0.036473\n",
      "Train Epoch: 8 [2500/6421 (39%)]\tLoss: 0.058228\n",
      "Train Epoch: 8 [3000/6421 (47%)]\tLoss: 0.066603\n",
      "Train Epoch: 8 [3500/6421 (54%)]\tLoss: 0.078954\n",
      "Train Epoch: 8 [4000/6421 (62%)]\tLoss: 0.049511\n",
      "Train Epoch: 8 [4500/6421 (70%)]\tLoss: 0.211757\n",
      "Train Epoch: 8 [5000/6421 (78%)]\tLoss: 0.038577\n",
      "Train Epoch: 8 [5500/6421 (86%)]\tLoss: 0.009393\n",
      "Train Epoch: 8 [6000/6421 (93%)]\tLoss: 0.067302\n",
      "\n",
      "Test set: Average loss: 0.0936, Accuracy: 6233/6421 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/6421 (0%)]\tLoss: 0.003499\n",
      "Train Epoch: 9 [500/6421 (8%)]\tLoss: 0.023393\n",
      "Train Epoch: 9 [1000/6421 (16%)]\tLoss: 0.029774\n",
      "Train Epoch: 9 [1500/6421 (23%)]\tLoss: 0.173367\n",
      "Train Epoch: 9 [2000/6421 (31%)]\tLoss: 0.069113\n",
      "Train Epoch: 9 [2500/6421 (39%)]\tLoss: 0.017923\n",
      "Train Epoch: 9 [3000/6421 (47%)]\tLoss: 0.027843\n",
      "Train Epoch: 9 [3500/6421 (54%)]\tLoss: 0.090621\n",
      "Train Epoch: 9 [4000/6421 (62%)]\tLoss: 0.180660\n",
      "Train Epoch: 9 [4500/6421 (70%)]\tLoss: 0.145609\n",
      "Train Epoch: 9 [5000/6421 (78%)]\tLoss: 0.051861\n",
      "Train Epoch: 9 [5500/6421 (86%)]\tLoss: 0.083063\n",
      "Train Epoch: 9 [6000/6421 (93%)]\tLoss: 0.082860\n",
      "\n",
      "Test set: Average loss: 0.0929, Accuracy: 6245/6421 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/6421 (0%)]\tLoss: 0.152599\n",
      "Train Epoch: 10 [500/6421 (8%)]\tLoss: 0.076812\n",
      "Train Epoch: 10 [1000/6421 (16%)]\tLoss: 0.029103\n",
      "Train Epoch: 10 [1500/6421 (23%)]\tLoss: 0.066129\n",
      "Train Epoch: 10 [2000/6421 (31%)]\tLoss: 0.047999\n",
      "Train Epoch: 10 [2500/6421 (39%)]\tLoss: 0.047660\n",
      "Train Epoch: 10 [3000/6421 (47%)]\tLoss: 0.117097\n",
      "Train Epoch: 10 [3500/6421 (54%)]\tLoss: 0.017418\n",
      "Train Epoch: 10 [4000/6421 (62%)]\tLoss: 0.049662\n",
      "Train Epoch: 10 [4500/6421 (70%)]\tLoss: 0.051769\n",
      "Train Epoch: 10 [5000/6421 (78%)]\tLoss: 0.151201\n",
      "Train Epoch: 10 [5500/6421 (86%)]\tLoss: 0.024722\n",
      "Train Epoch: 10 [6000/6421 (93%)]\tLoss: 0.033890\n",
      "\n",
      "Test set: Average loss: 0.0922, Accuracy: 6235/6421 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/6421 (0%)]\tLoss: 0.037976\n",
      "Train Epoch: 11 [500/6421 (8%)]\tLoss: 0.021898\n",
      "Train Epoch: 11 [1000/6421 (16%)]\tLoss: 0.035328\n",
      "Train Epoch: 11 [1500/6421 (23%)]\tLoss: 0.009180\n",
      "Train Epoch: 11 [2000/6421 (31%)]\tLoss: 0.069284\n",
      "Train Epoch: 11 [2500/6421 (39%)]\tLoss: 0.053287\n",
      "Train Epoch: 11 [3000/6421 (47%)]\tLoss: 0.132383\n",
      "Train Epoch: 11 [3500/6421 (54%)]\tLoss: 0.129222\n",
      "Train Epoch: 11 [4000/6421 (62%)]\tLoss: 0.065235\n",
      "Train Epoch: 11 [4500/6421 (70%)]\tLoss: 0.111766\n",
      "Train Epoch: 11 [5000/6421 (78%)]\tLoss: 0.155152\n",
      "Train Epoch: 11 [5500/6421 (86%)]\tLoss: 0.040593\n",
      "Train Epoch: 11 [6000/6421 (93%)]\tLoss: 0.055759\n",
      "\n",
      "Test set: Average loss: 0.0919, Accuracy: 6251/6421 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/6421 (0%)]\tLoss: 0.049466\n",
      "Train Epoch: 12 [500/6421 (8%)]\tLoss: 0.188533\n",
      "Train Epoch: 12 [1000/6421 (16%)]\tLoss: 0.138684\n",
      "Train Epoch: 12 [1500/6421 (23%)]\tLoss: 0.137097\n",
      "Train Epoch: 12 [2000/6421 (31%)]\tLoss: 0.034286\n",
      "Train Epoch: 12 [2500/6421 (39%)]\tLoss: 0.082414\n",
      "Train Epoch: 12 [3000/6421 (47%)]\tLoss: 0.055640\n",
      "Train Epoch: 12 [3500/6421 (54%)]\tLoss: 0.130247\n",
      "Train Epoch: 12 [4000/6421 (62%)]\tLoss: 0.057913\n",
      "Train Epoch: 12 [4500/6421 (70%)]\tLoss: 0.111209\n",
      "Train Epoch: 12 [5000/6421 (78%)]\tLoss: 0.074459\n",
      "Train Epoch: 12 [5500/6421 (86%)]\tLoss: 0.063863\n",
      "Train Epoch: 12 [6000/6421 (93%)]\tLoss: 0.048006\n",
      "\n",
      "Test set: Average loss: 0.0907, Accuracy: 6273/6421 (98%)\n",
      "\n",
      "Train Epoch: 13 [0/6421 (0%)]\tLoss: 0.072844\n",
      "Train Epoch: 13 [500/6421 (8%)]\tLoss: 0.095437\n",
      "Train Epoch: 13 [1000/6421 (16%)]\tLoss: 0.059067\n",
      "Train Epoch: 13 [1500/6421 (23%)]\tLoss: 0.034170\n",
      "Train Epoch: 13 [2000/6421 (31%)]\tLoss: 0.075045\n",
      "Train Epoch: 13 [2500/6421 (39%)]\tLoss: 0.043977\n",
      "Train Epoch: 13 [3000/6421 (47%)]\tLoss: 0.010367\n",
      "Train Epoch: 13 [3500/6421 (54%)]\tLoss: 0.079339\n",
      "Train Epoch: 13 [4000/6421 (62%)]\tLoss: 0.095357\n",
      "Train Epoch: 13 [4500/6421 (70%)]\tLoss: 0.064536\n",
      "Train Epoch: 13 [5000/6421 (78%)]\tLoss: 0.167547\n",
      "Train Epoch: 13 [5500/6421 (86%)]\tLoss: 0.074220\n",
      "Train Epoch: 13 [6000/6421 (93%)]\tLoss: 0.328034\n",
      "\n",
      "Test set: Average loss: 0.0959, Accuracy: 6243/6421 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/6421 (0%)]\tLoss: 0.181592\n",
      "Train Epoch: 14 [500/6421 (8%)]\tLoss: 0.054510\n",
      "Train Epoch: 14 [1000/6421 (16%)]\tLoss: 0.072268\n",
      "Train Epoch: 14 [1500/6421 (23%)]\tLoss: 0.321279\n",
      "Train Epoch: 14 [2000/6421 (31%)]\tLoss: 0.079958\n",
      "Train Epoch: 14 [2500/6421 (39%)]\tLoss: 0.178345\n",
      "Train Epoch: 14 [3000/6421 (47%)]\tLoss: 0.030616\n",
      "Train Epoch: 14 [3500/6421 (54%)]\tLoss: 0.057772\n",
      "Train Epoch: 14 [4000/6421 (62%)]\tLoss: 0.016384\n",
      "Train Epoch: 14 [4500/6421 (70%)]\tLoss: 0.038786\n",
      "Train Epoch: 14 [5000/6421 (78%)]\tLoss: 0.046120\n",
      "Train Epoch: 14 [5500/6421 (86%)]\tLoss: 0.012101\n",
      "Train Epoch: 14 [6000/6421 (93%)]\tLoss: 0.119168\n",
      "\n",
      "Test set: Average loss: 0.0872, Accuracy: 6276/6421 (98%)\n",
      "\n",
      "Train Epoch: 15 [0/6421 (0%)]\tLoss: 0.073152\n",
      "Train Epoch: 15 [500/6421 (8%)]\tLoss: 0.062916\n",
      "Train Epoch: 15 [1000/6421 (16%)]\tLoss: 0.018339\n",
      "Train Epoch: 15 [1500/6421 (23%)]\tLoss: 0.056975\n",
      "Train Epoch: 15 [2000/6421 (31%)]\tLoss: 0.047105\n",
      "Train Epoch: 15 [2500/6421 (39%)]\tLoss: 0.059899\n",
      "Train Epoch: 15 [3000/6421 (47%)]\tLoss: 0.063449\n",
      "Train Epoch: 15 [3500/6421 (54%)]\tLoss: 0.081300\n",
      "Train Epoch: 15 [4000/6421 (62%)]\tLoss: 0.208901\n",
      "Train Epoch: 15 [4500/6421 (70%)]\tLoss: 0.130526\n",
      "Train Epoch: 15 [5000/6421 (78%)]\tLoss: 0.031168\n",
      "Train Epoch: 15 [5500/6421 (86%)]\tLoss: 0.044524\n",
      "Train Epoch: 15 [6000/6421 (93%)]\tLoss: 0.254812\n",
      "\n",
      "Test set: Average loss: 0.0912, Accuracy: 6258/6421 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/6421 (0%)]\tLoss: 0.022064\n",
      "Train Epoch: 16 [500/6421 (8%)]\tLoss: 0.097937\n",
      "Train Epoch: 16 [1000/6421 (16%)]\tLoss: 0.146208\n",
      "Train Epoch: 16 [1500/6421 (23%)]\tLoss: 0.203831\n",
      "Train Epoch: 16 [2000/6421 (31%)]\tLoss: 0.010807\n",
      "Train Epoch: 16 [2500/6421 (39%)]\tLoss: 0.042294\n",
      "Train Epoch: 16 [3000/6421 (47%)]\tLoss: 0.017938\n",
      "Train Epoch: 16 [3500/6421 (54%)]\tLoss: 0.092506\n",
      "Train Epoch: 16 [4000/6421 (62%)]\tLoss: 0.183237\n",
      "Train Epoch: 16 [4500/6421 (70%)]\tLoss: 0.222017\n",
      "Train Epoch: 16 [5000/6421 (78%)]\tLoss: 0.051659\n",
      "Train Epoch: 16 [5500/6421 (86%)]\tLoss: 0.175234\n",
      "Train Epoch: 16 [6000/6421 (93%)]\tLoss: 0.072469\n",
      "\n",
      "Test set: Average loss: 0.0939, Accuracy: 6253/6421 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/6421 (0%)]\tLoss: 0.070920\n",
      "Train Epoch: 17 [500/6421 (8%)]\tLoss: 0.093087\n",
      "Train Epoch: 17 [1000/6421 (16%)]\tLoss: 0.061265\n",
      "Train Epoch: 17 [1500/6421 (23%)]\tLoss: 0.099195\n",
      "Train Epoch: 17 [2000/6421 (31%)]\tLoss: 0.105250\n",
      "Train Epoch: 17 [2500/6421 (39%)]\tLoss: 0.291696\n",
      "Train Epoch: 17 [3000/6421 (47%)]\tLoss: 0.091838\n",
      "Train Epoch: 17 [3500/6421 (54%)]\tLoss: 0.013656\n",
      "Train Epoch: 17 [4000/6421 (62%)]\tLoss: 0.139142\n",
      "Train Epoch: 17 [4500/6421 (70%)]\tLoss: 0.079606\n",
      "Train Epoch: 17 [5000/6421 (78%)]\tLoss: 0.056982\n",
      "Train Epoch: 17 [5500/6421 (86%)]\tLoss: 0.107452\n",
      "Train Epoch: 17 [6000/6421 (93%)]\tLoss: 0.129279\n",
      "\n",
      "Test set: Average loss: 0.0910, Accuracy: 6247/6421 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/6421 (0%)]\tLoss: 0.215798\n",
      "Train Epoch: 18 [500/6421 (8%)]\tLoss: 0.060190\n",
      "Train Epoch: 18 [1000/6421 (16%)]\tLoss: 0.387486\n",
      "Train Epoch: 18 [1500/6421 (23%)]\tLoss: 0.066942\n",
      "Train Epoch: 18 [2000/6421 (31%)]\tLoss: 0.068103\n",
      "Train Epoch: 18 [2500/6421 (39%)]\tLoss: 0.029386\n",
      "Train Epoch: 18 [3000/6421 (47%)]\tLoss: 0.044760\n",
      "Train Epoch: 18 [3500/6421 (54%)]\tLoss: 0.083108\n",
      "Train Epoch: 18 [4000/6421 (62%)]\tLoss: 0.020541\n",
      "Train Epoch: 18 [4500/6421 (70%)]\tLoss: 0.071588\n",
      "Train Epoch: 18 [5000/6421 (78%)]\tLoss: 0.152643\n",
      "Train Epoch: 18 [5500/6421 (86%)]\tLoss: 0.072622\n",
      "Train Epoch: 18 [6000/6421 (93%)]\tLoss: 0.011230\n",
      "\n",
      "Test set: Average loss: 0.0955, Accuracy: 6229/6421 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/6421 (0%)]\tLoss: 0.029325\n",
      "Train Epoch: 19 [500/6421 (8%)]\tLoss: 0.145080\n",
      "Train Epoch: 19 [1000/6421 (16%)]\tLoss: 0.062812\n",
      "Train Epoch: 19 [1500/6421 (23%)]\tLoss: 0.012455\n",
      "Train Epoch: 19 [2000/6421 (31%)]\tLoss: 0.052056\n",
      "Train Epoch: 19 [2500/6421 (39%)]\tLoss: 0.080878\n",
      "Train Epoch: 19 [3000/6421 (47%)]\tLoss: 0.217140\n",
      "Train Epoch: 19 [3500/6421 (54%)]\tLoss: 0.143161\n",
      "Train Epoch: 19 [4000/6421 (62%)]\tLoss: 0.041577\n",
      "Train Epoch: 19 [4500/6421 (70%)]\tLoss: 0.186997\n",
      "Train Epoch: 19 [5000/6421 (78%)]\tLoss: 0.053673\n",
      "Train Epoch: 19 [5500/6421 (86%)]\tLoss: 0.053284\n",
      "Train Epoch: 19 [6000/6421 (93%)]\tLoss: 0.045860\n",
      "\n",
      "Test set: Average loss: 0.0903, Accuracy: 6267/6421 (98%)\n",
      "\n",
      "Train Epoch: 20 [0/6421 (0%)]\tLoss: 0.010472\n",
      "Train Epoch: 20 [500/6421 (8%)]\tLoss: 0.064911\n",
      "Train Epoch: 20 [1000/6421 (16%)]\tLoss: 0.097612\n",
      "Train Epoch: 20 [1500/6421 (23%)]\tLoss: 0.009778\n",
      "Train Epoch: 20 [2000/6421 (31%)]\tLoss: 0.068707\n",
      "Train Epoch: 20 [2500/6421 (39%)]\tLoss: 0.108533\n",
      "Train Epoch: 20 [3000/6421 (47%)]\tLoss: 0.231795\n",
      "Train Epoch: 20 [3500/6421 (54%)]\tLoss: 0.215481\n",
      "Train Epoch: 20 [4000/6421 (62%)]\tLoss: 0.167086\n",
      "Train Epoch: 20 [4500/6421 (70%)]\tLoss: 0.084762\n",
      "Train Epoch: 20 [5000/6421 (78%)]\tLoss: 0.000097\n",
      "Train Epoch: 20 [5500/6421 (86%)]\tLoss: 0.031979\n",
      "Train Epoch: 20 [6000/6421 (93%)]\tLoss: 0.249343\n",
      "\n",
      "Test set: Average loss: 0.0891, Accuracy: 6243/6421 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1)\n",
    "for epoch in range(1, 20 + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, weights)\n",
    "    test(model, device, test_loader, weights)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "lined-terror",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       774\n",
      "           1       0.99      0.99      0.99      5647\n",
      "\n",
      "    accuracy                           0.99      6421\n",
      "   macro avg       0.98      0.97      0.97      6421\n",
      "weighted avg       0.99      0.99      0.99      6421\n",
      "\n",
      "[[ 737   37]\n",
      " [  33 5614]]\n"
     ]
    }
   ],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=10,\n",
    "    **cuda_kwargs)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        all_preds.extend(pred.tolist())\n",
    "\n",
    "print(classification_report(dataset.labels, all_preds))\n",
    "print(confusion_matrix(dataset.labels, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "respected-study",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(outfolder, 'NN_sub.pkl'), 'wb') as file:\n",
    "    torch.save(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brown-producer",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfolder = 'data/out/all-apps/hindroid-train-set-ABAT/'\n",
    "with open(os.path.join(outfolder, 'NN_sub.pkl'), 'rb') as file:\n",
    "    model = torch.load(file).to(device)\n",
    "\n",
    "batch_size = 10\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=batch_size,\n",
    "    shuffle = True,\n",
    "#     sampler = ImbalancedDatasetSampler(dataset, callback_get_label = hindroid_custom_get_label),\n",
    "    **cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accredited-principal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.attack.cw import to_tanh_space, from_tanh_space, L2Adversary\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "corporate-alignment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1492, 0.8508],\n",
      "        [0.0412, 0.9588],\n",
      "        [0.0228, 0.9772],\n",
      "        [0.0208, 0.9792],\n",
      "        [0.0159, 0.9841],\n",
      "        [0.1544, 0.8456],\n",
      "        [0.1733, 0.8267],\n",
      "        [0.0073, 0.9927],\n",
      "        [0.1880, 0.8120],\n",
      "        [0.0300, 0.9700]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.9740514118266645\n",
      "batch [10] loss: 32546899.86583168\n",
      "batch [20] loss: 32546899.86583168\n",
      "batch [30] loss: 32546899.86583168\n",
      "batch [40] loss: 32546899.86583168\n",
      "batch [50] loss: 32546899.86583168\n",
      "batch [60] loss: 32546899.86583168\n",
      "batch [70] loss: 32546899.86583168\n",
      "batch [80] loss: 32546899.86583168\n",
      "batch [90] loss: 32546899.86583168\n",
      "batch [100] loss: 32546899.86583168\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 32546899.86583168\n",
      "batch [10] loss: 32546899.86583168\n",
      "batch [20] loss: 32546899.86583168\n",
      "batch [30] loss: 32546899.86583168\n",
      "batch [40] loss: 32546899.86583168\n",
      "batch [50] loss: 32546899.86583168\n",
      "batch [60] loss: 32546899.86583168\n",
      "batch [70] loss: 32546899.86583168\n",
      "batch [80] loss: 32546899.86583168\n",
      "batch [90] loss: 32546899.86583168\n",
      "batch [100] loss: 32546899.86583168\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 32546899.86583168\n",
      "batch [10] loss: 32546899.86583168\n",
      "batch [20] loss: 32546899.86583168\n",
      "batch [30] loss: 32546899.86583168\n",
      "batch [40] loss: 32546899.86583168\n",
      "batch [50] loss: 32546899.86583168\n",
      "batch [60] loss: 32546899.86583168\n",
      "batch [70] loss: 32546899.86583168\n",
      "batch [80] loss: 32546899.86583168\n",
      "batch [90] loss: 32546899.86583168\n",
      "batch [100] loss: 32546899.86583168\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1301, 0.8699],\n",
      "        [0.1641, 0.8359],\n",
      "        [0.0304, 0.9696],\n",
      "        [0.0283, 0.9717],\n",
      "        [0.1733, 0.8267],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0719, 0.9281],\n",
      "        [0.0610, 0.9390],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.0130, 0.9870]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.8409477177282425\n",
      "batch [10] loss: 27990699.88461374\n",
      "batch [20] loss: 27990699.88461374\n",
      "batch [30] loss: 27990699.88461374\n",
      "batch [40] loss: 27990699.88461374\n",
      "batch [50] loss: 27990699.88461374\n",
      "batch [60] loss: 27990699.88461374\n",
      "batch [70] loss: 27990699.88461374\n",
      "batch [80] loss: 27990699.88461374\n",
      "batch [90] loss: 27990699.884613737\n",
      "batch [100] loss: 27990699.884613737\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 27990699.884613737\n",
      "batch [10] loss: 27990699.884613737\n",
      "batch [20] loss: 27990699.884613737\n",
      "batch [30] loss: 27990699.884613737\n",
      "batch [40] loss: 27990699.884613737\n",
      "batch [50] loss: 27990699.884613737\n",
      "batch [60] loss: 27990699.884613737\n",
      "batch [70] loss: 27990699.884613737\n",
      "batch [80] loss: 27990699.884613737\n",
      "batch [90] loss: 27990699.884613737\n",
      "batch [100] loss: 27990699.884613737\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 27990699.884613737\n",
      "batch [10] loss: 27990699.884613737\n",
      "batch [20] loss: 27990699.884613737\n",
      "batch [30] loss: 27990699.884613737\n",
      "batch [40] loss: 27990699.884613737\n",
      "batch [50] loss: 27990699.884613737\n",
      "batch [60] loss: 27990699.884613737\n",
      "batch [70] loss: 27990699.884613737\n",
      "batch [80] loss: 27990699.884613737\n",
      "batch [90] loss: 27990699.884613737\n",
      "batch [100] loss: 27990699.884613737\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9995e-01, 4.8161e-05],\n",
      "        [1.0150e-02, 9.8985e-01],\n",
      "        [1.9429e-01, 8.0571e-01],\n",
      "        [1.7580e-01, 8.2420e-01],\n",
      "        [2.3256e-01, 7.6744e-01],\n",
      "        [8.2053e-03, 9.9179e-01],\n",
      "        [2.9794e-02, 9.7021e-01],\n",
      "        [9.3111e-02, 9.0689e-01],\n",
      "        [2.6480e-02, 9.7352e-01],\n",
      "        [2.7526e-02, 9.7247e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.6405806474513827\n",
      "batch [10] loss: 25802300.879875258\n",
      "batch [20] loss: 25802300.879875258\n",
      "batch [30] loss: 25802300.879875258\n",
      "batch [40] loss: 25802300.879875258\n",
      "batch [50] loss: 25802300.879875258\n",
      "batch [60] loss: 25802300.879875258\n",
      "batch [70] loss: 25802300.879875258\n",
      "batch [80] loss: 25802300.879875258\n",
      "batch [90] loss: 25802300.879875258\n",
      "batch [100] loss: 25802300.879875258\n",
      "Using scale consts: [1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25802309.756037496\n",
      "batch [10] loss: 25804122.71606905\n",
      "batch [20] loss: 25804072.095516928\n",
      "batch [30] loss: 25804206.385624446\n",
      "batch [40] loss: 25804239.574786037\n",
      "batch [50] loss: 25804243.590123948\n",
      "batch [60] loss: 25803583.665946703\n",
      "batch [70] loss: 25804126.566822153\n",
      "batch [80] loss: 25803854.156954065\n",
      "batch [90] loss: 25804001.97584483\n",
      "batch [100] loss: 25803989.314030945\n",
      "Using scale consts: [10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25804159.75383863\n",
      "batch [10] loss: 25877392.53566239\n",
      "batch [20] loss: 25954990.503964014\n",
      "batch [30] loss: 26022367.436708786\n",
      "batch [40] loss: 26081992.865304522\n",
      "batch [50] loss: 26149831.064673766\n",
      "batch [60] loss: 26230838.497533374\n",
      "batch [70] loss: 26305262.68092879\n",
      "batch [80] loss: 26383560.90938227\n",
      "batch [90] loss: 26467373.869516872\n",
      "batch [100] loss: 26582800.95190543\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[8.0620e-03, 9.9194e-01],\n",
      "        [1.6164e-01, 8.3836e-01],\n",
      "        [1.7831e-02, 9.8217e-01],\n",
      "        [1.7831e-02, 9.8217e-01],\n",
      "        [9.9970e-01, 3.0221e-04],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [2.7510e-01, 7.2490e-01],\n",
      "        [4.7169e-02, 9.5283e-01],\n",
      "        [8.6661e-03, 9.9133e-01],\n",
      "        [8.2503e-02, 9.1750e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.5603750077265497\n",
      "batch [10] loss: 25871000.692403316\n",
      "batch [20] loss: 25871000.692403316\n",
      "batch [30] loss: 25871000.692403316\n",
      "batch [40] loss: 25871000.692403316\n",
      "batch [50] loss: 25871000.692403316\n",
      "batch [60] loss: 25871000.692403316\n",
      "batch [70] loss: 25871000.692403316\n",
      "batch [80] loss: 25871000.692403316\n",
      "batch [90] loss: 25871000.692403316\n",
      "batch [100] loss: 25871000.692403316\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25871007.88386689\n",
      "batch [10] loss: 25872917.737110432\n",
      "batch [20] loss: 25873592.044260424\n",
      "batch [30] loss: 25872590.15426497\n",
      "batch [40] loss: 25872651.672701996\n",
      "batch [50] loss: 25873187.80625651\n",
      "batch [60] loss: 25873575.254032526\n",
      "batch [70] loss: 25873431.93634204\n",
      "batch [80] loss: 25874397.505515516\n",
      "batch [90] loss: 25873787.363885008\n",
      "batch [100] loss: 25873786.13576901\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25874250.724156804\n",
      "batch [10] loss: 25945005.891734865\n",
      "batch [20] loss: 26023150.50794153\n",
      "batch [30] loss: 26092275.833936647\n",
      "batch [40] loss: 26150308.144691754\n",
      "batch [50] loss: 26198189.08981149\n",
      "batch [60] loss: 26237095.646605298\n",
      "batch [70] loss: 26276353.500880316\n",
      "batch [80] loss: 26306663.897826523\n",
      "batch [90] loss: 26334579.583353743\n",
      "batch [100] loss: 26360855.56463808\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.2278, 0.7722],\n",
      "        [0.0198, 0.9802],\n",
      "        [0.0216, 0.9784],\n",
      "        [0.1545, 0.8455],\n",
      "        [0.0050, 0.9950],\n",
      "        [0.1733, 0.8267],\n",
      "        [0.0502, 0.9498],\n",
      "        [0.1887, 0.8113],\n",
      "        [0.1683, 0.8317],\n",
      "        [0.1396, 0.8604]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.5297750116950786\n",
      "batch [10] loss: 24413599.899359643\n",
      "batch [20] loss: 24413599.899359643\n",
      "batch [30] loss: 24413599.899359643\n",
      "batch [40] loss: 24413599.899359643\n",
      "batch [50] loss: 24413599.899359643\n",
      "batch [60] loss: 24413599.899359643\n",
      "batch [70] loss: 24413599.899359643\n",
      "batch [80] loss: 24413599.899359643\n",
      "batch [90] loss: 24413599.899359643\n",
      "batch [100] loss: 24413599.899359643\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 24413599.899359643\n",
      "batch [10] loss: 24413599.899359643\n",
      "batch [20] loss: 24413599.899359643\n",
      "batch [30] loss: 24413599.899359643\n",
      "batch [40] loss: 24413599.899359643\n",
      "batch [50] loss: 24413599.899359643\n",
      "batch [60] loss: 24413599.899359643\n",
      "batch [70] loss: 24413599.899359643\n",
      "batch [80] loss: 24413599.899359643\n",
      "batch [90] loss: 24413599.899359643\n",
      "batch [100] loss: 24413599.899359643\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 24413599.899359643\n",
      "batch [10] loss: 24413599.899359643\n",
      "batch [20] loss: 24413599.899359643\n",
      "batch [30] loss: 24413599.899359643\n",
      "batch [40] loss: 24413599.899359643\n",
      "batch [50] loss: 24413599.899359643\n",
      "batch [60] loss: 24413599.899359643\n",
      "batch [70] loss: 24413599.899359643\n",
      "batch [80] loss: 24413599.899359643\n",
      "batch [90] loss: 24413599.899359643\n",
      "batch [100] loss: 24413599.899359643\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[3.4106e-01, 6.5894e-01],\n",
      "        [7.3011e-03, 9.9270e-01],\n",
      "        [2.8262e-02, 9.7174e-01],\n",
      "        [1.7183e-01, 8.2817e-01],\n",
      "        [1.4692e-01, 8.5308e-01],\n",
      "        [3.0509e-02, 9.6949e-01],\n",
      "        [9.9986e-01, 1.3736e-04],\n",
      "        [1.6006e-02, 9.8399e-01],\n",
      "        [1.7768e-01, 8.2232e-01],\n",
      "        [1.5649e-01, 8.4351e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.2127137827832986\n",
      "batch [10] loss: 26583700.77182509\n",
      "batch [20] loss: 26583700.77182509\n",
      "batch [30] loss: 26583700.77182509\n",
      "batch [40] loss: 26583700.77182509\n",
      "batch [50] loss: 26583700.77182509\n",
      "batch [60] loss: 26583700.771825086\n",
      "batch [70] loss: 26583700.771825086\n",
      "batch [80] loss: 26583700.771825086\n",
      "batch [90] loss: 26583700.771825086\n",
      "batch [100] loss: 26583700.771825086\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26583708.704526316\n",
      "batch [10] loss: 26586329.908996195\n",
      "batch [20] loss: 26586273.66709341\n",
      "batch [30] loss: 26585385.356092215\n",
      "batch [40] loss: 26585606.07961519\n",
      "batch [50] loss: 26585789.275539886\n",
      "batch [60] loss: 26585999.062171604\n",
      "batch [70] loss: 26585855.30652421\n",
      "batch [80] loss: 26585528.682972398\n",
      "batch [90] loss: 26585810.127700787\n",
      "batch [100] loss: 26585872.180486076\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 26586330.76223179\n",
      "batch [10] loss: 26664891.391160812\n",
      "batch [20] loss: 26747011.16589231\n",
      "batch [30] loss: 26818474.21018881\n",
      "batch [40] loss: 26880267.889854416\n",
      "batch [50] loss: 26934354.064134724\n",
      "batch [60] loss: 27026457.167197976\n",
      "batch [70] loss: 27189093.988194026\n",
      "batch [80] loss: 27348184.345015794\n",
      "batch [90] loss: 27538962.335146274\n",
      "batch [100] loss: 27766630.714790937\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1])\n",
      "Model pred:  tensor([[0.0601, 0.9399],\n",
      "        [0.0044, 0.9956],\n",
      "        [0.0294, 0.9706],\n",
      "        [0.0095, 0.9905],\n",
      "        [0.9978, 0.0022],\n",
      "        [0.2284, 0.7716],\n",
      "        [0.0721, 0.9279],\n",
      "        [0.9931, 0.0069],\n",
      "        [0.0295, 0.9705],\n",
      "        [0.1758, 0.8242]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.618073897462331\n",
      "batch [10] loss: 26483571.706480887\n",
      "batch [20] loss: 26446823.461849064\n",
      "batch [30] loss: 26442849.536504112\n",
      "batch [40] loss: 26443374.93850003\n",
      "batch [50] loss: 26442723.87847057\n",
      "batch [60] loss: 26443423.1428226\n",
      "batch [70] loss: 26443937.404674254\n",
      "batch [80] loss: 26443711.555549394\n",
      "batch [90] loss: 26444894.516928934\n",
      "batch [100] loss: 26444460.79846684\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26444694.51133208\n",
      "batch [10] loss: 26444100.242399804\n",
      "batch [20] loss: 26444100.131576214\n",
      "batch [30] loss: 26444100.13144311\n",
      "batch [40] loss: 26444185.27111108\n",
      "batch [50] loss: 26444281.796448365\n",
      "batch [60] loss: 26444300.170051325\n",
      "batch [70] loss: 26444300.13118252\n",
      "batch [80] loss: 26444300.131163802\n",
      "batch [90] loss: 26444491.094897877\n",
      "batch [100] loss: 26444400.135743648\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025]\n",
      "batch [0] loss: 26444400.251214635\n",
      "batch [10] loss: 26444567.019983463\n",
      "batch [20] loss: 26444602.509451505\n",
      "batch [30] loss: 26444499.70380731\n",
      "batch [40] loss: 26444400.297668792\n",
      "batch [50] loss: 26444499.372276805\n",
      "batch [60] loss: 26444500.255963396\n",
      "batch [70] loss: 26444597.962620556\n",
      "batch [80] loss: 26444500.2597125\n",
      "batch [90] loss: 26444700.252016064\n",
      "batch [100] loss: 26444800.28073285\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[2.3720e-02, 9.7628e-01],\n",
      "        [1.2935e-02, 9.8706e-01],\n",
      "        [2.1483e-01, 7.8517e-01],\n",
      "        [9.9993e-01, 6.9580e-05],\n",
      "        [1.4776e-02, 9.8522e-01],\n",
      "        [1.9323e-01, 8.0677e-01],\n",
      "        [2.9741e-02, 9.7026e-01],\n",
      "        [2.2025e-01, 7.7975e-01],\n",
      "        [3.9786e-01, 6.0214e-01],\n",
      "        [9.9897e-01, 1.0321e-03]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.658906825131892\n",
      "batch [10] loss: 38468111.22307736\n",
      "batch [20] loss: 38458614.195457384\n",
      "batch [30] loss: 38457859.66172837\n",
      "batch [40] loss: 38458297.15256644\n",
      "batch [50] loss: 38458147.9922238\n",
      "batch [60] loss: 38458404.708205946\n",
      "batch [70] loss: 38458259.3662311\n",
      "batch [80] loss: 38458432.25471312\n",
      "batch [90] loss: 38458388.22625221\n",
      "batch [100] loss: 38458501.34921458\n",
      "Using scale consts: [0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 38458609.207277514\n",
      "batch [10] loss: 38461132.25022672\n",
      "batch [20] loss: 38460079.13128805\n",
      "batch [30] loss: 38460968.081930384\n",
      "batch [40] loss: 38461112.17593899\n",
      "batch [50] loss: 38460561.88706064\n",
      "batch [60] loss: 38460412.522194296\n",
      "batch [70] loss: 38460978.243770495\n",
      "batch [80] loss: 38460661.247147895\n",
      "batch [90] loss: 38460457.2435185\n",
      "batch [100] loss: 38460500.40626573\n",
      "Using scale consts: [0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001]\n",
      "batch [0] loss: 38460781.91474702\n",
      "batch [10] loss: 38533805.69648425\n",
      "batch [20] loss: 38610073.803058244\n",
      "batch [30] loss: 38682887.85252987\n",
      "batch [40] loss: 38744417.28765428\n",
      "batch [50] loss: 38802084.47112906\n",
      "batch [60] loss: 38845900.70857651\n",
      "batch [70] loss: 38885460.32765612\n",
      "batch [80] loss: 38920185.53660388\n",
      "batch [90] loss: 38950343.52932895\n",
      "batch [100] loss: 38976487.08954704\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[3.0509e-02, 9.6949e-01],\n",
      "        [9.9958e-01, 4.2367e-04],\n",
      "        [9.8270e-03, 9.9017e-01],\n",
      "        [1.7580e-01, 8.2420e-01],\n",
      "        [2.3254e-01, 7.6746e-01],\n",
      "        [1.7575e-01, 8.2425e-01],\n",
      "        [1.6984e-01, 8.3016e-01],\n",
      "        [9.8162e-02, 9.0184e-01],\n",
      "        [1.5323e-01, 8.4677e-01],\n",
      "        [2.9966e-01, 7.0034e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.6485062989713133\n",
      "batch [10] loss: 23946900.67093122\n",
      "batch [20] loss: 23946900.67093122\n",
      "batch [30] loss: 23946900.67093122\n",
      "batch [40] loss: 23946900.67093122\n",
      "batch [50] loss: 23946900.67093122\n",
      "batch [60] loss: 23946900.67093122\n",
      "batch [70] loss: 23946900.67093122\n",
      "batch [80] loss: 23946900.67093122\n",
      "batch [90] loss: 23946900.67093122\n",
      "batch [100] loss: 23946900.67093122\n",
      "Using scale consts: [0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 23946907.597760364\n",
      "batch [10] loss: 23948676.45232542\n",
      "batch [20] loss: 23948824.0090992\n",
      "batch [30] loss: 23948488.821977388\n",
      "batch [40] loss: 23948458.60098491\n",
      "batch [50] loss: 23947951.222590975\n",
      "batch [60] loss: 23948214.93138459\n",
      "batch [70] loss: 23948249.520184353\n",
      "batch [80] loss: 23948238.438170776\n",
      "batch [90] loss: 23948202.891522035\n",
      "batch [100] loss: 23948154.005264565\n",
      "Using scale consts: [0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 23948182.52878491\n",
      "batch [10] loss: 24046130.511685237\n",
      "batch [20] loss: 24149412.222770922\n",
      "batch [30] loss: 24236227.18761008\n",
      "batch [40] loss: 24317472.461889848\n",
      "batch [50] loss: 24391160.89947904\n",
      "batch [60] loss: 24456057.48681075\n",
      "batch [70] loss: 24516260.222629383\n",
      "batch [80] loss: 24576660.899292663\n",
      "batch [90] loss: 24640898.12728392\n",
      "batch [100] loss: 24737969.337914445\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[0.0883, 0.9117],\n",
      "        [0.0783, 0.9217],\n",
      "        [0.2224, 0.7776],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0297, 0.9703],\n",
      "        [0.0706, 0.9294],\n",
      "        [0.0300, 0.9700],\n",
      "        [0.0529, 0.9471],\n",
      "        [0.0163, 0.9837],\n",
      "        [0.9982, 0.0018]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.0783295163168387\n",
      "batch [10] loss: 26374668.871868365\n",
      "batch [20] loss: 26370803.694107406\n",
      "batch [30] loss: 26370706.825447172\n",
      "batch [40] loss: 26370638.830467634\n",
      "batch [50] loss: 26370795.763045974\n",
      "batch [60] loss: 26370504.099019475\n",
      "batch [70] loss: 26370500.27268759\n",
      "batch [80] loss: 26370602.948346302\n",
      "batch [90] loss: 26370500.275425732\n",
      "batch [100] loss: 26370500.366488412\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26370500.084811635\n",
      "batch [10] loss: 26370500.081983484\n",
      "batch [20] loss: 26370500.081982374\n",
      "batch [30] loss: 26370500.08198238\n",
      "batch [40] loss: 26370500.08198239\n",
      "batch [50] loss: 26370500.08198251\n",
      "batch [60] loss: 26370500.08198238\n",
      "batch [70] loss: 26370500.081982374\n",
      "batch [80] loss: 26370500.081982374\n",
      "batch [90] loss: 26370500.081982374\n",
      "batch [100] loss: 26370500.081982374\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001]\n",
      "batch [0] loss: 26370500.177327223\n",
      "batch [10] loss: 26370500.177327223\n",
      "batch [20] loss: 26370500.177334405\n",
      "batch [30] loss: 26370500.17732723\n",
      "batch [40] loss: 26370500.177327223\n",
      "batch [50] loss: 26370500.177327238\n",
      "batch [60] loss: 26370500.177330554\n",
      "batch [70] loss: 26370500.177327223\n",
      "batch [80] loss: 26370500.177327298\n",
      "batch [90] loss: 26370500.17732723\n",
      "batch [100] loss: 26370500.17732738\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[0.0287, 0.9713],\n",
      "        [0.0459, 0.9541],\n",
      "        [0.2220, 0.7780],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0706, 0.9294],\n",
      "        [0.0798, 0.9202],\n",
      "        [0.2320, 0.7680],\n",
      "        [0.1758, 0.8242],\n",
      "        [0.0462, 0.9538],\n",
      "        [0.9990, 0.0010]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.739445715462725\n",
      "batch [10] loss: 21903775.225594446\n",
      "batch [20] loss: 21893935.82679247\n",
      "batch [30] loss: 21893090.54580646\n",
      "batch [40] loss: 21892813.514385708\n",
      "batch [50] loss: 21892726.15148122\n",
      "batch [60] loss: 21893260.614041395\n",
      "batch [70] loss: 21892923.165204167\n",
      "batch [80] loss: 21892757.912682764\n",
      "batch [90] loss: 21892712.121550016\n",
      "batch [100] loss: 21893038.585520722\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0]\n",
      "batch [0] loss: 21892707.403800644\n",
      "batch [10] loss: 22020092.329809994\n",
      "batch [20] loss: 22139034.55015123\n",
      "batch [30] loss: 22280739.027025394\n",
      "batch [40] loss: 22402780.89222069\n",
      "batch [50] loss: 22504209.91815976\n",
      "batch [60] loss: 22595748.870217413\n",
      "batch [70] loss: 22676559.486679092\n",
      "batch [80] loss: 22749769.54498024\n",
      "batch [90] loss: 22807281.59857825\n",
      "batch [100] loss: 22858857.28986226\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0]\n",
      "batch [0] loss: 22864115.787074484\n",
      "batch [10] loss: 23271582.188514486\n",
      "batch [20] loss: 23717181.359199874\n",
      "batch [30] loss: 24106153.026727017\n",
      "batch [40] loss: 24448516.80046159\n",
      "batch [50] loss: 24732797.213200755\n",
      "batch [60] loss: 24979833.736121126\n",
      "batch [70] loss: 25196970.904093377\n",
      "batch [80] loss: 25384901.51186493\n",
      "batch [90] loss: 25555537.754494593\n",
      "batch [100] loss: 25713580.68752256\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[0.2939, 0.7061],\n",
      "        [0.0181, 0.9819],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.0087, 0.9913],\n",
      "        [0.2319, 0.7681],\n",
      "        [0.1138, 0.8862],\n",
      "        [0.9989, 0.0011],\n",
      "        [0.0302, 0.9698],\n",
      "        [0.3433, 0.6567],\n",
      "        [0.1342, 0.8658]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.0500321163445774\n",
      "batch [10] loss: 24051299.666897036\n",
      "batch [20] loss: 24051100.572538074\n",
      "batch [30] loss: 24051100.572538048\n",
      "batch [40] loss: 24051100.572538048\n",
      "batch [50] loss: 24051100.572538048\n",
      "batch [60] loss: 24051100.572538048\n",
      "batch [70] loss: 24051100.572538197\n",
      "batch [80] loss: 24051100.572538048\n",
      "batch [90] loss: 24051100.572538048\n",
      "batch [100] loss: 24051100.572538048\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 24051106.617889933\n",
      "batch [10] loss: 24062642.187502638\n",
      "batch [20] loss: 24072402.73207691\n",
      "batch [30] loss: 24078922.58923915\n",
      "batch [40] loss: 24086689.977651075\n",
      "batch [50] loss: 24092846.14414209\n",
      "batch [60] loss: 24096028.285491843\n",
      "batch [70] loss: 24100787.973945774\n",
      "batch [80] loss: 24104559.37275247\n",
      "batch [90] loss: 24108283.352619704\n",
      "batch [100] loss: 24110185.904565174\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 24110864.12297905\n",
      "batch [10] loss: 24248588.92486622\n",
      "batch [20] loss: 24496972.651638024\n",
      "batch [30] loss: 25080980.824468687\n",
      "batch [40] loss: 25817278.029764693\n",
      "batch [50] loss: 26558259.02628024\n",
      "batch [60] loss: 27223455.23596551\n",
      "batch [70] loss: 27816390.422263987\n",
      "batch [80] loss: 28314348.662045695\n",
      "batch [90] loss: 28741324.093380693\n",
      "batch [100] loss: 29119248.843750026\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1469, 0.8531],\n",
      "        [0.1755, 0.8245],\n",
      "        [0.1469, 0.8531],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.1321, 0.8679],\n",
      "        [0.1273, 0.8727],\n",
      "        [0.5260, 0.4740],\n",
      "        [0.2220, 0.7780],\n",
      "        [0.1766, 0.8234],\n",
      "        [0.1864, 0.8136]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 1.481604371459602\n",
      "batch [10] loss: 18987699.92172687\n",
      "batch [20] loss: 18987699.92172687\n",
      "batch [30] loss: 18987699.92172687\n",
      "batch [40] loss: 18987699.92172687\n",
      "batch [50] loss: 18987699.92172687\n",
      "batch [60] loss: 18987699.92172687\n",
      "batch [70] loss: 18987699.92172687\n",
      "batch [80] loss: 18987699.92172687\n",
      "batch [90] loss: 18987699.92172687\n",
      "batch [100] loss: 18987699.92172687\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 18987699.92172687\n",
      "batch [10] loss: 18987699.92172687\n",
      "batch [20] loss: 18987699.92172687\n",
      "batch [30] loss: 18987699.92172687\n",
      "batch [40] loss: 18987699.92172687\n",
      "batch [50] loss: 18987699.92172687\n",
      "batch [60] loss: 18987699.92172687\n",
      "batch [70] loss: 18987699.92172687\n",
      "batch [80] loss: 18987699.92172687\n",
      "batch [90] loss: 18987699.92172687\n",
      "batch [100] loss: 18987699.92172687\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 18987699.92172687\n",
      "batch [10] loss: 18987699.92172687\n",
      "batch [20] loss: 18987699.92172687\n",
      "batch [30] loss: 18987699.92172687\n",
      "batch [40] loss: 18987699.92172687\n",
      "batch [50] loss: 18987699.92172687\n",
      "batch [60] loss: 18987699.92172687\n",
      "batch [70] loss: 18987699.92172687\n",
      "batch [80] loss: 18987699.92172687\n",
      "batch [90] loss: 18987699.92172687\n",
      "batch [100] loss: 18987699.92172687\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1887, 0.8113],\n",
      "        [0.2578, 0.7422],\n",
      "        [0.0938, 0.9062],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0992, 0.9008],\n",
      "        [0.0220, 0.9780],\n",
      "        [0.0754, 0.9246],\n",
      "        [0.2320, 0.7680],\n",
      "        [0.0931, 0.9069],\n",
      "        [0.1599, 0.8401]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.032328450864044\n",
      "batch [10] loss: 22379499.907744825\n",
      "batch [20] loss: 22379499.907744825\n",
      "batch [30] loss: 22379499.907744825\n",
      "batch [40] loss: 22379499.907744825\n",
      "batch [50] loss: 22379499.907744825\n",
      "batch [60] loss: 22379499.907744825\n",
      "batch [70] loss: 22379499.907744825\n",
      "batch [80] loss: 22379499.907744825\n",
      "batch [90] loss: 22379499.907744825\n",
      "batch [100] loss: 22379499.907744825\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 22379499.907744825\n",
      "batch [10] loss: 22379499.907744825\n",
      "batch [20] loss: 22379499.907744825\n",
      "batch [30] loss: 22379499.907744825\n",
      "batch [40] loss: 22379499.907744825\n",
      "batch [50] loss: 22379499.907744825\n",
      "batch [60] loss: 22379499.907744825\n",
      "batch [70] loss: 22379499.907744825\n",
      "batch [80] loss: 22379499.907744825\n",
      "batch [90] loss: 22379499.907744825\n",
      "batch [100] loss: 22379499.907744825\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 22379499.907744825\n",
      "batch [10] loss: 22379499.907744825\n",
      "batch [20] loss: 22379499.907744825\n",
      "batch [30] loss: 22379499.907744825\n",
      "batch [40] loss: 22379499.907744825\n",
      "batch [50] loss: 22379499.907744825\n",
      "batch [60] loss: 22379499.907744825\n",
      "batch [70] loss: 22379499.907744825\n",
      "batch [80] loss: 22379499.907744825\n",
      "batch [90] loss: 22379499.907744825\n",
      "batch [100] loss: 22379499.907744825\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[0.1301, 0.8699],\n",
      "        [0.0462, 0.9538],\n",
      "        [0.0774, 0.9226],\n",
      "        [0.0470, 0.9530],\n",
      "        [0.0108, 0.9892],\n",
      "        [0.0087, 0.9913],\n",
      "        [0.1683, 0.8317],\n",
      "        [0.1567, 0.8433],\n",
      "        [0.4284, 0.5716],\n",
      "        [0.1493, 0.8507]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.497857730657813\n",
      "batch [10] loss: 21204899.91258689\n",
      "batch [20] loss: 21204899.91258689\n",
      "batch [30] loss: 21204899.91258689\n",
      "batch [40] loss: 21204899.91258689\n",
      "batch [50] loss: 21204899.91258689\n",
      "batch [60] loss: 21204899.91258689\n",
      "batch [70] loss: 21204899.91258689\n",
      "batch [80] loss: 21204899.91258689\n",
      "batch [90] loss: 21204899.91258689\n",
      "batch [100] loss: 21204899.91258689\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 21204899.91258689\n",
      "batch [10] loss: 21204899.91258689\n",
      "batch [20] loss: 21204899.91258689\n",
      "batch [30] loss: 21204899.91258689\n",
      "batch [40] loss: 21204899.91258689\n",
      "batch [50] loss: 21204899.91258689\n",
      "batch [60] loss: 21204899.91258689\n",
      "batch [70] loss: 21204899.91258689\n",
      "batch [80] loss: 21204899.91258689\n",
      "batch [90] loss: 21204899.91258689\n",
      "batch [100] loss: 21204899.91258689\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 21204899.91258689\n",
      "batch [10] loss: 21204899.91258689\n",
      "batch [20] loss: 21204899.91258689\n",
      "batch [30] loss: 21204899.91258689\n",
      "batch [40] loss: 21204899.91258689\n",
      "batch [50] loss: 21204899.91258689\n",
      "batch [60] loss: 21204899.91258689\n",
      "batch [70] loss: 21204899.91258689\n",
      "batch [80] loss: 21204899.91258689\n",
      "batch [90] loss: 21204899.91258689\n",
      "batch [100] loss: 21204899.91258689\n",
      "OG Labels:  tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[9.9974e-01, 2.6164e-04],\n",
      "        [1.2935e-02, 9.8706e-01],\n",
      "        [1.2757e-02, 9.8724e-01],\n",
      "        [9.9490e-01, 5.1030e-03],\n",
      "        [2.2284e-01, 7.7716e-01],\n",
      "        [1.4202e-01, 8.5798e-01],\n",
      "        [4.6666e-02, 9.5333e-01],\n",
      "        [3.6193e-02, 9.6381e-01],\n",
      "        [5.1528e-02, 9.4847e-01],\n",
      "        [9.3001e-01, 6.9989e-02]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.7051085506400194\n",
      "batch [10] loss: 27600900.69851549\n",
      "batch [20] loss: 27600900.69851549\n",
      "batch [30] loss: 27600900.69851549\n",
      "batch [40] loss: 27600900.69851549\n",
      "batch [50] loss: 27600900.69851549\n",
      "batch [60] loss: 27600900.69851549\n",
      "batch [70] loss: 27600900.69851549\n",
      "batch [80] loss: 27600900.69851549\n",
      "batch [90] loss: 27600900.69851549\n",
      "batch [100] loss: 27600900.69851549\n",
      "Using scale consts: [1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 27600908.009169273\n",
      "batch [10] loss: 27602120.459854897\n",
      "batch [20] loss: 27602536.581437707\n",
      "batch [30] loss: 27602266.50306253\n",
      "batch [40] loss: 27602475.475667283\n",
      "batch [50] loss: 27603031.769147847\n",
      "batch [60] loss: 27602471.30793888\n",
      "batch [70] loss: 27602034.86862593\n",
      "batch [80] loss: 27602359.863661658\n",
      "batch [90] loss: 27602056.121501297\n",
      "batch [100] loss: 27602207.12251944\n",
      "Using scale consts: [10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 27602887.47002978\n",
      "batch [10] loss: 27701824.029453933\n",
      "batch [20] loss: 27803086.812937714\n",
      "batch [30] loss: 27897348.48605424\n",
      "batch [40] loss: 27980138.35378459\n",
      "batch [50] loss: 28049207.12602129\n",
      "batch [60] loss: 28111065.398754932\n",
      "batch [70] loss: 28164634.11033699\n",
      "batch [80] loss: 28215202.79933667\n",
      "batch [90] loss: 28260427.121316247\n",
      "batch [100] loss: 28306326.74540318\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0283, 0.9717],\n",
      "        [0.0931, 0.9069],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.1863, 0.8137],\n",
      "        [0.0281, 0.9719],\n",
      "        [0.0129, 0.9871],\n",
      "        [0.0468, 0.9532],\n",
      "        [0.0949, 0.9051],\n",
      "        [0.1758, 0.8242],\n",
      "        [0.0298, 0.9702]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.7035859167344247\n",
      "batch [10] loss: 32627999.865497362\n",
      "batch [20] loss: 32627999.865497362\n",
      "batch [30] loss: 32627999.865497362\n",
      "batch [40] loss: 32627999.865497362\n",
      "batch [50] loss: 32627999.865497362\n",
      "batch [60] loss: 32627999.865497362\n",
      "batch [70] loss: 32627999.865497362\n",
      "batch [80] loss: 32627999.865497362\n",
      "batch [90] loss: 32627999.865497362\n",
      "batch [100] loss: 32627999.865497362\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 32627999.865497362\n",
      "batch [10] loss: 32627999.865497362\n",
      "batch [20] loss: 32627999.865497362\n",
      "batch [30] loss: 32627999.865497362\n",
      "batch [40] loss: 32627999.865497362\n",
      "batch [50] loss: 32627999.865497362\n",
      "batch [60] loss: 32627999.865497362\n",
      "batch [70] loss: 32627999.865497362\n",
      "batch [80] loss: 32627999.865497362\n",
      "batch [90] loss: 32627999.865497362\n",
      "batch [100] loss: 32627999.865497362\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 32627999.865497362\n",
      "batch [10] loss: 32627999.865497362\n",
      "batch [20] loss: 32627999.865497362\n",
      "batch [30] loss: 32627999.865497362\n",
      "batch [40] loss: 32627999.865497362\n",
      "batch [50] loss: 32627999.865497362\n",
      "batch [60] loss: 32627999.865497362\n",
      "batch [70] loss: 32627999.865497362\n",
      "batch [80] loss: 32627999.865497362\n",
      "batch [90] loss: 32627999.865497362\n",
      "batch [100] loss: 32627999.865497362\n",
      "OG Labels:  tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1])\n",
      "Model pred:  tensor([[9.9978e-01, 2.2294e-04],\n",
      "        [9.9839e-01, 1.6081e-03],\n",
      "        [1.9139e-01, 8.0861e-01],\n",
      "        [9.8340e-01, 1.6603e-02],\n",
      "        [9.9898e-01, 1.0188e-03],\n",
      "        [9.4835e-03, 9.9052e-01],\n",
      "        [9.4303e-02, 9.0570e-01],\n",
      "        [9.9181e-01, 8.1871e-03],\n",
      "        [9.3450e-02, 9.0655e-01],\n",
      "        [1.5439e-01, 8.4561e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.293069486337774\n",
      "batch [10] loss: 17461887.254446574\n",
      "batch [20] loss: 17455253.000329044\n",
      "batch [30] loss: 17454923.8639025\n",
      "batch [40] loss: 17454972.018588558\n",
      "batch [50] loss: 17454702.48139848\n",
      "batch [60] loss: 17454765.505728796\n",
      "batch [70] loss: 17454809.79700372\n",
      "batch [80] loss: 17454808.297878165\n",
      "batch [90] loss: 17454705.147238113\n",
      "batch [100] loss: 17454799.00156053\n",
      "Using scale consts: [1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 17454882.405650385\n",
      "batch [10] loss: 17456045.89915654\n",
      "batch [20] loss: 17455916.846654296\n",
      "batch [30] loss: 17456824.148149997\n",
      "batch [40] loss: 17456406.338642813\n",
      "batch [50] loss: 17456705.12172175\n",
      "batch [60] loss: 17456734.48414228\n",
      "batch [70] loss: 17456418.733857278\n",
      "batch [80] loss: 17456478.107555367\n",
      "batch [90] loss: 17456925.257167876\n",
      "batch [100] loss: 17457010.056837335\n",
      "Using scale consts: [10.0, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025]\n",
      "batch [0] loss: 17457012.37172156\n",
      "batch [10] loss: 17557770.500711404\n",
      "batch [20] loss: 17803080.354551975\n",
      "batch [30] loss: 18103122.919577353\n",
      "batch [40] loss: 18538129.585138425\n",
      "batch [50] loss: 19258142.895035516\n",
      "batch [60] loss: 20245855.79057911\n",
      "batch [70] loss: 21292206.986043267\n",
      "batch [80] loss: 22241947.21429712\n",
      "batch [90] loss: 23020634.25794288\n",
      "batch [100] loss: 23629201.425139047\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[2.2240e-01, 7.7760e-01],\n",
      "        [2.1858e-02, 9.7814e-01],\n",
      "        [1.8062e-02, 9.8194e-01],\n",
      "        [1.3637e-01, 8.6363e-01],\n",
      "        [9.9912e-01, 8.8470e-04],\n",
      "        [9.8911e-01, 1.0890e-02],\n",
      "        [2.2791e-01, 7.7209e-01],\n",
      "        [7.0656e-02, 9.2934e-01],\n",
      "        [2.0008e-01, 7.9992e-01],\n",
      "        [8.3190e-02, 9.1681e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.0015141327711565\n",
      "batch [10] loss: 21111819.097469106\n",
      "batch [20] loss: 21102909.067698877\n",
      "batch [30] loss: 21101837.680481814\n",
      "batch [40] loss: 21101830.726800263\n",
      "batch [50] loss: 21101780.17781189\n",
      "batch [60] loss: 21102152.19347302\n",
      "batch [70] loss: 21102049.69909709\n",
      "batch [80] loss: 21102017.89998658\n",
      "batch [90] loss: 21102051.72576375\n",
      "batch [100] loss: 21101902.44714253\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 21102102.40981099\n",
      "batch [10] loss: 21101900.32935106\n",
      "batch [20] loss: 21101900.329371706\n",
      "batch [30] loss: 21101900.329347484\n",
      "batch [40] loss: 21101900.32934738\n",
      "batch [50] loss: 21101900.32934744\n",
      "batch [60] loss: 21101900.329350054\n",
      "batch [70] loss: 21101900.329347596\n",
      "batch [80] loss: 21101900.32934738\n",
      "batch [90] loss: 21101900.32934859\n",
      "batch [100] loss: 21101900.329347447\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 21101900.537515376\n",
      "batch [10] loss: 21101900.53752972\n",
      "batch [20] loss: 21102000.543260306\n",
      "batch [30] loss: 21101900.537526444\n",
      "batch [40] loss: 21101900.537516624\n",
      "batch [50] loss: 21101908.97189388\n",
      "batch [60] loss: 21101900.537516028\n",
      "batch [70] loss: 21101900.537515856\n",
      "batch [80] loss: 21101900.53751567\n",
      "batch [90] loss: 21101900.539292358\n",
      "batch [100] loss: 21101900.5375638\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.2220, 0.7780],\n",
      "        [0.0178, 0.9822],\n",
      "        [0.0179, 0.9821],\n",
      "        [0.1733, 0.8267],\n",
      "        [0.0403, 0.9597],\n",
      "        [0.1861, 0.8139],\n",
      "        [0.0163, 0.9837],\n",
      "        [0.0041, 0.9959],\n",
      "        [0.1683, 0.8317],\n",
      "        [0.1733, 0.8267]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.823018182964227\n",
      "batch [10] loss: 26897299.88912107\n",
      "batch [20] loss: 26897299.88912107\n",
      "batch [30] loss: 26897299.88912107\n",
      "batch [40] loss: 26897299.88912107\n",
      "batch [50] loss: 26897299.88912107\n",
      "batch [60] loss: 26897299.88912107\n",
      "batch [70] loss: 26897299.88912107\n",
      "batch [80] loss: 26897299.88912107\n",
      "batch [90] loss: 26897299.88912107\n",
      "batch [100] loss: 26897299.88912107\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26897299.88912107\n",
      "batch [10] loss: 26897299.88912107\n",
      "batch [20] loss: 26897299.88912107\n",
      "batch [30] loss: 26897299.88912107\n",
      "batch [40] loss: 26897299.88912107\n",
      "batch [50] loss: 26897299.88912107\n",
      "batch [60] loss: 26897299.88912107\n",
      "batch [70] loss: 26897299.88912107\n",
      "batch [80] loss: 26897299.88912107\n",
      "batch [90] loss: 26897299.88912107\n",
      "batch [100] loss: 26897299.88912107\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 26897299.88912107\n",
      "batch [10] loss: 26897299.88912107\n",
      "batch [20] loss: 26897299.88912107\n",
      "batch [30] loss: 26897299.88912107\n",
      "batch [40] loss: 26897299.88912107\n",
      "batch [50] loss: 26897299.88912107\n",
      "batch [60] loss: 26897299.88912107\n",
      "batch [70] loss: 26897299.88912107\n",
      "batch [80] loss: 26897299.88912107\n",
      "batch [90] loss: 26897299.88912107\n",
      "batch [100] loss: 26897299.88912107\n",
      "OG Labels:  tensor([0, 1, 0, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[0.4685, 0.5315],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.9987, 0.0013],\n",
      "        [0.0283, 0.9717],\n",
      "        [0.0041, 0.9959],\n",
      "        [0.1288, 0.8712],\n",
      "        [0.9987, 0.0013],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.3061, 0.6939]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.3760919934750766\n",
      "batch [10] loss: 20840977.063445978\n",
      "batch [20] loss: 20840501.04448006\n",
      "batch [30] loss: 20840588.666166335\n",
      "batch [40] loss: 20840501.035965152\n",
      "batch [50] loss: 20840501.035914537\n",
      "batch [60] loss: 20840501.043740027\n",
      "batch [70] loss: 20840501.03591431\n",
      "batch [80] loss: 20840501.03591442\n",
      "batch [90] loss: 20840501.041524634\n",
      "batch [100] loss: 20840501.035918385\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 20840500.476058006\n",
      "batch [10] loss: 20840500.47500148\n",
      "batch [20] loss: 20840500.47500148\n",
      "batch [30] loss: 20840500.47500148\n",
      "batch [40] loss: 20840500.47500148\n",
      "batch [50] loss: 20840500.47500148\n",
      "batch [60] loss: 20840500.47500148\n",
      "batch [70] loss: 20840500.47500148\n",
      "batch [80] loss: 20840500.47500148\n",
      "batch [90] loss: 20840500.47500148\n",
      "batch [100] loss: 20840500.47500148\n",
      "Using scale consts: [0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 20840500.75545771\n",
      "batch [10] loss: 20840500.75545771\n",
      "batch [20] loss: 20840500.755457714\n",
      "batch [30] loss: 20840500.75545771\n",
      "batch [40] loss: 20840500.75545773\n",
      "batch [50] loss: 20840500.75545771\n",
      "batch [60] loss: 20840500.75545771\n",
      "batch [70] loss: 20840500.75545771\n",
      "batch [80] loss: 20840500.755457714\n",
      "batch [90] loss: 20840500.75545771\n",
      "batch [100] loss: 20840500.75545771\n",
      "OG Labels:  tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[1.2935e-02, 9.8706e-01],\n",
      "        [9.9205e-02, 9.0080e-01],\n",
      "        [9.9983e-01, 1.6883e-04],\n",
      "        [1.0294e-01, 8.9706e-01],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [7.2226e-02, 9.2777e-01],\n",
      "        [1.4930e-01, 8.5070e-01],\n",
      "        [4.4317e-02, 9.5568e-01],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [9.9997e-01, 2.8415e-05]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.902560304973477\n",
      "batch [10] loss: 16232401.829518545\n",
      "batch [20] loss: 16232301.829519479\n",
      "batch [30] loss: 16232301.829519479\n",
      "batch [40] loss: 16232301.829519479\n",
      "batch [50] loss: 16232301.829519479\n",
      "batch [60] loss: 16232301.829519479\n",
      "batch [70] loss: 16232301.829519479\n",
      "batch [80] loss: 16232301.829519479\n",
      "batch [90] loss: 16232301.829519479\n",
      "batch [100] loss: 16232301.829519479\n",
      "Using scale consts: [0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0]\n",
      "batch [0] loss: 16232318.897425191\n",
      "batch [10] loss: 16237382.257888226\n",
      "batch [20] loss: 16236590.080813935\n",
      "batch [30] loss: 16235895.4893002\n",
      "batch [40] loss: 16235998.560595922\n",
      "batch [50] loss: 16235140.895825444\n",
      "batch [60] loss: 16234875.66042492\n",
      "batch [70] loss: 16235596.719365861\n",
      "batch [80] loss: 16235333.131510595\n",
      "batch [90] loss: 16234839.4516777\n",
      "batch [100] loss: 16235039.101668758\n",
      "Using scale consts: [0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0]\n",
      "batch [0] loss: 16235378.849262236\n",
      "batch [10] loss: 16373788.88284853\n",
      "batch [20] loss: 16519272.03683753\n",
      "batch [30] loss: 16734946.981138635\n",
      "batch [40] loss: 17099367.773057867\n",
      "batch [50] loss: 17492169.944005094\n",
      "batch [60] loss: 17924808.998348564\n",
      "batch [70] loss: 18488275.490244433\n",
      "batch [80] loss: 19178323.433055736\n",
      "batch [90] loss: 19888950.41512788\n",
      "batch [100] loss: 20608639.689350702\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[0.1943, 0.8057],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.1682, 0.8318],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0317, 0.9683],\n",
      "        [0.2235, 0.7765],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.1943, 0.8057],\n",
      "        [0.9987, 0.0013]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.5616956241757753\n",
      "batch [10] loss: 22344699.907888286\n",
      "batch [20] loss: 22344699.907888286\n",
      "batch [30] loss: 22344699.907888286\n",
      "batch [40] loss: 22344699.907888286\n",
      "batch [50] loss: 22344699.907888286\n",
      "batch [60] loss: 22344699.907888286\n",
      "batch [70] loss: 22344699.907888286\n",
      "batch [80] loss: 22344699.907888286\n",
      "batch [90] loss: 22344699.907888286\n",
      "batch [100] loss: 22344699.907888286\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 22344699.907888286\n",
      "batch [10] loss: 22344699.907888286\n",
      "batch [20] loss: 22344699.907888286\n",
      "batch [30] loss: 22344699.907888286\n",
      "batch [40] loss: 22344699.907888286\n",
      "batch [50] loss: 22344699.907888286\n",
      "batch [60] loss: 22344699.907888286\n",
      "batch [70] loss: 22344699.907888286\n",
      "batch [80] loss: 22344699.907888286\n",
      "batch [90] loss: 22344699.907888286\n",
      "batch [100] loss: 22344699.907888286\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 22344699.907888286\n",
      "batch [10] loss: 22344699.907888286\n",
      "batch [20] loss: 22344699.907888286\n",
      "batch [30] loss: 22344699.907888286\n",
      "batch [40] loss: 22344699.907888286\n",
      "batch [50] loss: 22344699.907888286\n",
      "batch [60] loss: 22344699.907888286\n",
      "batch [70] loss: 22344699.907888286\n",
      "batch [80] loss: 22344699.907888286\n",
      "batch [90] loss: 22344699.907888286\n",
      "batch [100] loss: 22344699.907888286\n",
      "OG Labels:  tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[0.0170, 0.9830],\n",
      "        [0.1493, 0.8507],\n",
      "        [0.9954, 0.0046],\n",
      "        [0.4488, 0.5512],\n",
      "        [0.0228, 0.9772],\n",
      "        [0.1914, 0.8086],\n",
      "        [0.3541, 0.6459],\n",
      "        [0.0234, 0.9766],\n",
      "        [0.9910, 0.0090],\n",
      "        [0.0163, 0.9837]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 0, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.9704678799406525\n",
      "batch [10] loss: 30145765.181635596\n",
      "batch [20] loss: 30144761.970830068\n",
      "batch [30] loss: 30144900.619977836\n",
      "batch [40] loss: 30144900.399783164\n",
      "batch [50] loss: 30144900.27580597\n",
      "batch [60] loss: 30144981.365522858\n",
      "batch [70] loss: 30144906.634900745\n",
      "batch [80] loss: 30144900.275897976\n",
      "batch [90] loss: 30144900.289546527\n",
      "batch [100] loss: 30144900.718368955\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 30144923.0657009\n",
      "batch [10] loss: 30144900.075736444\n",
      "batch [20] loss: 30144900.075736444\n",
      "batch [30] loss: 30144900.075736444\n",
      "batch [40] loss: 30144900.075736444\n",
      "batch [50] loss: 30144900.075736444\n",
      "batch [60] loss: 30145000.075659066\n",
      "batch [70] loss: 30144900.075736456\n",
      "batch [80] loss: 30144900.075736444\n",
      "batch [90] loss: 30144900.075736444\n",
      "batch [100] loss: 30144900.075736444\n",
      "Using scale consts: [0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 30144900.175737944\n",
      "batch [10] loss: 30144900.175737944\n",
      "batch [20] loss: 30144900.175739154\n",
      "batch [30] loss: 30144900.17573795\n",
      "batch [40] loss: 30144900.175844915\n",
      "batch [50] loss: 30144900.17573795\n",
      "batch [60] loss: 30144900.1775853\n",
      "batch [70] loss: 30144900.17573824\n",
      "batch [80] loss: 30144900.175737955\n",
      "batch [90] loss: 30144900.175737944\n",
      "batch [100] loss: 30144900.175902445\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0216, 0.9784],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.0128, 0.9872],\n",
      "        [0.0163, 0.9837],\n",
      "        [0.1493, 0.8507],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0399, 0.9601],\n",
      "        [0.3541, 0.6459],\n",
      "        [0.0172, 0.9828]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.011337895767231\n",
      "batch [10] loss: 30070899.876038514\n",
      "batch [20] loss: 30070899.876038514\n",
      "batch [30] loss: 30070899.876038514\n",
      "batch [40] loss: 30070899.876038514\n",
      "batch [50] loss: 30070899.876038514\n",
      "batch [60] loss: 30070899.876038514\n",
      "batch [70] loss: 30070899.876038514\n",
      "batch [80] loss: 30070899.876038514\n",
      "batch [90] loss: 30070899.876038514\n",
      "batch [100] loss: 30070899.876038514\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 30070899.876038514\n",
      "batch [10] loss: 30070899.876038514\n",
      "batch [20] loss: 30070899.876038514\n",
      "batch [30] loss: 30070899.876038514\n",
      "batch [40] loss: 30070899.876038514\n",
      "batch [50] loss: 30070899.876038514\n",
      "batch [60] loss: 30070899.876038514\n",
      "batch [70] loss: 30070899.876038514\n",
      "batch [80] loss: 30070899.876038514\n",
      "batch [90] loss: 30070899.876038514\n",
      "batch [100] loss: 30070899.876038514\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 30070899.876038514\n",
      "batch [10] loss: 30070899.876038514\n",
      "batch [20] loss: 30070899.876038514\n",
      "batch [30] loss: 30070899.876038514\n",
      "batch [40] loss: 30070899.876038514\n",
      "batch [50] loss: 30070899.876038514\n",
      "batch [60] loss: 30070899.876038514\n",
      "batch [70] loss: 30070899.876038514\n",
      "batch [80] loss: 30070899.876038514\n",
      "batch [90] loss: 30070899.876038514\n",
      "batch [100] loss: 30070899.876038514\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1083, 0.8917],\n",
      "        [0.2220, 0.7780],\n",
      "        [0.0063, 0.9937],\n",
      "        [0.0935, 0.9065],\n",
      "        [0.0534, 0.9466],\n",
      "        [0.1758, 0.8242],\n",
      "        [0.0070, 0.9930],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.0298, 0.9702],\n",
      "        [0.3433, 0.6567]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.578266907005566\n",
      "batch [10] loss: 29194099.879652955\n",
      "batch [20] loss: 29194099.879652955\n",
      "batch [30] loss: 29194099.879652955\n",
      "batch [40] loss: 29194099.879652955\n",
      "batch [50] loss: 29194099.879652955\n",
      "batch [60] loss: 29194099.879652955\n",
      "batch [70] loss: 29194099.879652955\n",
      "batch [80] loss: 29194099.879652955\n",
      "batch [90] loss: 29194099.879652955\n",
      "batch [100] loss: 29194099.879652955\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 29194099.879652955\n",
      "batch [10] loss: 29194099.879652955\n",
      "batch [20] loss: 29194099.879652955\n",
      "batch [30] loss: 29194099.879652955\n",
      "batch [40] loss: 29194099.879652955\n",
      "batch [50] loss: 29194099.879652955\n",
      "batch [60] loss: 29194099.879652955\n",
      "batch [70] loss: 29194099.879652955\n",
      "batch [80] loss: 29194099.879652955\n",
      "batch [90] loss: 29194099.879652955\n",
      "batch [100] loss: 29194099.879652955\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 29194099.879652955\n",
      "batch [10] loss: 29194099.879652955\n",
      "batch [20] loss: 29194099.879652955\n",
      "batch [30] loss: 29194099.879652955\n",
      "batch [40] loss: 29194099.879652955\n",
      "batch [50] loss: 29194099.879652955\n",
      "batch [60] loss: 29194099.879652955\n",
      "batch [70] loss: 29194099.879652955\n",
      "batch [80] loss: 29194099.879652955\n",
      "batch [90] loss: 29194099.879652955\n",
      "batch [100] loss: 29194099.879652955\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0235, 0.9765],\n",
      "        [0.2220, 0.7780],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.0062, 0.9938],\n",
      "        [0.1779, 0.8221],\n",
      "        [0.2279, 0.7721],\n",
      "        [0.0963, 0.9037],\n",
      "        [0.0258, 0.9742],\n",
      "        [0.2006, 0.7994],\n",
      "        [0.1301, 0.8699]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.353967113835029\n",
      "batch [10] loss: 19190099.920892514\n",
      "batch [20] loss: 19190099.920892514\n",
      "batch [30] loss: 19190099.920892514\n",
      "batch [40] loss: 19190099.920892514\n",
      "batch [50] loss: 19190099.920892514\n",
      "batch [60] loss: 19190099.920892514\n",
      "batch [70] loss: 19190099.920892514\n",
      "batch [80] loss: 19190099.920892514\n",
      "batch [90] loss: 19190099.920892514\n",
      "batch [100] loss: 19190099.920892514\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 19190099.920892514\n",
      "batch [10] loss: 19190099.920892514\n",
      "batch [20] loss: 19190099.920892514\n",
      "batch [30] loss: 19190099.920892514\n",
      "batch [40] loss: 19190099.920892514\n",
      "batch [50] loss: 19190099.920892514\n",
      "batch [60] loss: 19190099.920892514\n",
      "batch [70] loss: 19190099.920892514\n",
      "batch [80] loss: 19190099.920892514\n",
      "batch [90] loss: 19190099.920892514\n",
      "batch [100] loss: 19190099.920892514\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 19190099.920892514\n",
      "batch [10] loss: 19190099.920892514\n",
      "batch [20] loss: 19190099.920892514\n",
      "batch [30] loss: 19190099.920892514\n",
      "batch [40] loss: 19190099.920892514\n",
      "batch [50] loss: 19190099.920892514\n",
      "batch [60] loss: 19190099.920892514\n",
      "batch [70] loss: 19190099.920892514\n",
      "batch [80] loss: 19190099.920892514\n",
      "batch [90] loss: 19190099.920892514\n",
      "batch [100] loss: 19190099.920892514\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[4.0965e-03, 9.9590e-01],\n",
      "        [8.2053e-03, 9.9179e-01],\n",
      "        [1.1795e-02, 9.8820e-01],\n",
      "        [9.8898e-02, 9.0110e-01],\n",
      "        [9.9982e-01, 1.7802e-04],\n",
      "        [1.7768e-01, 8.2232e-01],\n",
      "        [2.2284e-01, 7.7716e-01],\n",
      "        [2.0840e-02, 9.7916e-01],\n",
      "        [1.3582e-01, 8.6418e-01],\n",
      "        [1.3007e-01, 8.6993e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.5941301531129213\n",
      "batch [10] loss: 24482700.754055165\n",
      "batch [20] loss: 24482700.754055165\n",
      "batch [30] loss: 24482700.754055165\n",
      "batch [40] loss: 24482700.754055165\n",
      "batch [50] loss: 24482700.754055165\n",
      "batch [60] loss: 24482700.754055165\n",
      "batch [70] loss: 24482700.754055165\n",
      "batch [80] loss: 24482700.754055165\n",
      "batch [90] loss: 24482700.754055165\n",
      "batch [100] loss: 24482700.754055165\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 24482708.448878415\n",
      "batch [10] loss: 24485369.73936883\n",
      "batch [20] loss: 24486193.250026472\n",
      "batch [30] loss: 24484241.70231618\n",
      "batch [40] loss: 24484547.287708733\n",
      "batch [50] loss: 24484723.496309195\n",
      "batch [60] loss: 24484553.955618285\n",
      "batch [70] loss: 24485216.024594236\n",
      "batch [80] loss: 24484887.752327602\n",
      "batch [90] loss: 24484653.103535317\n",
      "batch [100] loss: 24485002.301003218\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 24484922.371667393\n",
      "batch [10] loss: 24581737.90694979\n",
      "batch [20] loss: 24688431.799232263\n",
      "batch [30] loss: 24781413.008050792\n",
      "batch [40] loss: 24856545.96436072\n",
      "batch [50] loss: 24968839.786095023\n",
      "batch [60] loss: 25127273.866360612\n",
      "batch [70] loss: 25302041.27019038\n",
      "batch [80] loss: 25510200.3402073\n",
      "batch [90] loss: 25794454.714709222\n",
      "batch [100] loss: 26192975.93555887\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0302, 0.9698],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.0931, 0.9069],\n",
      "        [0.1342, 0.8658],\n",
      "        [0.0530, 0.9470],\n",
      "        [0.0548, 0.9452],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.1342, 0.8658],\n",
      "        [0.0235, 0.9765],\n",
      "        [0.1323, 0.8677]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.719628962953304\n",
      "batch [10] loss: 31940699.86833062\n",
      "batch [20] loss: 31940699.86833062\n",
      "batch [30] loss: 31940699.86833062\n",
      "batch [40] loss: 31940699.86833062\n",
      "batch [50] loss: 31940699.86833062\n",
      "batch [60] loss: 31940699.86833062\n",
      "batch [70] loss: 31940699.86833062\n",
      "batch [80] loss: 31940699.86833062\n",
      "batch [90] loss: 31940699.86833062\n",
      "batch [100] loss: 31940699.86833062\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 31940699.86833062\n",
      "batch [10] loss: 31940699.86833062\n",
      "batch [20] loss: 31940699.86833062\n",
      "batch [30] loss: 31940699.86833062\n",
      "batch [40] loss: 31940699.86833062\n",
      "batch [50] loss: 31940699.86833062\n",
      "batch [60] loss: 31940699.86833062\n",
      "batch [70] loss: 31940699.86833062\n",
      "batch [80] loss: 31940699.86833062\n",
      "batch [90] loss: 31940699.86833062\n",
      "batch [100] loss: 31940699.86833062\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 31940699.86833062\n",
      "batch [10] loss: 31940699.86833062\n",
      "batch [20] loss: 31940699.86833062\n",
      "batch [30] loss: 31940699.86833062\n",
      "batch [40] loss: 31940699.86833062\n",
      "batch [50] loss: 31940699.86833062\n",
      "batch [60] loss: 31940699.86833062\n",
      "batch [70] loss: 31940699.86833062\n",
      "batch [80] loss: 31940699.86833062\n",
      "batch [90] loss: 31940699.86833062\n",
      "batch [100] loss: 31940699.86833062\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1])\n",
      "Model pred:  tensor([[0.0082, 0.9918],\n",
      "        [0.0080, 0.9920],\n",
      "        [0.2235, 0.7765],\n",
      "        [0.9942, 0.0058],\n",
      "        [0.0129, 0.9871],\n",
      "        [0.2007, 0.7993],\n",
      "        [0.1914, 0.8086],\n",
      "        [0.9904, 0.0096],\n",
      "        [0.1318, 0.8682],\n",
      "        [0.0939, 0.9061]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 0, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.194866301243939\n",
      "batch [10] loss: 28387213.0323098\n",
      "batch [20] loss: 28380522.12337239\n",
      "batch [30] loss: 28379599.2990568\n",
      "batch [40] loss: 28379855.29867068\n",
      "batch [50] loss: 28379529.530961372\n",
      "batch [60] loss: 28379574.377229333\n",
      "batch [70] loss: 28379762.671785362\n",
      "batch [80] loss: 28379654.607926354\n",
      "batch [90] loss: 28379499.063133948\n",
      "batch [100] loss: 28379489.79448274\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28379462.52487991\n",
      "batch [10] loss: 28379399.91798522\n",
      "batch [20] loss: 28379399.917986073\n",
      "batch [30] loss: 28379399.917985216\n",
      "batch [40] loss: 28379399.917989235\n",
      "batch [50] loss: 28379399.917985357\n",
      "batch [60] loss: 28379399.917985205\n",
      "batch [70] loss: 28379399.917985428\n",
      "batch [80] loss: 28379399.917985216\n",
      "batch [90] loss: 28379399.917996064\n",
      "batch [100] loss: 28379399.917985227\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 28379399.935472116\n",
      "batch [10] loss: 28379399.935635865\n",
      "batch [20] loss: 28379399.935474094\n",
      "batch [30] loss: 28379597.39551362\n",
      "batch [40] loss: 28379499.93546302\n",
      "batch [50] loss: 28379499.935439393\n",
      "batch [60] loss: 28379598.63931215\n",
      "batch [70] loss: 28379499.935437135\n",
      "batch [80] loss: 28379499.935437135\n",
      "batch [90] loss: 28379499.936267294\n",
      "batch [100] loss: 28379499.93574308\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1758, 0.8242],\n",
      "        [0.0129, 0.9871],\n",
      "        [0.0164, 0.9836],\n",
      "        [0.9894, 0.0106],\n",
      "        [0.1469, 0.8531],\n",
      "        [0.0526, 0.9474],\n",
      "        [0.0283, 0.9717],\n",
      "        [0.0283, 0.9717],\n",
      "        [0.0280, 0.9720],\n",
      "        [0.0332, 0.9668]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.3160549743416157\n",
      "batch [10] loss: 34973799.85582725\n",
      "batch [20] loss: 34973799.85582725\n",
      "batch [30] loss: 34973799.85582725\n",
      "batch [40] loss: 34973799.85582725\n",
      "batch [50] loss: 34973799.85582725\n",
      "batch [60] loss: 34973799.85582725\n",
      "batch [70] loss: 34973799.85582725\n",
      "batch [80] loss: 34973799.85582725\n",
      "batch [90] loss: 34973799.85582725\n",
      "batch [100] loss: 34973799.85582725\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 34973799.85582725\n",
      "batch [10] loss: 34973799.85582725\n",
      "batch [20] loss: 34973799.85582725\n",
      "batch [30] loss: 34973799.85582725\n",
      "batch [40] loss: 34973799.85582725\n",
      "batch [50] loss: 34973799.85582725\n",
      "batch [60] loss: 34973799.85582725\n",
      "batch [70] loss: 34973799.85582725\n",
      "batch [80] loss: 34973799.85582725\n",
      "batch [90] loss: 34973799.85582725\n",
      "batch [100] loss: 34973799.85582725\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 34973799.85582725\n",
      "batch [10] loss: 34973799.85582725\n",
      "batch [20] loss: 34973799.85582725\n",
      "batch [30] loss: 34973799.85582725\n",
      "batch [40] loss: 34973799.85582725\n",
      "batch [50] loss: 34973799.85582725\n",
      "batch [60] loss: 34973799.85582725\n",
      "batch [70] loss: 34973799.85582725\n",
      "batch [80] loss: 34973799.85582725\n",
      "batch [90] loss: 34973799.85582725\n",
      "batch [100] loss: 34973799.85582725\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[1.6988e-02, 9.8301e-01],\n",
      "        [1.3622e-02, 9.8638e-01],\n",
      "        [2.2240e-01, 7.7760e-01],\n",
      "        [3.0509e-02, 9.6949e-01],\n",
      "        [1.7331e-01, 8.2669e-01],\n",
      "        [1.4922e-01, 8.5078e-01],\n",
      "        [6.3139e-02, 9.3686e-01],\n",
      "        [2.2240e-01, 7.7760e-01],\n",
      "        [1.7183e-01, 8.2817e-01],\n",
      "        [9.9991e-01, 9.2559e-05]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.116317577715953\n",
      "batch [10] loss: 24017900.82202324\n",
      "batch [20] loss: 24017900.82202324\n",
      "batch [30] loss: 24017900.82202324\n",
      "batch [40] loss: 24017900.82202324\n",
      "batch [50] loss: 24017900.82202324\n",
      "batch [60] loss: 24017900.82202324\n",
      "batch [70] loss: 24017900.82202324\n",
      "batch [80] loss: 24017900.82202324\n",
      "batch [90] loss: 24017900.82202324\n",
      "batch [100] loss: 24017900.82202324\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0]\n",
      "batch [0] loss: 24017909.111314673\n",
      "batch [10] loss: 24020741.40508918\n",
      "batch [20] loss: 24019829.376562543\n",
      "batch [30] loss: 24019178.71642276\n",
      "batch [40] loss: 24019285.475479655\n",
      "batch [50] loss: 24019792.61553426\n",
      "batch [60] loss: 24019136.17028293\n",
      "batch [70] loss: 24019057.911575206\n",
      "batch [80] loss: 24018745.58634261\n",
      "batch [90] loss: 24018573.630864672\n",
      "batch [100] loss: 24019158.183933534\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0]\n",
      "batch [0] loss: 24019098.641764276\n",
      "batch [10] loss: 24084406.11556635\n",
      "batch [20] loss: 24161426.151587963\n",
      "batch [30] loss: 24227788.529700913\n",
      "batch [40] loss: 24274996.090774953\n",
      "batch [50] loss: 24332201.63356498\n",
      "batch [60] loss: 24398611.77834013\n",
      "batch [70] loss: 24451196.77605944\n",
      "batch [80] loss: 24501906.446371235\n",
      "batch [90] loss: 24548539.536507666\n",
      "batch [100] loss: 24589749.866557784\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[0.1728, 0.8272],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.0077, 0.9923],\n",
      "        [0.9901, 0.0099],\n",
      "        [0.0321, 0.9679],\n",
      "        [0.1469, 0.8531],\n",
      "        [0.1863, 0.8137],\n",
      "        [0.0297, 0.9703],\n",
      "        [0.9948, 0.0052],\n",
      "        [0.1880, 0.8120]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.2656187482967267\n",
      "batch [10] loss: 28135957.243582316\n",
      "batch [20] loss: 28133749.117424466\n",
      "batch [30] loss: 28133623.182599828\n",
      "batch [40] loss: 28133632.142083876\n",
      "batch [50] loss: 28133698.886783235\n",
      "batch [60] loss: 28133622.54072798\n",
      "batch [70] loss: 28133600.303275082\n",
      "batch [80] loss: 28133600.823488954\n",
      "batch [90] loss: 28133600.423282363\n",
      "batch [100] loss: 28133672.74295341\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28133620.98905992\n",
      "batch [10] loss: 28133600.093571138\n",
      "batch [20] loss: 28133700.09349317\n",
      "batch [30] loss: 28133700.09349317\n",
      "batch [40] loss: 28133700.093493223\n",
      "batch [50] loss: 28133700.093493175\n",
      "batch [60] loss: 28133700.093493167\n",
      "batch [70] loss: 28133700.093493167\n",
      "batch [80] loss: 28133700.09349317\n",
      "batch [90] loss: 28133700.09349317\n",
      "batch [100] loss: 28133800.09341036\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025]\n",
      "batch [0] loss: 28133800.198103633\n",
      "batch [10] loss: 28133800.198103633\n",
      "batch [20] loss: 28133800.19810368\n",
      "batch [30] loss: 28133800.198103763\n",
      "batch [40] loss: 28133800.19811041\n",
      "batch [50] loss: 28133800.198103633\n",
      "batch [60] loss: 28133800.198103707\n",
      "batch [70] loss: 28133800.198248833\n",
      "batch [80] loss: 28133800.198103633\n",
      "batch [90] loss: 28133800.198103845\n",
      "batch [100] loss: 28133800.19810365\n",
      "OG Labels:  tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[0.1082, 0.8918],\n",
      "        [0.8726, 0.1274],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0219, 0.9781],\n",
      "        [0.9975, 0.0025],\n",
      "        [0.0298, 0.9702],\n",
      "        [0.0069, 0.9931],\n",
      "        [0.0773, 0.9227],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.4284, 0.5716]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.88478655827391\n",
      "batch [10] loss: 28390199.882966876\n",
      "batch [20] loss: 28390199.882966876\n",
      "batch [30] loss: 28390199.882966876\n",
      "batch [40] loss: 28390199.882966876\n",
      "batch [50] loss: 28390199.882966876\n",
      "batch [60] loss: 28390199.882966876\n",
      "batch [70] loss: 28390199.882966876\n",
      "batch [80] loss: 28390199.882966876\n",
      "batch [90] loss: 28390199.882966876\n",
      "batch [100] loss: 28390199.882966876\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28390199.882966876\n",
      "batch [10] loss: 28390199.882966876\n",
      "batch [20] loss: 28390199.882966876\n",
      "batch [30] loss: 28390199.882966876\n",
      "batch [40] loss: 28390199.882966876\n",
      "batch [50] loss: 28390199.882966876\n",
      "batch [60] loss: 28390199.882966876\n",
      "batch [70] loss: 28390199.882966876\n",
      "batch [80] loss: 28390199.882966876\n",
      "batch [90] loss: 28390199.882966876\n",
      "batch [100] loss: 28390199.882966876\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 28390199.882966876\n",
      "batch [10] loss: 28390199.882966876\n",
      "batch [20] loss: 28390199.882966876\n",
      "batch [30] loss: 28390199.882966876\n",
      "batch [40] loss: 28390199.882966876\n",
      "batch [50] loss: 28390199.882966876\n",
      "batch [60] loss: 28390199.882966876\n",
      "batch [70] loss: 28390199.882966876\n",
      "batch [80] loss: 28390199.882966876\n",
      "batch [90] loss: 28390199.882966876\n",
      "batch [100] loss: 28390199.882966876\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[4.2204e-02, 9.5780e-01],\n",
      "        [2.0551e-02, 9.7945e-01],\n",
      "        [9.0027e-02, 9.0997e-01],\n",
      "        [1.3733e-01, 8.6267e-01],\n",
      "        [8.3216e-03, 9.9168e-01],\n",
      "        [2.9664e-02, 9.7034e-01],\n",
      "        [9.9943e-01, 5.6631e-04],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [9.4671e-02, 9.0533e-01],\n",
      "        [1.6299e-02, 9.8370e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.5139660712358087\n",
      "batch [10] loss: 30363005.242350984\n",
      "batch [20] loss: 30362600.600364164\n",
      "batch [30] loss: 30362600.600364164\n",
      "batch [40] loss: 30362600.600364164\n",
      "batch [50] loss: 30362600.600364164\n",
      "batch [60] loss: 30362600.600364164\n",
      "batch [70] loss: 30362600.600364164\n",
      "batch [80] loss: 30362600.600364164\n",
      "batch [90] loss: 30362600.600364164\n",
      "batch [100] loss: 30362600.600364164\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 30362607.130117208\n",
      "batch [10] loss: 30371112.23482719\n",
      "batch [20] loss: 30375474.967301503\n",
      "batch [30] loss: 30379574.80198554\n",
      "batch [40] loss: 30383662.704238158\n",
      "batch [50] loss: 30386236.108958527\n",
      "batch [60] loss: 30389906.45902703\n",
      "batch [70] loss: 30393726.754589062\n",
      "batch [80] loss: 30395277.62306889\n",
      "batch [90] loss: 30397671.235516213\n",
      "batch [100] loss: 30399147.066667594\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 30400196.42804315\n",
      "batch [10] loss: 30595964.291150577\n",
      "batch [20] loss: 30868081.62732204\n",
      "batch [30] loss: 31263825.139772717\n",
      "batch [40] loss: 31814542.09945606\n",
      "batch [50] loss: 32416304.584761582\n",
      "batch [60] loss: 33050496.110931486\n",
      "batch [70] loss: 33659578.24755941\n",
      "batch [80] loss: 34207660.43685152\n",
      "batch [90] loss: 34682512.897165745\n",
      "batch [100] loss: 35092341.886807404\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0310, 0.9690],\n",
      "        [0.1493, 0.8507],\n",
      "        [0.1943, 0.8057],\n",
      "        [0.0163, 0.9837],\n",
      "        [0.1469, 0.8531],\n",
      "        [0.9975, 0.0025],\n",
      "        [0.0497, 0.9503],\n",
      "        [0.0721, 0.9279],\n",
      "        [0.2279, 0.7721],\n",
      "        [0.0242, 0.9758]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.886200185191623\n",
      "batch [10] loss: 24926316.051396124\n",
      "batch [20] loss: 24926200.41377656\n",
      "batch [30] loss: 24926200.413695864\n",
      "batch [40] loss: 24926200.413826734\n",
      "batch [50] loss: 24926200.41374679\n",
      "batch [60] loss: 24926200.413694967\n",
      "batch [70] loss: 24926200.414691634\n",
      "batch [80] loss: 24926200.413694926\n",
      "batch [90] loss: 24926200.413694926\n",
      "batch [100] loss: 24926200.41369497\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 24926200.155470777\n",
      "batch [10] loss: 24926200.155470736\n",
      "batch [20] loss: 24926200.155470736\n",
      "batch [30] loss: 24926200.155470736\n",
      "batch [40] loss: 24926200.155470736\n",
      "batch [50] loss: 24926200.155470736\n",
      "batch [60] loss: 24926200.155470736\n",
      "batch [70] loss: 24926200.155470736\n",
      "batch [80] loss: 24926200.155470736\n",
      "batch [90] loss: 24926200.155470736\n",
      "batch [100] loss: 24926200.155470736\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.07500000000000001, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 24926200.28458284\n",
      "batch [10] loss: 24926200.28458284\n",
      "batch [20] loss: 24926200.28458284\n",
      "batch [30] loss: 24926200.28458284\n",
      "batch [40] loss: 24926200.28458284\n",
      "batch [50] loss: 24926200.28458284\n",
      "batch [60] loss: 24926200.28458284\n",
      "batch [70] loss: 24926200.28458284\n",
      "batch [80] loss: 24926200.284582846\n",
      "batch [90] loss: 24926200.28458284\n",
      "batch [100] loss: 24926200.28458284\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1])\n",
      "Model pred:  tensor([[9.9427e-01, 5.7290e-03],\n",
      "        [3.2889e-02, 9.6711e-01],\n",
      "        [1.9139e-01, 8.0861e-01],\n",
      "        [3.0509e-02, 9.6949e-01],\n",
      "        [2.4009e-02, 9.7599e-01],\n",
      "        [1.1001e-01, 8.8999e-01],\n",
      "        [1.1446e-02, 9.8855e-01],\n",
      "        [1.3214e-01, 8.6786e-01],\n",
      "        [9.9943e-01, 5.6631e-04],\n",
      "        [1.1779e-01, 8.8221e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.5063103191438345\n",
      "batch [10] loss: 24583150.956838474\n",
      "batch [20] loss: 24578961.02062895\n",
      "batch [30] loss: 24579024.22109033\n",
      "batch [40] loss: 24579075.405334286\n",
      "batch [50] loss: 24578903.00943939\n",
      "batch [60] loss: 24578867.40422868\n",
      "batch [70] loss: 24578800.995795123\n",
      "batch [80] loss: 24578801.707311116\n",
      "batch [90] loss: 24578801.00149708\n",
      "batch [100] loss: 24578802.023267187\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05]\n",
      "batch [0] loss: 24578812.849513985\n",
      "batch [10] loss: 24587344.46604561\n",
      "batch [20] loss: 24593031.737533104\n",
      "batch [30] loss: 24597437.746160544\n",
      "batch [40] loss: 24602579.45222581\n",
      "batch [50] loss: 24605595.33533962\n",
      "batch [60] loss: 24609256.96149642\n",
      "batch [70] loss: 24612563.024874456\n",
      "batch [80] loss: 24615246.889195092\n",
      "batch [90] loss: 24616840.769064106\n",
      "batch [100] loss: 24619766.943159916\n",
      "Using scale consts: [0.07500000000000001, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025]\n",
      "batch [0] loss: 24620209.900932185\n",
      "batch [10] loss: 24771743.932529032\n",
      "batch [20] loss: 25021039.81616597\n",
      "batch [30] loss: 25434031.532039516\n",
      "batch [40] loss: 26012043.496057324\n",
      "batch [50] loss: 26703473.269521322\n",
      "batch [60] loss: 27403500.83667215\n",
      "batch [70] loss: 28043829.946474135\n",
      "batch [80] loss: 28585343.79843384\n",
      "batch [90] loss: 29050165.805768102\n",
      "batch [100] loss: 29445109.013598435\n",
      "OG Labels:  tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[1.6825e-01, 8.3175e-01],\n",
      "        [1.4930e-01, 8.5070e-01],\n",
      "        [1.6299e-02, 9.8370e-01],\n",
      "        [9.9913e-01, 8.7157e-04],\n",
      "        [3.3071e-02, 9.6693e-01],\n",
      "        [2.9794e-02, 9.7021e-01],\n",
      "        [2.1859e-01, 7.8141e-01],\n",
      "        [1.6988e-02, 9.8301e-01],\n",
      "        [1.7331e-01, 8.2669e-01],\n",
      "        [2.2346e-01, 7.7654e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.948127924503022\n",
      "batch [10] loss: 25879600.59019059\n",
      "batch [20] loss: 25879600.59019059\n",
      "batch [30] loss: 25879600.59019059\n",
      "batch [40] loss: 25879600.59019059\n",
      "batch [50] loss: 25879600.59019059\n",
      "batch [60] loss: 25879600.59019059\n",
      "batch [70] loss: 25879600.59019059\n",
      "batch [80] loss: 25879600.59019059\n",
      "batch [90] loss: 25879600.59019059\n",
      "batch [100] loss: 25879600.59019059\n",
      "Using scale consts: [0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25879606.86205872\n",
      "batch [10] loss: 25881074.741297778\n",
      "batch [20] loss: 25880635.68637137\n",
      "batch [30] loss: 25880896.537103076\n",
      "batch [40] loss: 25880391.6534623\n",
      "batch [50] loss: 25880309.100780167\n",
      "batch [60] loss: 25880820.556675427\n",
      "batch [70] loss: 25881070.030286096\n",
      "batch [80] loss: 25881065.50466006\n",
      "batch [90] loss: 25880996.18235779\n",
      "batch [100] loss: 25881598.103460938\n",
      "Using scale consts: [0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25881385.847729012\n",
      "batch [10] loss: 26145340.78090419\n",
      "batch [20] loss: 26461879.051148944\n",
      "batch [30] loss: 26904954.52548657\n",
      "batch [40] loss: 27748418.459347796\n",
      "batch [50] loss: 28874176.782717504\n",
      "batch [60] loss: 29925766.787212953\n",
      "batch [70] loss: 30853679.01160893\n",
      "batch [80] loss: 31619179.117308125\n",
      "batch [90] loss: 32196934.596058857\n",
      "batch [100] loss: 32677942.65735881\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1])\n",
      "Model pred:  tensor([[9.9995e-01, 5.3653e-05],\n",
      "        [4.0988e-02, 9.5901e-01],\n",
      "        [2.9510e-02, 9.7049e-01],\n",
      "        [3.0509e-02, 9.6949e-01],\n",
      "        [1.6211e-01, 8.3789e-01],\n",
      "        [2.9823e-01, 7.0177e-01],\n",
      "        [9.9987e-01, 1.2658e-04],\n",
      "        [4.7754e-02, 9.5225e-01],\n",
      "        [1.5217e-01, 8.4783e-01],\n",
      "        [2.9844e-02, 9.7016e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.96021207665734\n",
      "batch [10] loss: 28979501.747209214\n",
      "batch [20] loss: 28979501.747209214\n",
      "batch [30] loss: 28979501.747209214\n",
      "batch [40] loss: 28979501.747209214\n",
      "batch [50] loss: 28979501.747209214\n",
      "batch [60] loss: 28979501.747209214\n",
      "batch [70] loss: 28979501.747209214\n",
      "batch [80] loss: 28979501.747209214\n",
      "batch [90] loss: 28979501.747209214\n",
      "batch [100] loss: 28979501.747209214\n",
      "Using scale consts: [1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28979518.547253463\n",
      "batch [10] loss: 28982827.313996423\n",
      "batch [20] loss: 28984350.670182765\n",
      "batch [30] loss: 28983318.498663615\n",
      "batch [40] loss: 28982663.33590222\n",
      "batch [50] loss: 28982740.57514295\n",
      "batch [60] loss: 28982517.375012178\n",
      "batch [70] loss: 28981524.441753224\n",
      "batch [80] loss: 28981061.97183263\n",
      "batch [90] loss: 28982721.049539506\n",
      "batch [100] loss: 28982244.813486077\n",
      "Using scale consts: [10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 28981955.54830081\n",
      "batch [10] loss: 29126271.763377406\n",
      "batch [20] loss: 29284439.14591965\n",
      "batch [30] loss: 29421735.268633623\n",
      "batch [40] loss: 29537502.53615331\n",
      "batch [50] loss: 29677415.53421096\n",
      "batch [60] loss: 29865127.80081591\n",
      "batch [70] loss: 30080749.811087105\n",
      "batch [80] loss: 30434894.76424305\n",
      "batch [90] loss: 30860743.30361444\n",
      "batch [100] loss: 31361327.6477171\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0])\n",
      "Model pred:  tensor([[3.0430e-02, 9.6957e-01],\n",
      "        [1.7553e-01, 8.2447e-01],\n",
      "        [1.6988e-02, 9.8301e-01],\n",
      "        [4.0870e-03, 9.9591e-01],\n",
      "        [1.6988e-02, 9.8301e-01],\n",
      "        [9.9990e-01, 9.8811e-05],\n",
      "        [9.6875e-02, 9.0312e-01],\n",
      "        [9.9891e-01, 1.0924e-03],\n",
      "        [1.0666e-01, 8.9334e-01],\n",
      "        [9.9922e-01, 7.7805e-04]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 0, 1, 0, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.617656338632807\n",
      "batch [10] loss: 25126134.651201244\n",
      "batch [20] loss: 25101269.76052454\n",
      "batch [30] loss: 25100022.554415595\n",
      "batch [40] loss: 25098926.370071344\n",
      "batch [50] loss: 25099130.292661596\n",
      "batch [60] loss: 25098454.49923934\n",
      "batch [70] loss: 25098910.70362643\n",
      "batch [80] loss: 25099116.108255185\n",
      "batch [90] loss: 25099691.763026576\n",
      "batch [100] loss: 25099077.456648715\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25099278.268609088\n",
      "batch [10] loss: 25101502.692359403\n",
      "batch [20] loss: 25101078.327170845\n",
      "batch [30] loss: 25100461.656277843\n",
      "batch [40] loss: 25100503.27824846\n",
      "batch [50] loss: 25100317.17453727\n",
      "batch [60] loss: 25100430.26801959\n",
      "batch [70] loss: 25101026.907900743\n",
      "batch [80] loss: 25100873.766535982\n",
      "batch [90] loss: 25100857.791829016\n",
      "batch [100] loss: 25100860.61519941\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.07500000000000001, 0.025, 0.07500000000000001]\n",
      "batch [0] loss: 25100573.045976542\n",
      "batch [10] loss: 25179962.881365504\n",
      "batch [20] loss: 25262845.46556081\n",
      "batch [30] loss: 25338933.242838606\n",
      "batch [40] loss: 25398921.138001394\n",
      "batch [50] loss: 25448235.797165237\n",
      "batch [60] loss: 25493441.21618096\n",
      "batch [70] loss: 25529099.422906313\n",
      "batch [80] loss: 25562227.562477086\n",
      "batch [90] loss: 25591324.18626467\n",
      "batch [100] loss: 25618267.875549622\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0163, 0.9837],\n",
      "        [0.0476, 0.9524],\n",
      "        [0.1718, 0.8282],\n",
      "        [0.0113, 0.9887],\n",
      "        [0.1750, 0.8250],\n",
      "        [0.0950, 0.9050],\n",
      "        [0.0306, 0.9694],\n",
      "        [0.1943, 0.8057],\n",
      "        [0.0071, 0.9929],\n",
      "        [0.3819, 0.6181]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.7239197859199806\n",
      "batch [10] loss: 32857299.864552114\n",
      "batch [20] loss: 32857299.864552114\n",
      "batch [30] loss: 32857299.864552114\n",
      "batch [40] loss: 32857299.864552114\n",
      "batch [50] loss: 32857299.864552114\n",
      "batch [60] loss: 32857299.864552114\n",
      "batch [70] loss: 32857299.864552114\n",
      "batch [80] loss: 32857299.864552114\n",
      "batch [90] loss: 32857299.864552114\n",
      "batch [100] loss: 32857299.864552114\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 32857299.864552114\n",
      "batch [10] loss: 32857299.86455211\n",
      "batch [20] loss: 32857299.86455211\n",
      "batch [30] loss: 32857299.86455211\n",
      "batch [40] loss: 32857299.86455211\n",
      "batch [50] loss: 32857299.86455211\n",
      "batch [60] loss: 32857299.86455211\n",
      "batch [70] loss: 32857299.86455211\n",
      "batch [80] loss: 32857299.86455211\n",
      "batch [90] loss: 32857299.86455211\n",
      "batch [100] loss: 32857299.86455211\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 32857299.86455211\n",
      "batch [10] loss: 32857299.86455211\n",
      "batch [20] loss: 32857299.86455211\n",
      "batch [30] loss: 32857299.86455211\n",
      "batch [40] loss: 32857299.86455211\n",
      "batch [50] loss: 32857299.86455211\n",
      "batch [60] loss: 32857299.86455211\n",
      "batch [70] loss: 32857299.86455211\n",
      "batch [80] loss: 32857299.86455211\n",
      "batch [90] loss: 32857299.86455211\n",
      "batch [100] loss: 32857299.86455211\n",
      "OG Labels:  tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[3.7988e-02, 9.6201e-01],\n",
      "        [9.9986e-01, 1.3542e-04],\n",
      "        [1.9139e-01, 8.0861e-01],\n",
      "        [9.8925e-01, 1.0752e-02],\n",
      "        [2.9701e-02, 9.7030e-01],\n",
      "        [1.8875e-01, 8.1125e-01],\n",
      "        [1.1179e-01, 8.8821e-01],\n",
      "        [7.1683e-02, 9.2832e-01],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [9.9147e-01, 8.5320e-03]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.43355679697558\n",
      "batch [10] loss: 21408700.795469567\n",
      "batch [20] loss: 21408700.795469567\n",
      "batch [30] loss: 21408700.795469567\n",
      "batch [40] loss: 21408700.795469567\n",
      "batch [50] loss: 21408700.795469567\n",
      "batch [60] loss: 21408700.795469567\n",
      "batch [70] loss: 21408700.795469567\n",
      "batch [80] loss: 21408700.795469567\n",
      "batch [90] loss: 21408700.795469567\n",
      "batch [100] loss: 21408700.795469567\n",
      "Using scale consts: [0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 21408708.748974673\n",
      "batch [10] loss: 21411623.774872284\n",
      "batch [20] loss: 21411123.02386616\n",
      "batch [30] loss: 21410840.19138042\n",
      "batch [40] loss: 21410127.15549328\n",
      "batch [50] loss: 21410425.198985223\n",
      "batch [60] loss: 21410132.85813302\n",
      "batch [70] loss: 21409795.91306168\n",
      "batch [80] loss: 21409521.126047906\n",
      "batch [90] loss: 21409406.38382145\n",
      "batch [100] loss: 21409308.667506903\n",
      "Using scale consts: [0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 21409231.27836137\n",
      "batch [10] loss: 21472178.34610329\n",
      "batch [20] loss: 21549430.006122824\n",
      "batch [30] loss: 21624041.76966492\n",
      "batch [40] loss: 21711973.4677284\n",
      "batch [50] loss: 21782399.764567696\n",
      "batch [60] loss: 21841263.20139139\n",
      "batch [70] loss: 21890853.443628095\n",
      "batch [80] loss: 21925711.06329835\n",
      "batch [90] loss: 21955410.6046298\n",
      "batch [100] loss: 21981064.19334802\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.1427, 0.8573],\n",
      "        [0.0087, 0.9913],\n",
      "        [0.0297, 0.9703],\n",
      "        [0.1469, 0.8531],\n",
      "        [0.2007, 0.7993],\n",
      "        [0.1321, 0.8679],\n",
      "        [0.0170, 0.9830],\n",
      "        [0.0408, 0.9592],\n",
      "        [0.0263, 0.9737],\n",
      "        [0.0438, 0.9562]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.8949793961357533\n",
      "batch [10] loss: 30936799.872469008\n",
      "batch [20] loss: 30936799.872469008\n",
      "batch [30] loss: 30936799.872469008\n",
      "batch [40] loss: 30936799.872469008\n",
      "batch [50] loss: 30936799.872469008\n",
      "batch [60] loss: 30936799.872469008\n",
      "batch [70] loss: 30936799.872469008\n",
      "batch [80] loss: 30936799.872469008\n",
      "batch [90] loss: 30936799.872469008\n",
      "batch [100] loss: 30936799.872469008\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 30936799.872469008\n",
      "batch [10] loss: 30936799.872469008\n",
      "batch [20] loss: 30936799.872469008\n",
      "batch [30] loss: 30936799.872469008\n",
      "batch [40] loss: 30936799.872469008\n",
      "batch [50] loss: 30936799.872469008\n",
      "batch [60] loss: 30936799.872469008\n",
      "batch [70] loss: 30936799.872469008\n",
      "batch [80] loss: 30936799.872469008\n",
      "batch [90] loss: 30936799.872469008\n",
      "batch [100] loss: 30936799.872469008\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 30936799.872469008\n",
      "batch [10] loss: 30936799.872469008\n",
      "batch [20] loss: 30936799.872469008\n",
      "batch [30] loss: 30936799.872469008\n",
      "batch [40] loss: 30936799.872469008\n",
      "batch [50] loss: 30936799.872469008\n",
      "batch [60] loss: 30936799.872469008\n",
      "batch [70] loss: 30936799.872469008\n",
      "batch [80] loss: 30936799.872469008\n",
      "batch [90] loss: 30936799.872469008\n",
      "batch [100] loss: 30936799.872469008\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.0041, 0.9959],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.0194, 0.9806],\n",
      "        [0.1492, 0.8508],\n",
      "        [0.0313, 0.9687],\n",
      "        [0.1301, 0.8699],\n",
      "        [0.1396, 0.8604],\n",
      "        [0.0570, 0.9430],\n",
      "        [0.0082, 0.9918],\n",
      "        [0.1843, 0.8157]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 2.9297539322612836\n",
      "batch [10] loss: 25123399.89643363\n",
      "batch [20] loss: 25123399.89643363\n",
      "batch [30] loss: 25123399.89643363\n",
      "batch [40] loss: 25123399.89643363\n",
      "batch [50] loss: 25123399.89643363\n",
      "batch [60] loss: 25123399.89643363\n",
      "batch [70] loss: 25123399.89643363\n",
      "batch [80] loss: 25123399.89643363\n",
      "batch [90] loss: 25123399.89643363\n",
      "batch [100] loss: 25123399.89643363\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 25123399.89643363\n",
      "batch [10] loss: 25123399.89643363\n",
      "batch [20] loss: 25123399.89643363\n",
      "batch [30] loss: 25123399.89643363\n",
      "batch [40] loss: 25123399.89643363\n",
      "batch [50] loss: 25123399.89643363\n",
      "batch [60] loss: 25123399.89643363\n",
      "batch [70] loss: 25123399.89643363\n",
      "batch [80] loss: 25123399.89643363\n",
      "batch [90] loss: 25123399.89643363\n",
      "batch [100] loss: 25123399.89643363\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 25123399.89643363\n",
      "batch [10] loss: 25123399.89643363\n",
      "batch [20] loss: 25123399.89643363\n",
      "batch [30] loss: 25123399.89643363\n",
      "batch [40] loss: 25123399.89643363\n",
      "batch [50] loss: 25123399.89643363\n",
      "batch [60] loss: 25123399.89643363\n",
      "batch [70] loss: 25123399.89643363\n",
      "batch [80] loss: 25123399.89643363\n",
      "batch [90] loss: 25123399.89643363\n",
      "batch [100] loss: 25123399.89643363\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[3.0430e-02, 9.6957e-01],\n",
      "        [8.2053e-03, 9.9179e-01],\n",
      "        [2.7510e-01, 7.2490e-01],\n",
      "        [6.2958e-03, 9.9370e-01],\n",
      "        [9.9979e-01, 2.0514e-04],\n",
      "        [2.2240e-01, 7.7760e-01],\n",
      "        [2.3532e-02, 9.7647e-01],\n",
      "        [9.4080e-02, 9.0592e-01],\n",
      "        [1.5938e-02, 9.8406e-01],\n",
      "        [3.6223e-01, 6.3777e-01]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.470908275718056\n",
      "batch [10] loss: 26831700.730384678\n",
      "batch [20] loss: 26831700.730384678\n",
      "batch [30] loss: 26831700.730384674\n",
      "batch [40] loss: 26831700.730384674\n",
      "batch [50] loss: 26831700.730384674\n",
      "batch [60] loss: 26831700.730384674\n",
      "batch [70] loss: 26831700.730384674\n",
      "batch [80] loss: 26831700.730384674\n",
      "batch [90] loss: 26831700.730384674\n",
      "batch [100] loss: 26831700.730384674\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 26831708.299323216\n",
      "batch [10] loss: 26833850.50646367\n",
      "batch [20] loss: 26833463.976954855\n",
      "batch [30] loss: 26833251.7111078\n",
      "batch [40] loss: 26832976.322721876\n",
      "batch [50] loss: 26833001.10677979\n",
      "batch [60] loss: 26832422.938731037\n",
      "batch [70] loss: 26833386.468965437\n",
      "batch [80] loss: 26832587.112858534\n",
      "batch [90] loss: 26832459.123429485\n",
      "batch [100] loss: 26832254.586577814\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 26832671.68227542\n",
      "batch [10] loss: 26896089.18881727\n",
      "batch [20] loss: 26963674.29917227\n",
      "batch [30] loss: 27024577.507724047\n",
      "batch [40] loss: 27075928.27489263\n",
      "batch [50] loss: 27116482.453022428\n",
      "batch [60] loss: 27156134.622858472\n",
      "batch [70] loss: 27188517.93793629\n",
      "batch [80] loss: 27217038.454436645\n",
      "batch [90] loss: 27242700.91328384\n",
      "batch [100] loss: 27265182.082387954\n",
      "OG Labels:  tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.9972, 0.0028],\n",
      "        [0.9989, 0.0011],\n",
      "        [0.2325, 0.7675],\n",
      "        [0.0283, 0.9717],\n",
      "        [0.0230, 0.9770],\n",
      "        [0.0471, 0.9529],\n",
      "        [0.0238, 0.9762],\n",
      "        [0.1808, 0.8192],\n",
      "        [0.0754, 0.9246],\n",
      "        [0.0129, 0.9871]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.6187773143728466\n",
      "batch [10] loss: 28662870.921557054\n",
      "batch [20] loss: 28660227.59802766\n",
      "batch [30] loss: 28659850.911930043\n",
      "batch [40] loss: 28659865.23548952\n",
      "batch [50] loss: 28659701.19549571\n",
      "batch [60] loss: 28659801.057566583\n",
      "batch [70] loss: 28659801.57141863\n",
      "batch [80] loss: 28659801.785150956\n",
      "batch [90] loss: 28659918.079861037\n",
      "batch [100] loss: 28659901.164740037\n",
      "Using scale consts: [0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 28659943.191883303\n",
      "batch [10] loss: 28661554.502795897\n",
      "batch [20] loss: 28661621.364523664\n",
      "batch [30] loss: 28661780.845905118\n",
      "batch [40] loss: 28662162.901373588\n",
      "batch [50] loss: 28660961.968400337\n",
      "batch [60] loss: 28661605.166360915\n",
      "batch [70] loss: 28661234.759921797\n",
      "batch [80] loss: 28660640.91775348\n",
      "batch [90] loss: 28660799.07583323\n",
      "batch [100] loss: 28661102.90280787\n",
      "Using scale consts: [0.07500000000000001, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 28660717.953952625\n",
      "batch [10] loss: 28735087.44477994\n",
      "batch [20] loss: 28813746.92349817\n",
      "batch [30] loss: 28874062.487063047\n",
      "batch [40] loss: 28931238.543529183\n",
      "batch [50] loss: 28977438.74907869\n",
      "batch [60] loss: 29019310.60224758\n",
      "batch [70] loss: 29066037.3827146\n",
      "batch [80] loss: 29101319.368437104\n",
      "batch [90] loss: 29145505.28255684\n",
      "batch [100] loss: 29199365.377851896\n",
      "OG Labels:  tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Model pred:  tensor([[0.8892, 0.1108],\n",
      "        [0.0042, 0.9958],\n",
      "        [0.1471, 0.8529],\n",
      "        [0.0289, 0.9711],\n",
      "        [0.1914, 0.8086],\n",
      "        [0.2220, 0.7780],\n",
      "        [0.0144, 0.9856],\n",
      "        [0.0087, 0.9913],\n",
      "        [0.0113, 0.9887],\n",
      "        [0.1099, 0.8901]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.103615243066991\n",
      "batch [10] loss: 32102199.867664874\n",
      "batch [20] loss: 32102199.867664874\n",
      "batch [30] loss: 32102199.867664874\n",
      "batch [40] loss: 32102199.867664874\n",
      "batch [50] loss: 32102199.867664874\n",
      "batch [60] loss: 32102199.867664874\n",
      "batch [70] loss: 32102199.867664874\n",
      "batch [80] loss: 32102199.867664874\n",
      "batch [90] loss: 32102199.867664874\n",
      "batch [100] loss: 32102199.867664874\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 32102199.867664874\n",
      "batch [10] loss: 32102199.867664874\n",
      "batch [20] loss: 32102199.867664874\n",
      "batch [30] loss: 32102199.867664874\n",
      "batch [40] loss: 32102199.867664874\n",
      "batch [50] loss: 32102199.867664874\n",
      "batch [60] loss: 32102199.867664874\n",
      "batch [70] loss: 32102199.867664874\n",
      "batch [80] loss: 32102199.867664874\n",
      "batch [90] loss: 32102199.867664874\n",
      "batch [100] loss: 32102199.867664874\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 32102199.867664874\n",
      "batch [10] loss: 32102199.867664874\n",
      "batch [20] loss: 32102199.867664874\n",
      "batch [30] loss: 32102199.867664874\n",
      "batch [40] loss: 32102199.867664874\n",
      "batch [50] loss: 32102199.867664874\n",
      "batch [60] loss: 32102199.867664874\n",
      "batch [70] loss: 32102199.867664874\n",
      "batch [80] loss: 32102199.867664874\n",
      "batch [90] loss: 32102199.867664874\n",
      "batch [100] loss: 32102199.867664874\n",
      "OG Labels:  tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1])\n",
      "Model pred:  tensor([[0.0063, 0.9937],\n",
      "        [0.0298, 0.9702],\n",
      "        [0.0163, 0.9837],\n",
      "        [0.0189, 0.9811],\n",
      "        [0.0117, 0.9883],\n",
      "        [0.0198, 0.9802],\n",
      "        [0.9964, 0.0036],\n",
      "        [0.0081, 0.9919],\n",
      "        [0.6775, 0.3225],\n",
      "        [0.0137, 0.9863]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 0, 1, 0, 1], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 4.03828198860693\n",
      "batch [10] loss: 36951699.847673744\n",
      "batch [20] loss: 36951699.847673744\n",
      "batch [30] loss: 36951699.847673744\n",
      "batch [40] loss: 36951699.847673744\n",
      "batch [50] loss: 36951699.847673744\n",
      "batch [60] loss: 36951699.847673744\n",
      "batch [70] loss: 36951699.847673744\n",
      "batch [80] loss: 36951699.847673744\n",
      "batch [90] loss: 36951699.847673744\n",
      "batch [100] loss: 36951699.847673744\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 36951699.847673744\n",
      "batch [10] loss: 36951699.847673744\n",
      "batch [20] loss: 36951699.847673744\n",
      "batch [30] loss: 36951699.847673744\n",
      "batch [40] loss: 36951699.847673744\n",
      "batch [50] loss: 36951699.847673744\n",
      "batch [60] loss: 36951699.847673744\n",
      "batch [70] loss: 36951699.847673744\n",
      "batch [80] loss: 36951699.847673744\n",
      "batch [90] loss: 36951699.847673744\n",
      "batch [100] loss: 36951699.847673744\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 36951699.847673744\n",
      "batch [10] loss: 36951699.847673744\n",
      "batch [20] loss: 36951699.847673744\n",
      "batch [30] loss: 36951699.847673744\n",
      "batch [40] loss: 36951699.847673744\n",
      "batch [50] loss: 36951699.847673744\n",
      "batch [60] loss: 36951699.847673744\n",
      "batch [70] loss: 36951699.847673744\n",
      "batch [80] loss: 36951699.847673744\n",
      "batch [90] loss: 36951699.847673744\n",
      "batch [100] loss: 36951699.847673744\n",
      "OG Labels:  tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[3.1675e-02, 9.6833e-01],\n",
      "        [2.2274e-01, 7.7726e-01],\n",
      "        [1.0666e-01, 8.9334e-01],\n",
      "        [1.7183e-01, 8.2817e-01],\n",
      "        [4.7082e-01, 5.2918e-01],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [3.1658e-02, 9.6834e-01],\n",
      "        [3.2188e-02, 9.6781e-01],\n",
      "        [1.3007e-01, 8.6993e-01],\n",
      "        [9.9999e-01, 7.7904e-06]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.0871909884449282\n",
      "batch [10] loss: 17779501.09578468\n",
      "batch [20] loss: 17779501.09578468\n",
      "batch [30] loss: 17779501.09578468\n",
      "batch [40] loss: 17779501.09578468\n",
      "batch [50] loss: 17779501.09578468\n",
      "batch [60] loss: 17779501.09578468\n",
      "batch [70] loss: 17779501.09578468\n",
      "batch [80] loss: 17779501.09578468\n",
      "batch [90] loss: 17779501.09578468\n",
      "batch [100] loss: 17779501.09578468\n",
      "Using scale consts: [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 1.0]\n",
      "batch [0] loss: 17779511.617479615\n",
      "batch [10] loss: 17782318.85603514\n",
      "batch [20] loss: 17781620.1026655\n",
      "batch [30] loss: 17781813.74565025\n",
      "batch [40] loss: 17780930.24419386\n",
      "batch [50] loss: 17781356.90320024\n",
      "batch [60] loss: 17780613.000234753\n",
      "batch [70] loss: 17780838.445440814\n",
      "batch [80] loss: 17780965.35506138\n",
      "batch [90] loss: 17780146.424506355\n",
      "batch [100] loss: 17780010.78938804\n",
      "Using scale consts: [0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 10.0]\n",
      "batch [0] loss: 17780427.9916832\n",
      "batch [10] loss: 17843597.558247235\n",
      "batch [20] loss: 17909926.356483158\n",
      "batch [30] loss: 17975250.849230204\n",
      "batch [40] loss: 18024403.057619337\n",
      "batch [50] loss: 18069766.161625527\n",
      "batch [60] loss: 18110324.151286755\n",
      "batch [70] loss: 18142287.743919384\n",
      "batch [80] loss: 18173199.923025046\n",
      "batch [90] loss: 18202037.29039638\n",
      "batch [100] loss: 18227838.14272436\n",
      "OG Labels:  tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "Model pred:  tensor([[1.0619e-01, 8.9381e-01],\n",
      "        [9.9986e-01, 1.3741e-04],\n",
      "        [5.7958e-02, 9.4204e-01],\n",
      "        [1.6825e-01, 8.3175e-01],\n",
      "        [1.4330e-01, 8.5670e-01],\n",
      "        [2.9794e-02, 9.7021e-01],\n",
      "        [9.6581e-02, 9.0342e-01],\n",
      "        [1.8717e-01, 8.1283e-01],\n",
      "        [7.7298e-02, 9.2270e-01],\n",
      "        [9.9577e-01, 4.2337e-03]], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "tensor([1, 0, 1, 1, 1, 1, 1, 1, 1, 0], device='cuda:0')\n",
      "torch.Size([10, 2535703])\n",
      "Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 3.232479525930658\n",
      "batch [10] loss: 20842100.795804724\n",
      "batch [20] loss: 20842100.795804724\n",
      "batch [30] loss: 20842100.795804724\n",
      "batch [40] loss: 20842100.795804724\n",
      "batch [50] loss: 20842100.795804724\n",
      "batch [60] loss: 20842100.795804724\n",
      "batch [70] loss: 20842100.795804724\n",
      "batch [80] loss: 20842100.795804724\n",
      "batch [90] loss: 20842100.795804724\n",
      "batch [100] loss: 20842100.795804724\n",
      "Using scale consts: [0.05, 1.0, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
      "batch [0] loss: 20842108.731304936\n",
      "batch [10] loss: 20843984.08431725\n",
      "batch [20] loss: 20844218.815160044\n",
      "batch [30] loss: 20843864.87793503\n",
      "batch [40] loss: 20843565.642128132\n",
      "batch [50] loss: 20843137.578314733\n",
      "batch [60] loss: 20843840.45983118\n",
      "batch [70] loss: 20843207.91366574\n",
      "batch [80] loss: 20843399.755404968\n",
      "batch [90] loss: 20843276.11675732\n",
      "batch [100] loss: 20843136.704468604\n",
      "Using scale consts: [0.025, 10.0, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025]\n",
      "batch [0] loss: 20843271.187022906\n",
      "batch [10] loss: 20915190.52812893\n",
      "batch [20] loss: 21040158.41914086\n",
      "batch [30] loss: 21140701.40815832\n",
      "batch [40] loss: 21226360.479577627\n",
      "batch [50] loss: 21296147.9363648\n",
      "batch [60] loss: 21355963.488014296\n",
      "batch [70] loss: 21398234.27198706\n",
      "batch [80] loss: 21438155.677261442\n",
      "batch [90] loss: 21472053.90835862\n",
      "batch [100] loss: 21500807.25003251\n"
     ]
    }
   ],
   "source": [
    "cw_attack = L2Adversary(\n",
    "    targeted=False, confidence=0.0, c_range=(1e-1, 1e10),\n",
    "    search_steps=3, max_steps=1000, abort_early=True,\n",
    "    box=(0., 1.), optimizer_lr=1e-2, init_rand=True\n",
    ")\n",
    "advxs = None\n",
    "all_inputs = []\n",
    "all_advx = []\n",
    "\n",
    "for data, target in test_loader:\n",
    "    if len(all_advx) * batch_size >= 500:\n",
    "        break\n",
    "    \n",
    "    print(\"OG Labels: \", target)\n",
    "    target = F.softmax(model(data.to(device)), dim=1)\n",
    "    print(\"Model pred: \", target)\n",
    "    target = torch.argmax(target, dim=1)\n",
    "    print(target)\n",
    "    data = data.view(batch_size, -1)\n",
    "    with open(os.path.join(outfolder, 'input.npz'), 'wb') as file:\n",
    "        all_inputs.append(sparse.csr_matrix(data))\n",
    "        sparse.save_npz(file, sparse.csr_matrix(data))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    advxs = cw_attack(model, data, target, to_numpy=False)\n",
    "    sparse_advxs = sparse.csr_matrix(torch.round(advxs), dtype='i1')\n",
    "    all_advx.append(sparse_advxs)\n",
    "    \n",
    "    with open(os.path.join(outfolder, 'advxs.npz'), 'wb') as file:\n",
    "        sparse.save_npz(file, sparse_advxs)\n",
    "        \n",
    "all_inputs = sparse.vstack(all_inputs)\n",
    "all_advx = sparse.vstack(all_advx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "religious-jason",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_folder = os.path.join('data', 'out', 'all-apps', 'hind')\n",
    "# os.makedirs(attack_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(outfolder, 'inputs.npz'), 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(all_inputs))\n",
    "\n",
    "with open(os.path.join(outfolder, 'advxs.npz'), 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(all_advx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advxs.npz', 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(advxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dynamic-teddy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.2956e-05)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(torch.abs(advxs - 0.5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "subtle-shirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 0, 0, 0, 0, 1, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(F.softmax(model(torch.round(advxs.double().to(device))), dim=1), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "conditional-directory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004719046355192229"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = sparse.csr_matrix(torch.round(advxs))\n",
    "density = mat.getnnz() / np.prod(mat.shape)\n",
    "density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "turkish-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55096668"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.data.nbytes + mat.indptr.nbytes + mat.indices.nbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fiscal-summit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.35703"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2535703 * 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "uniform-librarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('advxs.npz', 'wb') as file:\n",
    "    sparse.save_npz(file, sparse.csr_matrix(advxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "medium-hepatitis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [0., 1.])\n",
    "pd.Series(to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [0., 1.]).tolist(), \n",
    "          index=np.arange(-1,1,.001)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "permanent-workplace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 0., 1.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor(np.random.randint(0,2, (3,4)))\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "pursuant-mother",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-inf, -inf, -inf, inf],\n",
       "        [inf, inf, inf, -inf],\n",
       "        [inf, inf, -inf, inf]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_tanh_space(inputs, [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fleet-ideal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-10., -10., -10.,  inf],\n",
       "        [ inf,  inf,  inf, -10.],\n",
       "        [ inf,  inf, -10.,  inf]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(to_tanh_space(inputs, [0., 1.]), min=-1e4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "arabic-shock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0.],\n",
       "        [0., 0., 1., 1.],\n",
       "        [0., 1., 0., 1.]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(torch.clamp(to_tanh_space(inputs, [0., 1.]), min=-1e4), [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "entire-flood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(inputs + from_tanh_space(pert_tanh*1000, [0., 1.]), max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sexual-graduate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9125,  1.7148, -0.6042,  0.3287],\n",
       "        [ 0.8110,  0.7492, -0.4228, -0.1139],\n",
       "        [-0.0307,  0.3254,  0.4284,  1.6197]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_tanh = torch.zeros((3,4))  # type: torch.FloatTensor\n",
    "nn.init.normal_(pert_tanh, mean=0, std=1)\n",
    "pert_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "protecting-validation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 1.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(pert_tanh*1e4, [0., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "lesser-ecuador",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.clamp(inputs + from_tanh_space(pert_tanh*1e4, [0., 1.]), max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mediterranean-argentina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(from_tanh_space(\n",
    "    (to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [-1., 1.])*1000), [-1., 1]).tolist(),\n",
    "    index=to_tanh_space(torch.Tensor(np.arange(-1,1,.001)).double(), [-1., 1.]).tolist()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "reliable-power",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(from_tanh_space(torch.Tensor(np.arange(-1000, 1000, 1)), box=[0,1]).tolist(),\n",
    "    index=np.arange(-1000, 1000, 1)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "durable-system",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.9990], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_tanh_space(to_tanh_space(torch.Tensor([0,.999]).double(), [-1., 1.]), [-1., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-vanilla",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # Training settings\n",
    "#     parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "#     parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "#                         help='input batch size for training (default: 64)')\n",
    "#     parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "#                         help='input batch size for testing (default: 1000)')\n",
    "#     parser.add_argument('--epochs', type=int, default=14, metavar='N',\n",
    "#                         help='number of epochs to train (default: 14)')\n",
    "#     parser.add_argument('--lr', type=float, default=1.0, metavar='LR',\n",
    "#                         help='learning rate (default: 1.0)')\n",
    "#     parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "#                         help='Learning rate step gamma (default: 0.7)')\n",
    "#     parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "#                         help='disables CUDA training')\n",
    "#     parser.add_argument('--dry-run', action='store_true', default=False,\n",
    "#                         help='quickly check a single pass')\n",
    "#     parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "#                         help='random seed (default: 1)')\n",
    "#     parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "#                         help='how many batches to wait before logging training status')\n",
    "#     parser.add_argument('--save-model', action='store_true', default=False,\n",
    "#                         help='For Saving the current Model')\n",
    "#     args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_kwargs = {'batch_size': args.batch_size}\n",
    "    test_kwargs = {'batch_size': args.test_batch_size}\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # load data (will need to be adapted as well)\n",
    "    # 1) load A_test\n",
    "    # 2) load labels\n",
    "    # 3) Perform train-test-split\n",
    "#     transform=transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#         ])\n",
    "#     dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "#                        transform=transform)\n",
    "#     dataset2 = datasets.MNIST('../data', train=False,\n",
    "#                        transform=transform)\n",
    "    dataset = HindroidDataset(\n",
    "        'data/out/all-apps/hindroid-train-half/A_test.npz', \n",
    "        'data/out/all-apps/hindroid-train-half/predictions.csv',\n",
    "        'AAT'\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(dataset,**train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, **test_kwargs)\n",
    "\n",
    "    model = HindroidSubstitute().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
