{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from scipy.stats import t\n",
    "from scipy import interp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_config = json.load(open('../config/notebook.json', 'r'))\n",
    "if notebook_config['testing']:\n",
    "    data_dir = '../test/'\n",
    "else:\n",
    "    data_dir = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_pickle(data_dir + 'tmp/model_results.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('baseline accuracy: ' + str(sum(test_data['target'] == test_data['base_pred']) / len(test_data['target'])))\n",
    "print('unigram accuracy: ' + str(sum(test_data['target'] == test_data['unigram_pred']) / len(test_data['target'])))\n",
    "print('phrase accuracy: ' + str(sum(test_data['target'] == test_data['phrase_pred']) / len(test_data['target'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note**:\n",
    "    - Adding text info did improve the performance of the models\n",
    "    - Phrase models and unigram models are not as different as we expected, maybe because there are many high-quality unigrams as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_data = test_data.loc[test_data['phrase_pred'] == 'UP'].copy()\n",
    "stay_data = test_data.loc[test_data['phrase_pred'] == 'STAY'].copy()\n",
    "down_data = test_data.loc[test_data['phrase_pred'] == 'DOWN'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [up_data, stay_data, down_data]\n",
    "models = ['base_pred', 'unigram_pred', 'phrase_pred']\n",
    "accuracy = []\n",
    "for model in models:\n",
    "    temp = []\n",
    "    for label in labels:\n",
    "        if len(label) == 0:\n",
    "            temp.append(0)\n",
    "        else:\n",
    "            temp.append(len(label[label['target'] == label[model]]) / len(label))\n",
    "    accuracy.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'baseline': accuracy[0],\n",
    "             'unigram': accuracy[1],\n",
    "             'phrase': accuracy[2]},\n",
    "            index = ['UP', 'STAY', 'DOWN'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Note**: Our phrase model performs much better for the `STAY` class compared to other baselines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle(data_dir + 'processed/feature_encoded_merged_data.pkl')\n",
    "\n",
    "train = data.loc[data['dataset'] == 'train'].copy()\n",
    "val = data.loc[data['dataset'] == 'val'].copy()\n",
    "test = data.loc[data['dataset'] == 'test'].copy()\n",
    "\n",
    "unigrams = pd.read_csv(data_dir + 'processed/model_unigrams.csv')\n",
    "phrases = pd.read_csv(data_dir + 'financial_phrases_sample.txt', sep = '\\t', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_model = pickle.load(open(data_dir + 'models/uni_model', 'rb'))\n",
    "phrase_model = pickle.load(open(data_dir + 'models/phrase_model', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "all_events = pd.DataFrame(mlb.fit_transform(data['cleaned_event']),\n",
    "                   columns = mlb.classes_,\n",
    "                   index = data['cleaned_event'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = np.array(all_events.columns)\n",
    "numerical = np.array(['Surprise(%)', 'price_change_7', \n",
    "              'price_change_30', 'price_change_90', 'price_change_365',\n",
    "              'prev_vix_values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_importance = uni_model.feature_importances_\n",
    "phrase_importance = phrase_model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Unigram Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_feature_importance = pd.DataFrame({'feature': np.concatenate((events, numerical, unigrams.values.ravel())),\n",
    "                                       'importance': uni_importance})\n",
    "uni_feature_importance.sort_values(by = 'importance', ascending = False).head(20).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Phrase Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(phrases) > len(phrase_importance):\n",
    "    phrase_features = np.concatenate((events, numerical, phrases[1].values.ravel()))[:len(phrase_importance)]\n",
    "else:\n",
    "    phrase_importance = phrase_importance[:len(phrases)]\n",
    "\n",
    "phrase_feature_importance = pd.DataFrame({'feature': phrase_features,\n",
    "                                       'importance': phrase_importance})\n",
    "phrase_feature_importance.sort_values(by = 'importance', ascending = False).head(20).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Summary**: As we can see from the two tables above, the important features in our model, compared to those in the unigram model, are relatively similar. In fact, all of the text features have not make it into top 5 :/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
